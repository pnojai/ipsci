---
title: "R Notebook"
output: html_notebook
---


```{r}


# of topics, using LDA
top_terms_by_topic_LDA <- function(input_text, # should be a columm from a dataframe
                                   plot = F, # return a plot? TRUE by defult
                                   number_of_topics = 2) # number of topics (4 by default)
{    
    # create a corpus (type of object expected by tm) and document term matrix
    Corpus <- Corpus(VectorSource(input_text)) # make a corpus object
    

  Corpus <- tm_map(Corpus, content_transformer(tolower)) 
  Corpus <- tm_map(Corpus, content_transformer(function(x) gsub("[[:punct:]]+"," ", x)))
  Corpus <- tm_map(Corpus, function(x) removePunctuation(x,
                                    preserve_intra_word_contractions = FALSE,
                                    preserve_intra_word_dashes = FALSE))
  Corpus <- tm_map(Corpus, removeNumbers)
  Corpus <- tm_map(Corpus, removeWords, stopwords("english"))
  #c_ham <- tm_map(c_ham, stemDocument, lazy = TRUE)

    DTM <- DocumentTermMatrix(Corpus) # get the count of words/document

    # remove any empty rows in our document term matrix (if there are any 
    # we'll get an error when we try to run our LDA)
    unique_indexes <- unique(DTM$i) # get the index of each unique value
    DTM <- DTM[unique_indexes,] # get a subset of only those indexes
    
    # preform LDA & get the words/topic in a tidy text format
    lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
    topics <- tidy(lda, matrix = "beta")

    # get the top ten terms for each topic
    top_terms <- topics  %>% # take the topics data frame and..
      group_by(topic) %>% # treat each topic as a different group
      top_n(10, beta) %>% # get the top 10 most informative words
      ungroup() %>% # ungroup
      arrange(topic, -beta) # arrange words in descending informativeness
    one_term <- top_terms[seq(1,nrow(top_terms),10),]
    

    # if the user asks for a plot (TRUE by default)
    if(plot == T){
        # plot the top ten terms for each topic in order
        top_terms %>% # take the top terms
          mutate(term = reorder(term, beta)) %>% # sort terms by beta value 
          ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
          geom_col(show.legend = FALSE) + # as a bar plot
          facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
          labs(x = NULL, y = "Beta") + # no x label, change y label 
          coord_flip() # turn bars sideways
    }else{ 
        # if the user does not request a plot
        # return a list of sorted terms instead
        return(one_term)
        #return(top_terms)
    }
}


 
# Get abstracts
sample <- data.frame(txt=as.character(patents$abstract)) 
sample$txt<-str_split(sample$txt, " ")
df<-top_terms_by_topic_LDA(sample, number_of_topics = nrow(patents))
# 
topic_df <- cbind(patents, df))
head(topic_df)
draft_topic_file <- write.csv(df,"data/topic_df.csv")
```



 
Ref
https://ropensci.org/blog/2017/09/19/patentsview/
https://www.patentsview.org/web/#viz/locations
https://raw.githubusercontent.com/ropensci/patentsview/483be92df486211bb2e6b54d64d5d7aacef40ab3/inst/site/vignettes/top-assignees.Rmd

## Section beginning Methods


"The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures."


The core tidyverse:
 *  ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics.  
 *  dplyr provides a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges.  
 *  tidyr provides a set of functions that help you get to tidy data.  
 *  readr provides a fast and friendly way to read rectangular data (like csv, tsv, and fwf).  
 *  purrr enhances Râ€™s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. 
 *  tibble is a modern re-imagining of the data frame, keeping what time has proven to be effective, and throwing out what it has not. Tibbles are data.frames that are lazy and surly.
 *  stringr provides a cohesive set of functions designed to make working with strings as easy as possible.  
 *  forcats provides a suite of useful tools that solve common problems with factors.

More specialised usage or Tidyverse-adjacent:
 *  readr, for reading flat files 
 *  readxl for .xls and .xlsx sheets.
 *  haven for SPSS, Stata, and SAS data.
 *  googledrive package allows you to interact with files on Google Drive from R.
 *  jsonlite for JSON.
 *  xml2 for XML.
 *  httr for web APIs.
 *  rvest for web scraping.
 *  DBI for relational databases. 
 *  lubridate for dates and date-times.
 *  hms for time-of-day values.
 *  blob for storing blob (binary) data. 
 *  magrittr provides the pipe, %>% used throughout the tidyverse. It also provide a number of more specialised piping operators (like %$% and %<>%) that can be useful in other places.
 *  glue provides an alternative to paste() that makes it easier to combine data and strings.
 *  Modelling within the tidyverse is largely a work in progress. This work will largely replace the modelr package used in R4DS.
 *  You may also find broom to be useful: it turns models into tidy data which you can then wrangle and visualise using the tools you already know.


Ref:
TODO: add notes on this section
tidyverse.org
https://rstudio.com/resources/webinars/the-grammar-and-graphics-of-data-science/


## Exploration of raw data
Compare uspto and google (focus on google for now (it is deprecated so may be missing files))
Use html for now, instead of google bigquery api
#>US8293178B2 - Chemochromic detector for sensing gas leakage and process for producing the same 
#https://patents.google.com/patent/US8293178
#http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&d=PALL&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.htm&r=1&f=G&l=50&s1=8293178.PN.&OS=PN/8293178&RS=PN/8293178 (a link explaining the quality of patentsview, uspto, and google patent capabilities and information outlining the uspot full text API is here https://developer.uspto.gov/api-catalog/patentsview -- TODO: add to FAQ)

```{r}
library(rvest)
#html_u <- read_html(file.path("data", "paft_uspto_8293178.html"))
html_g <- read_html(file.path("data", "8293178.html"))
html_g
```

```{r}
library(magrittr) #for pipes
library(dplyr) #for pull function
library(rvest) #get html nodes
library(xml2) #pull html data
library(selectr) #for xpath element
library(tibble)
library(purrr) #for map functions
library(datapasta) #for recreating tibble's with ease
#Sample Data
sample_data <- tibble::tibble(
  name = c("test"),
  link = c("https://patents.google.com/patent/US8293178"
  )
)
```

 
https://towardsdatascience.com/functions-with-r-and-rvest-a-laymens-guide-acda42325a77
### Tests
```{r}
title <- html_g %>%
        html_nodes("h1") %>%
        html_text()
title
claim <- html_g %>% html_nodes("div.claim") 
claim
divs <- html_g %>%
        html_nodes('div') %>% 
        html_text()
```


```{r} 
library(tidytext) 
library(tidyr)
library(stringr)



section_nodes <- html_g %>% html_nodes("section")

head(section_nodes)

section_ids <- section_nodes %>% html_attr("itemprop") 
#section_ids <- html_g %>% html_nodes(xpath="section//h2")
section_ids


#https://stackoverflow.com/questions/43598427/r-how-to-extract-items-from-xml-nodeset
#number=rep(1: (section_nodes)),


#TODO: combine
sections_text <- section_nodes %>% 
    html_text() %>% 
    str_trim(side = "both") %>%
    str_replace_all('(\\s){2,}', '\\1') %>% 
    str_replace_all('(\\r\\n|\\n){2}', '\\1') %>% 
    str_replace_all('[^A-z ]', '')
    #substr(start = nchar(body_text)-700, stop = nchar(body_text))
#        strsplit(split = "\n") %>%
#        unlist() %>%
#        .[. != ""]
sections_text


text_df <- tibble(document=section_ids, terms=sections_text)

text_df


```

```{r}


#TODO: combine
sections_text <- section_nodes %>% 
    html_text() %>% 
    str_trim(side = "both") %>%
    str_replace_all('(\\s){2,}', '\\1') %>% 
    str_replace_all('(\\r\\n|\\n){2}', '\\1') %>% 
    str_replace_all('[^A-z ]', '')
    #substr(start = nchar(body_text)-700, stop = nchar(body_text))
#        strsplit(split = "\n") %>%
#        unlist() %>%
#        .[. != ""]
sections_text

```


Demo of LDA
https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling#Unsupervised-topic-modeling-with-LDA
```{r}
# read in the libraries we're going to use
library(tidyverse) # general utility & workflow functions
library(tidytext) # tidy implimentation of NLP methods
library(topicmodels) # for LDA topic modelling 
library(tm) # general text mining functions, making document term matrixes
library(SnowballC) # for stemming



# of topics, using LDA
top_terms_by_topic_LDA <- function(input_text, # should be a columm from a dataframe
                                   plot = T, # return a plot? TRUE by defult
                                   number_of_topics = 4) # number of topics (4 by default)
{    
    # create a corpus (type of object expected by tm) and document term matrix
    Corpus <- Corpus(VectorSource(input_text)) # make a corpus object
    DTM <- DocumentTermMatrix(Corpus) # get the count of words/document

    # remove any empty rows in our document term matrix (if there are any 
    # we'll get an error when we try to run our LDA)
    unique_indexes <- unique(DTM$i) # get the index of each unique value
    DTM <- DTM[unique_indexes,] # get a subset of only those indexes
    
    # preform LDA & get the words/topic in a tidy text format
    lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
    topics <- tidy(lda, matrix = "beta")

    # get the top ten terms for each topic
    top_terms <- topics  %>% # take the topics data frame and..
      group_by(topic) %>% # treat each topic as a different group
      top_n(10, beta) %>% # get the top 10 most informative words
      ungroup() %>% # ungroup
      arrange(topic, -beta) # arrange words in descending informativeness

    # if the user asks for a plot (TRUE by default)
    if(plot == T){
        # plot the top ten terms for each topic in order
        top_terms %>% # take the top terms
          mutate(term = reorder(term, beta)) %>% # sort terms by beta value 
          ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
          geom_col(show.legend = FALSE) + # as a bar plot
          facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
          labs(x = NULL, y = "Beta") + # no x label, change y label 
          coord_flip() # turn bars sideways
    }else{ 
        # if the user does not request a plot
        # return a list of sorted terms instead
        return(top_terms)
    }
}




```


```{r}


top_terms_by_topic_LDA(text_df$terms, number_of_topics = 5)
```


Ref:
idea, use nasa highlights via tesseract to analyze https://www.techbriefs.com/dl/HOT100/MonSys.pdf
option, do a 1 level crawl into cited by or related patents https://patents.google.com/patent/US7383238B1/en#citedBy
html_node basics https://bradleyboehmke.github.io/2015/12/scraping-html-text.html
uspto html structure ideas https://en.wikipedia.org/wiki/User:Jrincayc/Patent_utils
another option for patent search https://cloud.google.com/blog/products/gcp/google-patents-public-datasets-connecting-public-paid-and-private-patent-data
patentsview (todo: put research here)

 
 
More tidy http://vita.had.co.nz/papers/tidy-data.pdf
Advanced:
https://rddj.info/
https://cloud.r-project.org/web/packages/hunspell/vignettes/intro.html
https://juliasilge.com/blog/life-changing-magic/
https://rud.is/projects/clinton_emails_01.html







## Just some notes

```{r}
require("devtools")
#install_github("tidymodels/textrecipes")

```






```{r}
library(recipes)
library(textrecipes)

data(okc_text)
o<-data(okc_text)
okc_rec <- recipe(~ ., data = okc_text) %>%
  step_tokenize(essay0, essay1)
okc_text
```


```{r}
okc_rec <- recipe(~ ., data = okc_text) %>%
  step_tokenize(essay0, essay1) %>% # Tokenizes to words by default
  step_stopwords(essay0, essay1) %>% # Uses the english snowball list by default
  step_tokenfilter(essay0, essay1, max_tokens = 100) %>%
  step_tfidf(essay0, essay1)
   
okc_obj <- okc_rec %>%
  prep(training = okc_text)
   
str(bake(okc_obj, okc_text), list.len = 15)
```

```{r}
recipe(~ ., data = data) %>%
  step_tokenize(text) %>%
  step_stem(text) %>%
  step_stopwords(text) %>%
  step_topwords(text) %>%
  step_tf(text)

# or

recipe(~ ., data = data) %>%
  step_tokenize(text) %>%
  step_stem(text) %>%
  step_tfidf(text)
```
Ref:
https://www.cbinsights.com/research/team-blog/patent-analytics-is-here/
https://medium.com/@glmack/patent-language-processing-fc99b57c0cd5
https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling#Unsupervised-topic-modeling-with-LDA
https://tidymodels.github.io/textrecipes/
https://tidymodels.github.io/textrecipes/articles/cookbook---using-more-complex-recipes-involving-text.html

gganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.

Also https://www.tidyverse.org/blog/2019/07/ragg-0-1-0/

More ideas on innovation https://www.patentsview.org/web/#viz/comparisons&cmp=all/state/numDesc/2019
https://tidymodels.github.io/textrecipes/articles/cookbook---using-more-complex-recipes-involving-text.html
https://www.rdocumentation.org/packages/naniar/versions/0.4.2
http://naniar.njtierney.com/articles/getting-started-w-naniar.html#modelling-missingness
https://arxiv.org/ftp/arxiv/papers/1703/1703.09570.pdf
https://bookdown.org/martin_monkman/DataScienceResources_book/textanalysis.html
https://cran.r-project.org/web/packages/monkeylearn/index.html
https://cran.r-project.org/web/packages/tidytext/index.html
https://cran.r-project.org/web/packages/tidytext/vignettes/tf_idf.html
https://cran.r-project.org/web/packages/tidytext/vignettes/topic_modeling.html
https://www.r-bloggers.com/an-overview-of-the-nlp-ecosystem-in-r-nlproc-textasdata/
https://cran.r-project.org/web/views/NaturalLanguageProcessing.html
https://cran.r-project.org/web/packages/tidytext/tidytext.pdf
https://cran.r-project.org/web/views/NaturalLanguageProcessing.html
https://cran.r-project.org/web/packages/quanteda/index.html
https://cran.r-project.org/web/packages/quanteda/vignettes/quickstart.html


https://rddj.info/
https://cloud.r-project.org/web/packages/hunspell/vignettes/intro.html
https://juliasilge.com/blog/life-changing-magic/
https://rud.is/projects/clinton_emails_01.html
 
https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html
https://www.tidytextmining.com/tidytext.html
https://yulab-smu.github.io/treedata-book/chapter2.html
http://www.stat.columbia.edu/~gelman/research/published/abandon_final.pdf

https://stackoverflow.com/questions/43598427/r-how-to-extract-items-from-xml-nodeset


```{r}

```


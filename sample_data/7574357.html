<!doctype html>
<html lang="en">
  <head>
    <title>US7574357B1 - Applications of sub-audible speech recognition based upon electromyographic signals 
        - Google Patents</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8">
    <meta name="referrer" content="origin-when-crossorigin">
    <link rel="canonical" href="https://patents.google.com/patent/US7574357B1/en">
    <meta name="description" content="
     Method and system for generating electromyographic or sub-audible signals (“SAWPs”) and for transmitting and recognizing the SAWPs that represent the original words and/or phrases. The SAWPs may be generated in an environment that interferes excessively with normal speech or that requires stealth communications, and may be transmitted using encoded, enciphered or otherwise transformed signals that are less subject to signal distortion or degradation in the ambient environment. 
   
   ">
    
    <meta name="DC.type" content="patent">
    
    <meta name="DC.title" content="Applications of sub-audible speech recognition based upon electromyographic signals 
       ">
    
    <meta name="DC.date" content="2005-06-24" scheme="dateSubmitted">
    
    <meta name="DC.description" content="
     Method and system for generating electromyographic or sub-audible signals (“SAWPs”) and for transmitting and recognizing the SAWPs that represent the original words and/or phrases. The SAWPs may be generated in an environment that interferes excessively with normal speech or that requires stealth communications, and may be transmitted using encoded, enciphered or otherwise transformed signals that are less subject to signal distortion or degradation in the ambient environment. 
   
   ">
    
    <meta name="citation_patent_application_number" content="US:11/169,265">
    
    <meta name="citation_pdf_url" content="https://patentimages.storage.googleapis.com/bf/91/24/23d3bb61aabdb9/US7574357.pdf">
    
    <meta name="citation_patent_number" content="US:7574357">
    
    <meta name="DC.date" content="2009-08-11" scheme="issue">
    
    <meta name="DC.contributor" content="C. Charles Jorgensen" scheme="inventor">
    
    <meta name="DC.contributor" content="Bradley J. Betts" scheme="inventor">
    
    <meta name="DC.contributor" content="National Aeronautics and Space Administration (NASA)" scheme="assignee">
    
    <meta name="DC.relation" content="US:6182039" scheme="references">
    
    <meta name="DC.relation" content="US:6366908" scheme="references">
    
    <meta name="DC.relation" content="US:6151571" scheme="references">
    
    <meta name="DC.relation" content="US:20040044517:A1" scheme="references">
    
    <meta name="DC.relation" content="US:20060129394:A1" scheme="references">
    
    <meta name="citation_reference" content="Brady, et al., Multisensor MELPe Using Parameter Substitution, IEEE International Conference on Acoustics, Speech, and Signal Processing, May 17-21, 2004, I-477-480, 1, IEEE." scheme="references">
    
    <meta name="citation_reference" content="Graciarena, et al., Combining Standard and Throat Microphones for Robust Speech Recognition, IEEE Signal Processing Letters, Mar. 2003, 72-74, 10-3, IEEE." scheme="references">
    
    <meta name="citation_reference" content="Jou, et al., Whispery Speech Recognition Using Adapted Articulatory, IEEE International Conference on Acoustics, Speech, and Signal Processing, Mar. 18-23, 2005, 1009-1012, 1, IEEE." scheme="references">
    
    <meta name="citation_reference" content="Junqua, et al., The Lombard Effect: A Reflex to Better Communicate With Others in Noise, IEEE Internationl Conference on Acoustics, Speech, and Signal Processing, Mar. 15-19, 1999, 2083-2086, 4, IEEE." scheme="references">
    
    <meta name="citation_reference" content="Manabe et al. &#39;Unvoiced Speech Recognition using EMG-Mime Speech Recognition-&#39;, CHI &#39;03 extended abstracts on Human factors in computing systems, Ft. Lauderdale, Florida, pp. 794-795." scheme="references">
    
    <meta name="citation_reference" content="NG, et al., Denoising of Human Speech Using Combined Acoustic and EM Sensorsignal Processing, IEEE International Conference on Acoustics, Speech, and Signal Processing, Jun. 5-9, 2000, 229-232, 1, IEEE." scheme="references">
    
    <meta name="citation_reference" content="Shahina, et al., Language Identification in Noisy Environments Using Throat Microphone Signals, Conference on Intelligent Sensing and Information Processing, Jan. 4, 2005, 400-403, IEEE." scheme="references">
    
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Product+Sans">
    <style>
      body { transition: none; }
    </style>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-27188110-4', 'auto');

      version = 'patent-search.search_20191120_RC00';

      function sendFeedback() {
        userfeedback.api.startFeedback({
          'productId': '713680',
          'bucket': 'patent-search-web',
          'productVersion': version,
        });
      }

      window.experiments = {};
      window.experiments.patentCountries = "ae,ag,al,am,ao,ap,ar,at,au,aw,az,ba,bb,bd,be,bf,bg,bh,bj,bn,bo,br,bw,bx,by,bz,ca,cf,cg,ch,ci,cl,cm,cn,co,cr,cs,cu,cy,cz,dd,de,dj,dk,dm,do,dz,ea,ec,ee,eg,em,ep,es,fi,fr,ga,gb,gc,gd,ge,gh,gm,gn,gq,gr,gt,gw,hk,hn,hr,hu,ib,id,ie,il,in,ir,is,it,jo,jp,ke,kg,kh,km,kn,kp,kr,kw,kz,la,lc,li,lk,lr,ls,lt,lu,lv,ly,ma,mc,md,me,mg,mk,ml,mn,mo,mr,mt,mw,mx,my,mz,na,ne,ng,ni,nl,no,nz,oa,om,pa,pe,pg,ph,pl,pt,py,qa,ro,rs,ru,rw,sa,sc,sd,se,sg,si,sk,sl,sm,sn,st,su,sv,sy,sz,td,tg,th,tj,tm,tn,tr,tt,tw,tz,ua,ug,us,uy,uz,vc,ve,vn,wo,yu,za,zm,zw";
      
      
      window.profilePicture = "";

      window.Polymer = {
        dom: 'shady',
        lazyRegister: true,
      };
    </script>

    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20191120_RC00/scs/compiled_dir/webcomponentsjs/webcomponents-lite.min.js"></script>
    
    <link rel="import" href="//www.gstatic.com/patent-search/frontend/patent-search.search_20191120_RC00/scs/compiled_dir/search-app-vulcanized.html">
    
  </head>
  <body unresolved>
    
    <script src="//www.gstatic.com/patent-search/frontend/patent-search.search_20191120_RC00/scs/compiled_dir/search-app-vulcanized.js"></script>
    
    <search-app>
      
      

      <article class="result" itemscope itemtype="http://schema.org/ScholarlyArticle">
  <h1 itemprop="pageTitle">US7574357B1 - Applications of sub-audible speech recognition based upon electromyographic signals 
        - Google Patents</h1>
  <span itemprop="title">Applications of sub-audible speech recognition based upon electromyographic signals 
       </span>

  <meta itemprop="type" content="patent">
  <a href="https://patentimages.storage.googleapis.com/bf/91/24/23d3bb61aabdb9/US7574357.pdf" itemprop="pdfLink">Download PDF</a>
  <h2>Info</h2>

  <dl>
    <dt>Publication number</dt>
    <dd itemprop="publicationNumber">US7574357B1</dd>
    <meta itemprop="numberWithoutCodes" content="7574357">
    <meta itemprop="kindCode" content="B1">
    <meta itemprop="publicationDescription" content="Patent ( no pre-grant publication)">
    
    <span>US7574357B1</span>
    
    <span>US11/169,265</span>
    
    <span>US16926505A</span>
    
    <span>US7574357B1</span>
    
    <span>US 7574357 B1</span>
    
    <span>US7574357 B1</span>
    
    <span>US 7574357B1</span>
    
    <span>  </span>
    
    <span> </span>
    
    <span> </span>
    
    <span>US 16926505 A</span>
    
    <span>US16926505 A</span>
    
    <span>US 16926505A</span>
    
    <span>US 7574357 B1</span>
    
    <span>US7574357 B1</span>
    
    <span>US 7574357B1</span>
    

    <dt>Authority</dt>
    <dd itemprop="countryCode">US</dd>
    <dd itemprop="countryName">United States</dd>

    <dt>Prior art keywords</dt>
    
    <dd itemprop="priorArtKeywords" repeat>sawp</dd>
    <dd itemprop="priorArtKeywords" repeat>word</dd>
    <dd itemprop="priorArtKeywords" repeat>matrix</dd>
    <dd itemprop="priorArtKeywords" repeat>values</dd>
    <dd itemprop="priorArtKeywords" repeat>number</dd>

    <dt>Prior art date</dt>
    <dd><time itemprop="priorArtDate" datetime="2005-06-24">2005-06-24</time></dd>

    <dt>Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)</dt>
    <dd itemprop="legalStatusIfi" itemscope>
      <span itemprop="status">Active</span>, expires <time itemprop="expiration" datetime="2026-11-09">2026-11-09</time>
    </dd>
  </dl>

  <dt>Application number</dt>
  <dd itemprop="applicationNumber">US11/169,265</dd>

  

  

  <dt>Inventor</dt>
  <dd itemprop="inventor" repeat>C. Charles Jorgensen</dd>
  <dd itemprop="inventor" repeat>Bradley J. Betts</dd>
  <dt>Current Assignee (The listed assignees may be inaccurate. Google has not performed a legal analysis and makes no representation or warranty as to the accuracy of the list.)</dt>
  <dd itemprop="assigneeCurrent" repeat>
    Jorgensen Charles C
  </dd>

  <dt>Original Assignee</dt>
  <dd itemprop="assigneeOriginal" repeat>National Aeronautics and Space Administration (NASA)</dd>

  <dt>Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.)</dt>
  <dd><time itemprop="priorityDate" datetime="2005-06-24">2005-06-24</time></dd>

  <dt>Filing date</dt>
  <dd><time itemprop="filingDate" datetime="2005-06-24">2005-06-24</time></dd>

  <dt>Publication date</dt>
  <dd><time itemprop="publicationDate" datetime="2009-08-11">2009-08-11</time></dd>

  

  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2005-06-24">2005-06-24</time>
    <span itemprop="title">Application filed by National Aeronautics and Space Administration (NASA)</span>
    <span itemprop="type">filed</span>
    
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    
    <span itemprop="assigneeSearch">National Aeronautics and Space Administration (NASA)</span>
    
    
  </dd>
  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2005-06-24">2005-06-24</time>
    <span itemprop="title">Priority to US11/169,265</span>
    <span itemprop="type">priority</span>
    
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    <span itemprop="documentId">patent/US7574357B1/en</span>
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2006-05-09">2006-05-09</time>
    <span itemprop="title">Assigned to USA AS REPRESENTED BY THE ADMINISTRATOR OF THE NASA</span>
    <span itemprop="type">reassignment</span>
    
    
    
    
    <span itemprop="assigneeSearch">USA AS REPRESENTED BY THE ADMINISTRATOR OF THE NASA</span>
    
    
    <span itemprop="description" repeat>ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS).</span>
    
    <span itemprop="description" repeat>Assignors: JORGENSEN, CHARLES C.</span>
    
  </dd>
  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2008-06-04">2008-06-04</time>
    <span itemprop="title">Assigned to USA AS REPRESENTED BY THE ADMINISTRATOR OF THE NASA</span>
    <span itemprop="type">reassignment</span>
    
    
    
    
    <span itemprop="assigneeSearch">USA AS REPRESENTED BY THE ADMINISTRATOR OF THE NASA</span>
    
    
    <span itemprop="description" repeat>ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS).</span>
    
    <span itemprop="description" repeat>Assignors: COMPUTER SCIENCE CORPORATION</span>
    
  </dd>
  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2009-08-11">2009-08-11</time>
    <span itemprop="title">Application granted</span>
    <span itemprop="type">granted</span>
    
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2009-08-11">2009-08-11</time>
    <span itemprop="title">Publication of US7574357B1</span>
    <span itemprop="type">publication</span>
    
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    <span itemprop="documentId">patent/US7574357B1/en</span>
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2017-06-05">2017-06-05</time>
    <span itemprop="title">Assigned to JORGENSEN, CHARLES C.</span>
    <span itemprop="type">reassignment</span>
    
    
    
    
    <span itemprop="assigneeSearch">JORGENSEN, CHARLES C.</span>
    
    
    <span itemprop="description" repeat>ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS).</span>
    
    <span itemprop="description" repeat>Assignors: USA AS REPRESENTED BY THE ADMINISTRATOR OF THE NASA</span>
    
  </dd>
  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2019-11-26">2019-11-26</time>
    <span itemprop="title">Application status is Active</span>
    <span itemprop="type">legal-status</span>
    
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    
    
  </dd>
  
  <dd itemprop="events" itemscope repeat>
    <time itemprop="date" datetime="2026-11-09">2026-11-09</time>
    <span itemprop="title">Adjusted expiration</span>
    <span itemprop="type">legal-status</span>
    
    <span itemprop="critical" content="true" bool>Critical</span>
    
    
    
    
    
  </dd>
  

  <h2>Links</h2>

  <ul>
    
          <li itemprop="links" itemscope repeat>
            <meta itemprop="id" content="usptoLink">
            <a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&p=1&u=/netahtml/PTO/srchnum.html&r=1&f=G&l=50&d=PALL&s1=7574357.PN." itemprop="url" target="_blank"><span itemprop="text">USPTO</span></a>
          </li>
        <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="usptoAssignmentLink">
          <a href="https://assignment.uspto.gov/patent/index.html#/patent/search/resultFilter?searchInput=7574357" itemprop="url" target="_blank"><span itemprop="text">USPTO Assignment</span></a>
        </li>

    <li itemprop="links" itemscope repeat>
        <meta itemprop="id" content="espacenetLink">
        <a href="http://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=7574357B1&amp;KC=B1&amp;FT=D" itemprop="url" target="_blank"><span itemprop="text">Espacenet</span></a>
      </li>
      

    

    
      <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="globalDossierLink">
          <a href="http://globaldossier.uspto.gov/#/result/patent/US/7574357/1" itemprop="url" target="_blank"><span itemprop="text">Global Dossier</span></a>
        </li>
      

      

      

      

      <li itemprop="links" itemscope repeat>
          <meta itemprop="id" content="stackexchangeLink">
          <a href="https://patents.stackexchange.com/questions/tagged/US7574357" itemprop="url"><span itemprop="text">Discuss</span></a>
        </li>
      
  </ul>

  
  <ul itemprop="concept" itemscope>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000015556</span>
      <span itemprop="name">catabolic process</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>abstract</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000006731</span>
      <span itemprop="name">degradation</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>abstract</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004059</span>
      <span itemprop="name">degradation</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>abstract</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000011159</span>
      <span itemprop="name">matrix materials</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">66</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001537</span>
      <span itemprop="name">neural</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">38</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000000034</span>
      <span itemprop="name">methods</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">31</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003213</span>
      <span itemprop="name">activating</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">19</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000875</span>
      <span itemprop="name">corresponding</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">18</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000010410</span>
      <span itemprop="name">layers</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">18</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004458</span>
      <span itemprop="name">analytical methods</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">16</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000009740</span>
      <span itemprop="name">moulding (composite fabrication)</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">9</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000011664</span>
      <span itemprop="name">signaling</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">6</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000047</span>
      <span itemprop="name">products</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">5</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001419</span>
      <span itemprop="name">dependent</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000006073</span>
      <span itemprop="name">displacement</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002123</span>
      <span itemprop="name">temporal effects</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001131</span>
      <span itemprop="name">transforming</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="count">3</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001702</span>
      <span itemprop="name">transmitter</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>claims</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004891</span>
      <span itemprop="name">communication</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">24</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000562</span>
      <span itemprop="name">conjugates</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">8</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000029058</span>
      <span itemprop="name">respiratory gaseous exchange</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">7</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">210000003205</span>
      <span itemprop="name">Muscles</span>
      <span itemprop="domain">Anatomy</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">6</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000003570</span>
      <span itemprop="name">air</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">6</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000383</span>
      <span itemprop="name">hazardous chemicals</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">6</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002829</span>
      <span itemprop="name">reduced</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">5</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">241000282414</span>
      <span itemprop="name">Homo sapiens</span>
      <span itemprop="domain">Species</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">210000000867</span>
      <span itemprop="name">Larynx</span>
      <span itemprop="domain">Anatomy</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002452</span>
      <span itemprop="name">interceptive</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000005365</span>
      <span itemprop="name">production</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004044</span>
      <span itemprop="name">response</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000001228</span>
      <span itemprop="name">spectrum</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">4</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">210000004556</span>
      <span itemprop="name">Brain</span>
      <span itemprop="domain">Anatomy</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">3</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">210000003800</span>
      <span itemprop="name">Pharynx</span>
      <span itemprop="domain">Anatomy</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">3</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004422</span>
      <span itemprop="name">calculation algorithm</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">3</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001965</span>
      <span itemprop="name">increased</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">3</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000015654</span>
      <span itemprop="name">memory</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">3</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003595</span>
      <span itemprop="name">spectral</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">3</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002588</span>
      <span itemprop="name">toxic</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">3</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">231100000331</span>
      <span itemprop="name">toxic</span>
      <span itemprop="domain">Toxicity</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">3</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">102100017828</span>
      <span itemprop="name">ASPRV1</span>
      <span itemprop="domain">Human genes</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">206010011224</span>
      <span itemprop="name">Cough</span>
      <span itemprop="domain">Diseases</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">108060007889</span>
      <span itemprop="name">SspB family</span>
      <span itemprop="domain">Proteins</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">102100015244</span>
      <span itemprop="name">TXN</span>
      <span itemprop="domain">Human genes</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">108060002105</span>
      <span itemprop="name">ThiO family</span>
      <span itemprop="domain">Proteins</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000006399</span>
      <span itemprop="name">behavior</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000015572</span>
      <span itemprop="name">biosynthetic process</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000694</span>
      <span itemprop="name">effects</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000005755</span>
      <span itemprop="name">formation</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000007789</span>
      <span itemprop="name">gases</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001976</span>
      <span itemprop="name">improved</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000013016</span>
      <span itemprop="name">learning</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000203</span>
      <span itemprop="name">mixtures</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000003909</span>
      <span itemprop="name">pattern recognition</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000007781</span>
      <span itemprop="name">pre-processing</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000005070</span>
      <span itemprop="name">sampling</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000035945</span>
      <span itemprop="name">sensitivity</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000010944</span>
      <span itemprop="name">silver (metal)</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000779</span>
      <span itemprop="name">smoke</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001755</span>
      <span itemprop="name">vocal</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">2</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">102100019805</span>
      <span itemprop="name">AGXT</span>
      <span itemprop="domain">Human genes</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">206010011878</span>
      <span itemprop="name">Deafness</span>
      <span itemprop="domain">Diseases</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">241001415961</span>
      <span itemprop="name">Gaviidae</span>
      <span itemprop="domain">Species</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">241000167880</span>
      <span itemprop="name">Hirundinidae</span>
      <span itemprop="domain">Species</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">241000282412</span>
      <span itemprop="name">Homo</span>
      <span itemprop="domain">Species</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">206010049565</span>
      <span itemprop="name">Muscle fatigue</span>
      <span itemprop="domain">Diseases</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">231100000614</span>
      <span itemprop="name">Poisons</span>
      <span itemprop="domain">Toxicity</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">101700001919</span>
      <span itemprop="name">SPT family</span>
      <span itemprop="domain">Proteins</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">101700047626</span>
      <span itemprop="name">SPYA family</span>
      <span itemprop="domain">Proteins</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">241000269400</span>
      <span itemprop="name">Sirenidae</span>
      <span itemprop="domain">Species</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">206010044565</span>
      <span itemprop="name">Tremor</span>
      <span itemprop="domain">Diseases</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">210000001260</span>
      <span itemprop="name">Vocal Cords</span>
      <span itemprop="domain">Anatomy</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">210000000707</span>
      <span itemprop="name">Wrist</span>
      <span itemprop="domain">Anatomy</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000009825</span>
      <span itemprop="name">accumulation</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000853</span>
      <span itemprop="name">adhesives</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003466</span>
      <span itemprop="name">anti-cipated</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">CURLTUGMZLYLDI-UHFFFAOYSA-N</span>
      <span itemprop="name">carbon dioxide</span>
      <span itemprop="domain">Chemical compound</span>
      <span itemprop="svg_large">data:image/svg&#43;xml;base64,PD94bWwgdmVyc2lvbj0nMS4wJyBlbmNvZGluZz0naXNvLTg4NTktMSc/Pgo8c3ZnIHZlcnNpb249JzEuMScgYmFzZVByb2ZpbGU9J2Z1bGwnCiAgICAgICAgICAgICAgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJwogICAgICAgICAgICAgICAgICAgICAgeG1sbnM6cmRraXQ9J2h0dHA6Ly93d3cucmRraXQub3JnL3htbCcKICAgICAgICAgICAgICAgICAgICAgIHhtbG5zOnhsaW5rPSdodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rJwogICAgICAgICAgICAgICAgICB4bWw6c3BhY2U9J3ByZXNlcnZlJwp3aWR0aD0nMzAwcHgnIGhlaWdodD0nMzAwcHgnID4KPCEtLSBFTkQgT0YgSEVBREVSIC0tPgo8cmVjdCBzdHlsZT0nb3BhY2l0eToxLjA7ZmlsbDojRkZGRkZGO3N0cm9rZTpub25lJyB3aWR0aD0nMzAwJyBoZWlnaHQ9JzMwMCcgeD0nMCcgeT0nMCc&#43;IDwvcmVjdD4KPHBhdGggY2xhc3M9J2JvbmQtMCcgZD0nTSAyNDYuOTg0LDEzOS42MDEgMTk4LjQ5MiwxMzkuNjAxJyBzdHlsZT0nZmlsbDpub25lO2ZpbGwtcnVsZTpldmVub2RkO3N0cm9rZTojRkYwMDAwO3N0cm9rZS13aWR0aDoycHg7c3Ryb2tlLWxpbmVjYXA6YnV0dDtzdHJva2UtbGluZWpvaW46bWl0ZXI7c3Ryb2tlLW9wYWNpdHk6MScgLz4KPHBhdGggY2xhc3M9J2JvbmQtMCcgZD0nTSAxOTguNDkyLDEzOS42MDEgMTUwLDEzOS42MDEnIHN0eWxlPSdmaWxsOm5vbmU7ZmlsbC1ydWxlOmV2ZW5vZGQ7c3Ryb2tlOiMwMDAwMDA7c3Ryb2tlLXdpZHRoOjJweDtzdHJva2UtbGluZWNhcDpidXR0O3N0cm9rZS1saW5lam9pbjptaXRlcjtzdHJva2Utb3BhY2l0eToxJyAvPgo8cGF0aCBjbGFzcz0nYm9uZC0wJyBkPSdNIDI0Ni45ODQsMTYwLjM5OSAxOTguNDkyLDE2MC4zOTknIHN0eWxlPSdmaWxsOm5vbmU7ZmlsbC1ydWxlOmV2ZW5vZGQ7c3Ryb2tlOiNGRjAwMDA7c3Ryb2tlLXdpZHRoOjJweDtzdHJva2UtbGluZWNhcDpidXR0O3N0cm9rZS1saW5lam9pbjptaXRlcjtzdHJva2Utb3BhY2l0eToxJyAvPgo8cGF0aCBjbGFzcz0nYm9uZC0wJyBkPSdNIDE5OC40OTIsMTYwLjM5OSAxNTAsMTYwLjM5OScgc3R5bGU9J2ZpbGw6bm9uZTtmaWxsLXJ1bGU6ZXZlbm9kZDtzdHJva2U6IzAwMDAwMDtzdHJva2Utd2lkdGg6MnB4O3N0cm9rZS1saW5lY2FwOmJ1dHQ7c3Ryb2tlLWxpbmVqb2luOm1pdGVyO3N0cm9rZS1vcGFjaXR5OjEnIC8&#43;CjxwYXRoIGNsYXNzPSdib25kLTEnIGQ9J00gMTUwLDEzOS42MDEgMTAxLjUwOCwxMzkuNjAxJyBzdHlsZT0nZmlsbDpub25lO2ZpbGwtcnVsZTpldmVub2RkO3N0cm9rZTojMDAwMDAwO3N0cm9rZS13aWR0aDoycHg7c3Ryb2tlLWxpbmVjYXA6YnV0dDtzdHJva2UtbGluZWpvaW46bWl0ZXI7c3Ryb2tlLW9wYWNpdHk6MScgLz4KPHBhdGggY2xhc3M9J2JvbmQtMScgZD0nTSAxMDEuNTA4LDEzOS42MDEgNTMuMDE1NiwxMzkuNjAxJyBzdHlsZT0nZmlsbDpub25lO2ZpbGwtcnVsZTpldmVub2RkO3N0cm9rZTojRkYwMDAwO3N0cm9rZS13aWR0aDoycHg7c3Ryb2tlLWxpbmVjYXA6YnV0dDtzdHJva2UtbGluZWpvaW46bWl0ZXI7c3Ryb2tlLW9wYWNpdHk6MScgLz4KPHBhdGggY2xhc3M9J2JvbmQtMScgZD0nTSAxNTAsMTYwLjM5OSAxMDEuNTA4LDE2MC4zOTknIHN0eWxlPSdmaWxsOm5vbmU7ZmlsbC1ydWxlOmV2ZW5vZGQ7c3Ryb2tlOiMwMDAwMDA7c3Ryb2tlLXdpZHRoOjJweDtzdHJva2UtbGluZWNhcDpidXR0O3N0cm9rZS1saW5lam9pbjptaXRlcjtzdHJva2Utb3BhY2l0eToxJyAvPgo8cGF0aCBjbGFzcz0nYm9uZC0xJyBkPSdNIDEwMS41MDgsMTYwLjM5OSA1My4wMTU2LDE2MC4zOTknIHN0eWxlPSdmaWxsOm5vbmU7ZmlsbC1ydWxlOmV2ZW5vZGQ7c3Ryb2tlOiNGRjAwMDA7c3Ryb2tlLXdpZHRoOjJweDtzdHJva2UtbGluZWNhcDpidXR0O3N0cm9rZS1saW5lam9pbjptaXRlcjtzdHJva2Utb3BhY2l0eToxJyAvPgo8dGV4dCB4PScyNDYuOTg0JyB5PScxNTcuNScgc3R5bGU9J2ZvbnQtc2l6ZToxNXB4O2ZvbnQtc3R5bGU6bm9ybWFsO2ZvbnQtd2VpZ2h0Om5vcm1hbDtmaWxsLW9wYWNpdHk6MTtzdHJva2U6bm9uZTtmb250LWZhbWlseTpzYW5zLXNlcmlmO3RleHQtYW5jaG9yOnN0YXJ0O2ZpbGw6I0ZGMDAwMCcgPjx0c3Bhbj5PPC90c3Bhbj48L3RleHQ&#43;Cjx0ZXh0IHg9JzM5LjAwNicgeT0nMTU3LjUnIHN0eWxlPSdmb250LXNpemU6MTVweDtmb250LXN0eWxlOm5vcm1hbDtmb250LXdlaWdodDpub3JtYWw7ZmlsbC1vcGFjaXR5OjE7c3Ryb2tlOm5vbmU7Zm9udC1mYW1pbHk6c2Fucy1zZXJpZjt0ZXh0LWFuY2hvcjpzdGFydDtmaWxsOiNGRjAwMDAnID48dHNwYW4&#43;TzwvdHNwYW4&#43;PC90ZXh0Pgo8L3N2Zz4K</span>
      <span itemprop="svg_small">data:image/svg&#43;xml;base64,PD94bWwgdmVyc2lvbj0nMS4wJyBlbmNvZGluZz0naXNvLTg4NTktMSc/Pgo8c3ZnIHZlcnNpb249JzEuMScgYmFzZVByb2ZpbGU9J2Z1bGwnCiAgICAgICAgICAgICAgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJwogICAgICAgICAgICAgICAgICAgICAgeG1sbnM6cmRraXQ9J2h0dHA6Ly93d3cucmRraXQub3JnL3htbCcKICAgICAgICAgICAgICAgICAgICAgIHhtbG5zOnhsaW5rPSdodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rJwogICAgICAgICAgICAgICAgICB4bWw6c3BhY2U9J3ByZXNlcnZlJwp3aWR0aD0nODVweCcgaGVpZ2h0PSc4NXB4JyA&#43;CjwhLS0gRU5EIE9GIEhFQURFUiAtLT4KPHJlY3Qgc3R5bGU9J29wYWNpdHk6MS4wO2ZpbGw6I0ZGRkZGRjtzdHJva2U6bm9uZScgd2lkdGg9Jzg1JyBoZWlnaHQ9Jzg1JyB4PScwJyB5PScwJz4gPC9yZWN0Pgo8cGF0aCBjbGFzcz0nYm9uZC0wJyBkPSdNIDY2Ljg3NzIsMzkuMDUzNiA1NC40Mzg2LDM5LjA1MzYnIHN0eWxlPSdmaWxsOm5vbmU7ZmlsbC1ydWxlOmV2ZW5vZGQ7c3Ryb2tlOiNGRjAwMDA7c3Ryb2tlLXdpZHRoOjJweDtzdHJva2UtbGluZWNhcDpidXR0O3N0cm9rZS1saW5lam9pbjptaXRlcjtzdHJva2Utb3BhY2l0eToxJyAvPgo8cGF0aCBjbGFzcz0nYm9uZC0wJyBkPSdNIDU0LjQzODYsMzkuMDUzNiA0MiwzOS4wNTM2JyBzdHlsZT0nZmlsbDpub25lO2ZpbGwtcnVsZTpldmVub2RkO3N0cm9rZTojMDAwMDAwO3N0cm9rZS13aWR0aDoycHg7c3Ryb2tlLWxpbmVjYXA6YnV0dDtzdHJva2UtbGluZWpvaW46bWl0ZXI7c3Ryb2tlLW9wYWNpdHk6MScgLz4KPHBhdGggY2xhc3M9J2JvbmQtMCcgZD0nTSA2Ni44NzcyLDQ0Ljk0NjQgNTQuNDM4Niw0NC45NDY0JyBzdHlsZT0nZmlsbDpub25lO2ZpbGwtcnVsZTpldmVub2RkO3N0cm9rZTojRkYwMDAwO3N0cm9rZS13aWR0aDoycHg7c3Ryb2tlLWxpbmVjYXA6YnV0dDtzdHJva2UtbGluZWpvaW46bWl0ZXI7c3Ryb2tlLW9wYWNpdHk6MScgLz4KPHBhdGggY2xhc3M9J2JvbmQtMCcgZD0nTSA1NC40Mzg2LDQ0Ljk0NjQgNDIsNDQuOTQ2NCcgc3R5bGU9J2ZpbGw6bm9uZTtmaWxsLXJ1bGU6ZXZlbm9kZDtzdHJva2U6IzAwMDAwMDtzdHJva2Utd2lkdGg6MnB4O3N0cm9rZS1saW5lY2FwOmJ1dHQ7c3Ryb2tlLWxpbmVqb2luOm1pdGVyO3N0cm9rZS1vcGFjaXR5OjEnIC8&#43;CjxwYXRoIGNsYXNzPSdib25kLTEnIGQ9J00gNDIsMzkuMDUzNiAyOS41NjE0LDM5LjA1MzYnIHN0eWxlPSdmaWxsOm5vbmU7ZmlsbC1ydWxlOmV2ZW5vZGQ7c3Ryb2tlOiMwMDAwMDA7c3Ryb2tlLXdpZHRoOjJweDtzdHJva2UtbGluZWNhcDpidXR0O3N0cm9rZS1saW5lam9pbjptaXRlcjtzdHJva2Utb3BhY2l0eToxJyAvPgo8cGF0aCBjbGFzcz0nYm9uZC0xJyBkPSdNIDI5LjU2MTQsMzkuMDUzNiAxNy4xMjI4LDM5LjA1MzYnIHN0eWxlPSdmaWxsOm5vbmU7ZmlsbC1ydWxlOmV2ZW5vZGQ7c3Ryb2tlOiNGRjAwMDA7c3Ryb2tlLXdpZHRoOjJweDtzdHJva2UtbGluZWNhcDpidXR0O3N0cm9rZS1saW5lam9pbjptaXRlcjtzdHJva2Utb3BhY2l0eToxJyAvPgo8cGF0aCBjbGFzcz0nYm9uZC0xJyBkPSdNIDQyLDQ0Ljk0NjQgMjkuNTYxNCw0NC45NDY0JyBzdHlsZT0nZmlsbDpub25lO2ZpbGwtcnVsZTpldmVub2RkO3N0cm9rZTojMDAwMDAwO3N0cm9rZS13aWR0aDoycHg7c3Ryb2tlLWxpbmVjYXA6YnV0dDtzdHJva2UtbGluZWpvaW46bWl0ZXI7c3Ryb2tlLW9wYWNpdHk6MScgLz4KPHBhdGggY2xhc3M9J2JvbmQtMScgZD0nTSAyOS41NjE0LDQ0Ljk0NjQgMTcuMTIyOCw0NC45NDY0JyBzdHlsZT0nZmlsbDpub25lO2ZpbGwtcnVsZTpldmVub2RkO3N0cm9rZTojRkYwMDAwO3N0cm9rZS13aWR0aDoycHg7c3Ryb2tlLWxpbmVjYXA6YnV0dDtzdHJva2UtbGluZWpvaW46bWl0ZXI7c3Ryb2tlLW9wYWNpdHk6MScgLz4KPHRleHQgeD0nNjYuODc3MicgeT0nNDYuOTEwNicgc3R5bGU9J2ZvbnQtc2l6ZTo5cHg7Zm9udC1zdHlsZTpub3JtYWw7Zm9udC13ZWlnaHQ6bm9ybWFsO2ZpbGwtb3BhY2l0eToxO3N0cm9rZTpub25lO2ZvbnQtZmFtaWx5OnNhbnMtc2VyaWY7dGV4dC1hbmNob3I6c3RhcnQ7ZmlsbDojRkYwMDAwJyA&#43;PHRzcGFuPk88L3RzcGFuPjwvdGV4dD4KPHRleHQgeD0nNy45NTAwMScgeT0nNDYuOTEwNicgc3R5bGU9J2ZvbnQtc2l6ZTo5cHg7Zm9udC1zdHlsZTpub3JtYWw7Zm9udC13ZWlnaHQ6bm9ybWFsO2ZpbGwtb3BhY2l0eToxO3N0cm9rZTpub25lO2ZvbnQtZmFtaWx5OnNhbnMtc2VyaWY7dGV4dC1hbmNob3I6c3RhcnQ7ZmlsbDojRkYwMDAwJyA&#43;PHRzcGFuPk88L3RzcGFuPjwvdGV4dD4KPC9zdmc&#43;Cg==</span>
      <span itemprop="smiles">O=C=O</span>
      <span itemprop="inchi_key">CURLTUGMZLYLDI-UHFFFAOYSA-N</span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">229910002092</span>
      <span itemprop="name">carbon dioxides</span>
      <span itemprop="domain">Inorganic materials</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001413</span>
      <span itemprop="name">cellular</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">125000001309</span>
      <span itemprop="name">chloro group</span>
      <span itemprop="domain">Chemical group</span>
      <span itemprop="svg_large">data:image/svg&#43;xml;base64,PD94bWwgdmVyc2lvbj0nMS4wJyBlbmNvZGluZz0naXNvLTg4NTktMSc/Pgo8c3ZnIHZlcnNpb249JzEuMScgYmFzZVByb2ZpbGU9J2Z1bGwnCiAgICAgICAgICAgICAgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJwogICAgICAgICAgICAgICAgICAgICAgeG1sbnM6cmRraXQ9J2h0dHA6Ly93d3cucmRraXQub3JnL3htbCcKICAgICAgICAgICAgICAgICAgICAgIHhtbG5zOnhsaW5rPSdodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rJwogICAgICAgICAgICAgICAgICB4bWw6c3BhY2U9J3ByZXNlcnZlJwp3aWR0aD0nMzAwcHgnIGhlaWdodD0nMzAwcHgnID4KPCEtLSBFTkQgT0YgSEVBREVSIC0tPgo8cmVjdCBzdHlsZT0nb3BhY2l0eToxLjA7ZmlsbDojRkZGRkZGO3N0cm9rZTpub25lJyB3aWR0aD0nMzAwJyBoZWlnaHQ9JzMwMCcgeD0nMCcgeT0nMCc&#43;IDwvcmVjdD4KPHBhdGggY2xhc3M9J2JvbmQtMCcgZD0nTSAyMTAuNjc5LDE1MCAxMjcuNzUyLDE1MCcgc3R5bGU9J2ZpbGw6bm9uZTtmaWxsLXJ1bGU6ZXZlbm9kZDtzdHJva2U6IzAwQ0MwMDtzdHJva2Utd2lkdGg6MnB4O3N0cm9rZS1saW5lY2FwOmJ1dHQ7c3Ryb2tlLWxpbmVqb2luOm1pdGVyO3N0cm9rZS1vcGFjaXR5OjEnIC8&#43;CjxwYXRoIGNsYXNzPSdib25kLTAnIGQ9J00gMTI3Ljc1MiwxNTAgNDQuODI0MywxNTAnIHN0eWxlPSdmaWxsOm5vbmU7ZmlsbC1ydWxlOmV2ZW5vZGQ7c3Ryb2tlOiM3RjdGN0Y7c3Ryb2tlLXdpZHRoOjJweDtzdHJva2UtbGluZWNhcDpidXR0O3N0cm9rZS1saW5lam9pbjptaXRlcjtzdHJva2Utb3BhY2l0eToxJyAvPgo8dGV4dCB4PScyMTAuNjc5JyB5PScxNTcuNScgc3R5bGU9J2ZvbnQtc2l6ZToxNHB4O2ZvbnQtc3R5bGU6bm9ybWFsO2ZvbnQtd2VpZ2h0Om5vcm1hbDtmaWxsLW9wYWNpdHk6MTtzdHJva2U6bm9uZTtmb250LWZhbWlseTpzYW5zLXNlcmlmO3RleHQtYW5jaG9yOnN0YXJ0O2ZpbGw6IzAwQ0MwMCcgPjx0c3Bhbj5DbDwvdHNwYW4&#43;PC90ZXh0Pgo8dGV4dCB4PSczNy44MTk1JyB5PScxNTcuNScgc3R5bGU9J2ZvbnQtc2l6ZToxNHB4O2ZvbnQtc3R5bGU6bm9ybWFsO2ZvbnQtd2VpZ2h0Om5vcm1hbDtmaWxsLW9wYWNpdHk6MTtzdHJva2U6bm9uZTtmb250LWZhbWlseTpzYW5zLXNlcmlmO3RleHQtYW5jaG9yOnN0YXJ0O2ZpbGw6IzdGN0Y3RicgPjx0c3Bhbj4qPC90c3Bhbj48L3RleHQ&#43;Cjwvc3ZnPgo=</span>
      <span itemprop="svg_small">data:image/svg&#43;xml;base64,PD94bWwgdmVyc2lvbj0nMS4wJyBlbmNvZGluZz0naXNvLTg4NTktMSc/Pgo8c3ZnIHZlcnNpb249JzEuMScgYmFzZVByb2ZpbGU9J2Z1bGwnCiAgICAgICAgICAgICAgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJwogICAgICAgICAgICAgICAgICAgICAgeG1sbnM6cmRraXQ9J2h0dHA6Ly93d3cucmRraXQub3JnL3htbCcKICAgICAgICAgICAgICAgICAgICAgIHhtbG5zOnhsaW5rPSdodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rJwogICAgICAgICAgICAgICAgICB4bWw6c3BhY2U9J3ByZXNlcnZlJwp3aWR0aD0nODVweCcgaGVpZ2h0PSc4NXB4JyA&#43;CjwhLS0gRU5EIE9GIEhFQURFUiAtLT4KPHJlY3Qgc3R5bGU9J29wYWNpdHk6MS4wO2ZpbGw6I0ZGRkZGRjtzdHJva2U6bm9uZScgd2lkdGg9Jzg1JyBoZWlnaHQ9Jzg1JyB4PScwJyB5PScwJz4gPC9yZWN0Pgo8cGF0aCBjbGFzcz0nYm9uZC0wJyBkPSdNIDUyLjA4MjYsNDIgMzMuNjA2Myw0Micgc3R5bGU9J2ZpbGw6bm9uZTtmaWxsLXJ1bGU6ZXZlbm9kZDtzdHJva2U6IzAwQ0MwMDtzdHJva2Utd2lkdGg6MnB4O3N0cm9rZS1saW5lY2FwOmJ1dHQ7c3Ryb2tlLWxpbmVqb2luOm1pdGVyO3N0cm9rZS1vcGFjaXR5OjEnIC8&#43;CjxwYXRoIGNsYXNzPSdib25kLTAnIGQ9J00gMzMuNjA2Myw0MiAxNS4xMyw0Micgc3R5bGU9J2ZpbGw6bm9uZTtmaWxsLXJ1bGU6ZXZlbm9kZDtzdHJva2U6IzdGN0Y3RjtzdHJva2Utd2lkdGg6MnB4O3N0cm9rZS1saW5lY2FwOmJ1dHQ7c3Ryb2tlLWxpbmVqb2luOm1pdGVyO3N0cm9rZS1vcGFjaXR5OjEnIC8&#43;Cjx0ZXh0IHg9JzUyLjA4MjYnIHk9JzUwLjM5ODgnIHN0eWxlPSdmb250LXNpemU6MTZweDtmb250LXN0eWxlOm5vcm1hbDtmb250LXdlaWdodDpub3JtYWw7ZmlsbC1vcGFjaXR5OjE7c3Ryb2tlOm5vbmU7Zm9udC1mYW1pbHk6c2Fucy1zZXJpZjt0ZXh0LWFuY2hvcjpzdGFydDtmaWxsOiMwMENDMDAnID48dHNwYW4&#43;Q2w8L3RzcGFuPjwvdGV4dD4KPHRleHQgeD0nNy4yODU3NScgeT0nNTAuMzk4OCcgc3R5bGU9J2ZvbnQtc2l6ZToxNnB4O2ZvbnQtc3R5bGU6bm9ybWFsO2ZvbnQtd2VpZ2h0Om5vcm1hbDtmaWxsLW9wYWNpdHk6MTtzdHJva2U6bm9uZTtmb250LWZhbWlseTpzYW5zLXNlcmlmO3RleHQtYW5jaG9yOnN0YXJ0O2ZpbGw6IzdGN0Y3RicgPjx0c3Bhbj4qPC90c3Bhbj48L3RleHQ&#43;Cjwvc3ZnPgo=</span>
      <span itemprop="smiles">Cl*</span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">231100000078</span>
      <span itemprop="name">corrosive</span>
      <span itemprop="domain">Toxicity</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002354</span>
      <span itemprop="name">daily</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000003247</span>
      <span itemprop="name">decreasing</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000018109</span>
      <span itemprop="name">developmental process</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000005516</span>
      <span itemprop="name">engineering processes</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000003517</span>
      <span itemprop="name">fumes</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000014509</span>
      <span itemprop="name">gene expression</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000036541</span>
      <span itemprop="name">health</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000004301</span>
      <span itemprop="name">light adaptation</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000003012</span>
      <span itemprop="name">network analysis</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">210000000056</span>
      <span itemprop="name">organs</span>
      <span itemprop="domain">Anatomy</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">MYMOFIZGZYHOMD-UHFFFAOYSA-N</span>
      <span itemprop="name">oxygen</span>
      <span itemprop="domain">Chemical compound</span>
      <span itemprop="svg_large">data:image/svg&#43;xml;base64,PD94bWwgdmVyc2lvbj0nMS4wJyBlbmNvZGluZz0naXNvLTg4NTktMSc/Pgo8c3ZnIHZlcnNpb249JzEuMScgYmFzZVByb2ZpbGU9J2Z1bGwnCiAgICAgICAgICAgICAgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJwogICAgICAgICAgICAgICAgICAgICAgeG1sbnM6cmRraXQ9J2h0dHA6Ly93d3cucmRraXQub3JnL3htbCcKICAgICAgICAgICAgICAgICAgICAgIHhtbG5zOnhsaW5rPSdodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rJwogICAgICAgICAgICAgICAgICB4bWw6c3BhY2U9J3ByZXNlcnZlJwp3aWR0aD0nMzAwcHgnIGhlaWdodD0nMzAwcHgnID4KPCEtLSBFTkQgT0YgSEVBREVSIC0tPgo8cmVjdCBzdHlsZT0nb3BhY2l0eToxLjA7ZmlsbDojRkZGRkZGO3N0cm9rZTpub25lJyB3aWR0aD0nMzAwJyBoZWlnaHQ9JzMwMCcgeD0nMCcgeT0nMCc&#43;IDwvcmVjdD4KPHBhdGggY2xhc3M9J2JvbmQtMCcgZD0nTSA3Mi45NjcxLDE2Ni44MDggMjI3LjAzMywxNjYuODA4JyBzdHlsZT0nZmlsbDpub25lO2ZpbGwtcnVsZTpldmVub2RkO3N0cm9rZTojRkYwMDAwO3N0cm9rZS13aWR0aDoycHg7c3Ryb2tlLWxpbmVjYXA6YnV0dDtzdHJva2UtbGluZWpvaW46bWl0ZXI7c3Ryb2tlLW9wYWNpdHk6MScgLz4KPHBhdGggY2xhc3M9J2JvbmQtMCcgZD0nTSA3Mi45NjcxLDEzMy4xOTIgMjI3LjAzMywxMzMuMTkyJyBzdHlsZT0nZmlsbDpub25lO2ZpbGwtcnVsZTpldmVub2RkO3N0cm9rZTojRkYwMDAwO3N0cm9rZS13aWR0aDoycHg7c3Ryb2tlLWxpbmVjYXA6YnV0dDtzdHJva2UtbGluZWpvaW46bWl0ZXI7c3Ryb2tlLW9wYWNpdHk6MScgLz4KPHRleHQgeD0nNTguOTU3NScgeT0nMTU3LjUnIHN0eWxlPSdmb250LXNpemU6MTVweDtmb250LXN0eWxlOm5vcm1hbDtmb250LXdlaWdodDpub3JtYWw7ZmlsbC1vcGFjaXR5OjE7c3Ryb2tlOm5vbmU7Zm9udC1mYW1pbHk6c2Fucy1zZXJpZjt0ZXh0LWFuY2hvcjpzdGFydDtmaWxsOiNGRjAwMDAnID48dHNwYW4&#43;TzwvdHNwYW4&#43;PC90ZXh0Pgo8dGV4dCB4PScyMjcuMDMzJyB5PScxNTcuNScgc3R5bGU9J2ZvbnQtc2l6ZToxNXB4O2ZvbnQtc3R5bGU6bm9ybWFsO2ZvbnQtd2VpZ2h0Om5vcm1hbDtmaWxsLW9wYWNpdHk6MTtzdHJva2U6bm9uZTtmb250LWZhbWlseTpzYW5zLXNlcmlmO3RleHQtYW5jaG9yOnN0YXJ0O2ZpbGw6I0ZGMDAwMCcgPjx0c3Bhbj5PPC90c3Bhbj48L3RleHQ&#43;Cjwvc3ZnPgo=</span>
      <span itemprop="svg_small">data:image/svg&#43;xml;base64,PD94bWwgdmVyc2lvbj0nMS4wJyBlbmNvZGluZz0naXNvLTg4NTktMSc/Pgo8c3ZnIHZlcnNpb249JzEuMScgYmFzZVByb2ZpbGU9J2Z1bGwnCiAgICAgICAgICAgICAgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJwogICAgICAgICAgICAgICAgICAgICAgeG1sbnM6cmRraXQ9J2h0dHA6Ly93d3cucmRraXQub3JnL3htbCcKICAgICAgICAgICAgICAgICAgICAgIHhtbG5zOnhsaW5rPSdodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rJwogICAgICAgICAgICAgICAgICB4bWw6c3BhY2U9J3ByZXNlcnZlJwp3aWR0aD0nODVweCcgaGVpZ2h0PSc4NXB4JyA&#43;CjwhLS0gRU5EIE9GIEhFQURFUiAtLT4KPHJlY3Qgc3R5bGU9J29wYWNpdHk6MS4wO2ZpbGw6I0ZGRkZGRjtzdHJva2U6bm9uZScgd2lkdGg9Jzg1JyBoZWlnaHQ9Jzg1JyB4PScwJyB5PScwJz4gPC9yZWN0Pgo8cGF0aCBjbGFzcz0nYm9uZC0wJyBkPSdNIDI1LjYwMjIsNDYuNzYyMSA1OC4zOTc4LDQ2Ljc2MjEnIHN0eWxlPSdmaWxsOm5vbmU7ZmlsbC1ydWxlOmV2ZW5vZGQ7c3Ryb2tlOiNGRjAwMDA7c3Ryb2tlLXdpZHRoOjJweDtzdHJva2UtbGluZWNhcDpidXR0O3N0cm9rZS1saW5lam9pbjptaXRlcjtzdHJva2Utb3BhY2l0eToxJyAvPgo8cGF0aCBjbGFzcz0nYm9uZC0wJyBkPSdNIDI1LjYwMjIsMzcuMjM3OSA1OC4zOTc4LDM3LjIzNzknIHN0eWxlPSdmaWxsOm5vbmU7ZmlsbC1ydWxlOmV2ZW5vZGQ7c3Ryb2tlOiNGRjAwMDA7c3Ryb2tlLXdpZHRoOjJweDtzdHJva2UtbGluZWNhcDpidXR0O3N0cm9rZS1saW5lam9pbjptaXRlcjtzdHJva2Utb3BhY2l0eToxJyAvPgo8dGV4dCB4PScxMC43NzY1JyB5PSc0OS45MzY5JyBzdHlsZT0nZm9udC1zaXplOjE1cHg7Zm9udC1zdHlsZTpub3JtYWw7Zm9udC13ZWlnaHQ6bm9ybWFsO2ZpbGwtb3BhY2l0eToxO3N0cm9rZTpub25lO2ZvbnQtZmFtaWx5OnNhbnMtc2VyaWY7dGV4dC1hbmNob3I6c3RhcnQ7ZmlsbDojRkYwMDAwJyA&#43;PHRzcGFuPk88L3RzcGFuPjwvdGV4dD4KPHRleHQgeD0nNTguMzk3OCcgeT0nNDkuOTM2OScgc3R5bGU9J2ZvbnQtc2l6ZToxNXB4O2ZvbnQtc3R5bGU6bm9ybWFsO2ZvbnQtd2VpZ2h0Om5vcm1hbDtmaWxsLW9wYWNpdHk6MTtzdHJva2U6bm9uZTtmb250LWZhbWlseTpzYW5zLXNlcmlmO3RleHQtYW5jaG9yOnN0YXJ0O2ZpbGw6I0ZGMDAwMCcgPjx0c3Bhbj5PPC90c3Bhbj48L3RleHQ&#43;Cjwvc3ZnPgo=</span>
      <span itemprop="smiles">O=O</span>
      <span itemprop="inchi_key">MYMOFIZGZYHOMD-UHFFFAOYSA-N</span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">229910052760</span>
      <span itemprop="name">oxygen</span>
      <span itemprop="domain">Inorganic materials</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000001301</span>
      <span itemprop="name">oxygen</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">229920001690</span>
      <span itemprop="name">polydopamine</span>
      <span itemprop="domain">Polymers</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000001965</span>
      <span itemprop="name">potato dextrose agar</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001681</span>
      <span itemprop="name">protective</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000001603</span>
      <span itemprop="name">reducing</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000006722</span>
      <span itemprop="name">reduction reaction</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000002104</span>
      <span itemprop="name">routine</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000011218</span>
      <span itemprop="name">segmentation</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">239000000126</span>
      <span itemprop="name">substances</span>
      <span itemprop="domain">Substances</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">238000004441</span>
      <span itemprop="name">surface measurement</span>
      <span itemprop="domain">Methods</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000152</span>
      <span itemprop="name">swallowing</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">230000000007</span>
      <span itemprop="name">visual effect</span>
      <span itemprop="domain">Effects</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
    <li itemprop="match" itemscope repeat>
      <span itemprop="id">229910001868</span>
      <span itemprop="name">water</span>
      <span itemprop="domain">Inorganic materials</span>
      <span itemprop="svg_large"></span>
      <span itemprop="svg_small"></span>
      <span itemprop="smiles"></span>
      <span itemprop="inchi_key"></span>
      <span itemprop="similarity">0</span>
      
      <span itemprop="sections" repeat>description</span>
      
      <span itemprop="count">1</span>
    </li>
  
  </ul>
  

  

  <section>
    <h2>Classifications</h2>
    
    <ul>
      <li>
        <ul itemprop="cpcs" itemscope repeat>
          <li itemprop="cpcs" itemscope repeat>
            <span itemprop="Code">G</span>&mdash;<span itemprop="Description">PHYSICS</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope repeat>
            <span itemprop="Code">G10</span>&mdash;<span itemprop="Description">MUSICAL INSTRUMENTS; ACOUSTICS</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope repeat>
            <span itemprop="Code">G10L</span>&mdash;<span itemprop="Description">SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope repeat>
            <span itemprop="Code">G10L15/00</span>&mdash;<span itemprop="Description">Speech recognition</span>
            
            
            
          </li>
          <li itemprop="cpcs" itemscope repeat>
            <span itemprop="Code">G10L15/24</span>&mdash;<span itemprop="Description">Speech recognition using non-acoustical features</span>
            <meta itemprop="Leaf" content="true">
            
            <meta itemprop="FirstCode" content="true">
          </li>
          </ul>
      </li>
      </ul>
  </section>

  <section itemprop="abstract" itemscope>
    <h2>Abstract</h2>
    
    <div itemprop="content" html><abstract mxw-id="PA62597989" lang="EN" load-source="patent-office">
    <div num="p-0001" class="abstract">Method and system for generating electromyographic or sub-audible signals (“SAWPs”) and for transmitting and recognizing the SAWPs that represent the original words and/or phrases. The SAWPs may be generated in an environment that interferes excessively with normal speech or that requires stealth communications, and may be transmitted using encoded, enciphered or otherwise transformed signals that are less subject to signal distortion or degradation in the ambient environment.</div>
  </abstract>
  </div>
  </section>

  <section itemprop="description" itemscope>
    <h2>Description</h2>
    
    <div itemprop="content" html><div mxw-id="PDES28044630" lang="EN" load-source="patent-office" class="description">

  <heading>FIELD OF THE INVENTION</heading>
  <p num="p-0002">This invention relates to analysis and communications applications of electromyographic signals produced in a human body</p>
  <heading>BACKGROUND OF THE INVENTION</heading>
  <p num="p-0003">Communications between two or more humans, or between a human and a machine, is traditionally dominated by visual and verbal information and alphanumeric input. Efforts to automate human-to-human or human-to-machine communication, such as commercial speech recognition, have emphasized the audible aspects. A totally auditory communication strategy places a number of constraints on the communication channels, including sensitivity to ambient noise, a requirement for proper formation and enunciation of words, and use of a shared language. The physical limitations of sound production and recognition also become problematic in unusual environments, such as those involving hazardous materials (HAZMATs), extra vehicular activity (EVA) space tasks, underwater operations and chemical/biological warfare (CBW). Conventional auditory expression may be undesirable for private communication needed in many situations encountered daily, such as discrete or confidential telephone calls, offline or sotto voce comments during a teleconference call, certain military operations, and some human-to-machine commands and queries. Communication alternatives that are both private and not dependent upon production of audible signals are valuable.</p>
  <p num="p-0004">*Emergency response teams face dangerous and challenging communication difficulties from presence of heavy smoke, hazardous substance gases, speech restrictions arising from use of self-contained breathing apparati (“SCBAs”), self-contained underwater breathing apparatus (SCUBA), and high external noise levels, among other things. Where a responder wears an SCBA, an in-mask microphone or external voice-only communication gear can be overwhelmed or masked by air hiss, heavy breathing, and signal degradation from breath moisture and perspiration within the SCBA during a response, and external ambient noise. New threats to homeland security require use of even more protective gear, where audio communication problems become more severe. Issuance of stealth commands to a robot control system, or of stealth communications on a cell phone (e.g., by a SWAT team member or an Air Marshall aboard an aircraft), also requires use of relatively silent audio input.</p>
  <p num="p-0005">One proposed method for studying alternative means of communication is direct understanding of brain signals, which bypasses speech and its analysis altogether. J. R. Wolpaw et al, “Brain-computer interface technology: a review of the first international meeting,” I.E.E.E. Trans. on Rehabilitation Engineering, vol. 8 (2000)164-171, recently published a review of electroencephalograph (EEG) analysis. Several practical difficulties are encountered for near term application of pure EEG approaches, due to use in EEG of aggregated surface measured brain potential and/or use of a large number of EEG sensors. Additionally, one confronts the nonlinear complexity and idiosyncratic nature of the signals. An alternative, invasive EEG measurement and analysis, is not considered practical for widespread use, except for extreme medical conditions.</p>
  <p num="p-0006">What is needed is a sub-audible communication system that provides one or more tiers, in addition to conventional audible communication, to exchange or transfer information compactly, reliably and reasonably accurately. Preferably, the amount of computation required should be modest and should not be out of proportion to the information obtained through the signal processing, should be resistant to the presence of noise and should allow soundless communication in difficult environments.</p>
  <heading>SUMMARY OF THE INVENTION</heading>
  <p num="p-0007">These needs are met by the invention, which provides and applies a system for receipt and analysis of sub-audible signals to estimate and provide a characterization of speech that is sotto voce or is not fully formed for purposes of normal speech recognition. This system relies on surface measurement of muscle signals (i.e., electromyographic “EMG” signals) to discriminate and recognize sub-audible speech signals produced with relatively little acoustic input. In one alternative, EMG signals are measured on the side of a subject&#39;s throat, near the larynx, and under the chin near the tongue, to pick up and analyze surface signals generated by a tongue (so-called electropalatogram “EPG” signals)</p>
  <p num="p-0008">*Sub-audible speech is a new form of human communication that uses tiny neural impulses (EMG signals) in the human vocal tract rather than audible sounds. These EMG signals arise from commands sent by the brain&#39;s speech center to tongue and larynx muscles that enable production of audible sounds. Sub-audible speech arises from EMG signals intercepted before an audible sound is produced and, in many instances, allows inference of the corresponding word or sound. Where sub-audible speech is received and appropriately processed, production of recognizable sounds is no longer important. Further, the presence of noise and of intelligibility barriers, such as accents associated with the audible speech, no longer hinder communication. Neural signals are consistent, arising from use of a similar communication mechanism between (sub-audible) speaker and listener.</p>
  <p num="p-0009">*This approach relies on the fact that audible speech muscle control signals must be highly repeatable, in order to be understood by others. These audible and sub-audible signals are intercepted and analyzed before sound is generated by air pressure using these signals. The recognized signals are then fed into a neural network pattern classifier, and near-silent or sub-audible speech that occurs when a person “talks to himself or to herself” is processed. In this alternative, the tongue and throat muscles still respond, at a lowered intensity level, as if a word or phrase (referred to collectively herein as a “word”) is to be made audible, with little or no external movement cues present. Where sufficiently precise sensing, optimal feature selection and good signal processing are available, it is possible to analyze these weak signals to perform useful tasks without conventional vocalization, thus mimicking an idealized thought-based approach.</p>
  <p num="p-0010">*This approach uses a training phase and a subsequent word recognition phase. In the training phase, the beginning and end of a sub-audible speech pattern (“SASP”) is first determined for each of R spoken instances of a word in a database. This includes Q words in a window of temporal length 1-4 sec each (preferably about 1.5 sec) that are provided and processed. A signal processing transform is applied to obtain a sub-sequence of transform parameter values, which become entries in a matrix M, where the two matrix axes may represent scale factors and time intervals associated with a window. The matrix M is tessellated into groups of cells (e.g., of rectangular or other shape), each cell is represented by a feature value for that cell, and the cell features are rearranged as a vector. Weighted sums of the vector components are formed and subsequently used as comparison indices. This completes the training phase and provides a lexicon of words available for comparison.</p>
  <p num="p-0011">*In the word recognition phase, a SASP including an unknown word is provided and sampled, as in the training phase. The training phase procedures are applied to the SASP, and error indices Δ2(q) (numerical differences between each training phase index q and the corresponding SASP index) are computed. If at least one word (q=q0) can be found for which Δ2(q0) is no greater than an error threshold ε(thr), at least one word (q=q0) in the lexicon with minimum error is interpreted as corresponding to the unknown word.</p>
  <p num="p-0012">*The training phase is a learning procedure, whereby the system learns to distinguish between different features of known words in a database and provides reference sets of neural net weight coefficients for this purpose. In a second (word recognition) phase, the weight coefficients are applied to one or more unknown words to determine if an unknown word is sufficiently similar to a word in the database. This technique provides several advantages, including minimization of word variations through: (i) use of similar communication mechanisms (useful because muscle patterns for a given person are stable and reproducible) and (ii) use of reliable speech recognition techniques to locate vowels and consonants and to identify a time interval corresponding to generation of a particular vowel and/or consonant combination. This approach provides non-invasive sensing, reasonably robust response to physiological variations, and privacy.</p>
  <p num="p-0013">*Separating speech from sound generation allows new communication options. For example, words spoken sub-audibly by a handicapped person with some consistent voice muscle behavior may now be understood, even where the corresponding audible words might not be understood. An example is a laryngectomy patient where vocal cords are removed. Another example is a deaf person who cannot hear what his/her audible speech sounds like to others. Use of sub-audible speech can overcome large speech generation impediments associated with underwater diver mouthpieces, with emergency responder breathing units and with the presence of extreme audible noise.</p>
  <p num="p-0014">*Sub-audible speech can be used for reliable communication by medical service workers, emergency service workers (firefighters, hazardous substance first responders, etc.), homeland security investigators, SWAT team members, Air Marshalls, special forces, physically disabled and speech-disabled persons, and for interactive database searches, PDA data access, robotic command/control, and silent cellular phones.</p>


  <description-of-drawings>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="p-0015"> <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates placement of signal recording electrodes in an initial experiment on sub-audible speech analysis.</p>
    <p num="p-0016"> <figref idrefs="DRAWINGS">FIGS. 2A-2F</figref> are graphical views of sub-audible signals corresponding to the generic words “stop”, “go”, “left”, “right”, “alpha” and “omega.”</p>
    <p num="p-0017"> <figref idrefs="DRAWINGS">FIG. 3</figref> illustrates a simplified neural network classifier, with one hidden layer, that may be applied in practicing the invention.</p>
    <p num="p-0018"> <figref idrefs="DRAWINGS">FIGS. 4 and 5</figref> are high level flow charts of procedures for practicing a training procedure and a word recognition procedure according to the invention.</p>
    <p num="p-0019"> <figref idrefs="DRAWINGS">FIGS. 6-10</figref> are flow charts of intermediate procedures associated with the steps in the <figref idrefs="DRAWINGS">FIG. 4</figref> or <figref idrefs="DRAWINGS">FIG. 5</figref> flow chart.</p>
    <p num="p-0020">*<figref idrefs="DRAWINGS">FIG. 11</figref> is a flow chart of a procedure for practicing the invention.</p>
    <p num="p-0021">*<figref idrefs="DRAWINGS">FIG. 12</figref> schematically illustrates an embodiment of the invention.</p>
  </description-of-drawings>


  <heading>DESCRIPTION OF BEST MODES OF THE INVENTION</heading>
  <p num="p-0022">In some initial tests, sub-audible pronunciation of six English words (“stop”, “go”, “left”, “right”, “alpha” and “omega”) and ten numerals (“1”, “2”, “3”, “4”, “5”, “6”, “7”, “8”, “9” and “0”) were recorded for each of three subjects, ages 55, 35 and 24, to provide a control set of words for a small graphic model that might be used to provide commands on a Mars Rover system, for example. The words “alpha” and “omega” may be used to enter a command to move faster or slower, or up or down, or forward or backward, as appropriate under the circumstances. EMG data were collected for each subject, using two pairs of self-adhesive Ag/Ag—Cl electrodes, located near the left and right anterior external area of the throat, about 0.25 cm back from the chin cleft and about 1.5 cm from the right and left sides of the larynx, as indicated in <figref idrefs="DRAWINGS">FIG. 1</figref>. Initial results indicate that one pair, or more pairs if desired, of electrodes, located diagonally between the cleft of the chin and the larynx, preferably in non-symmetrical locations, will suffice for recognition in small word sets. Signal grounding usually relies on attachment of an additional electrode to the right or left wrist, jaw bone or another location on the body. When data are acquired using wet electrodes, each electrode pair is connected to a commercial signal amplifier (Neuroscan or equivalent) and recorder that records the EMG responses at a sampling rate of up to 20 kHz. A 60 Hz notch filter is used to reduce ambient signal interference.</p>
  <p num="p-0023">One hundred or more exemplars for each word were initially recorded for each subject over a six-day interval, in morning and afternoon sessions. In a first group of experiments, the signals were sectioned offline into two-second time windows with variable window start times, and extraneous signals (coughs, swallows, body noises, etc.) were removed using SCAN 4 Neuroscan software. <figref idrefs="DRAWINGS">FIGS. 2A</figref>, <b>2</b>B, <b>2</b>C, <b>2</b>D, <b>2</b>E and <b>2</b>F graphically illustrate representative EMG blocked signals for six windows, corresponding to the words “stop”, “go”, “left”, “right”, “alpha” and “omega”, respectively. The blocked signals for these words are not wholly reproducible and may be affected by the test subject&#39;s health and the time (of day) the particular signal is recorded and analyzed. The technique must also take into account the changing signal-noise ratio and/or changing amplitudes of the signals.</p>
  <p num="p-0024">For signal feature processing, Matlab scripts were developed to provide a uniform signal processing system from recording through network training. These routines were used to receive and transform the raw signals into feature sets, to dynamically apply a threshold to the transformed signals, to adjust signal-to-noise ratios, and to implement neural network algorithms for pattern recognition and training. EMG artifacts, such as swallowing, muscle fatigue tremors and coughs, were removed during preprocessing of the windowed samples. In a real time application, artifact filters would be incorporated to prevent such anomalies from being classified.</p>
  <p num="p-0025">Sectioned signal data for each word were transformed into usable classifier feature vectors using preprocessing transforms, combined with a coefficient reduction technique. Several transforms were tested, including: (i) a short time interval Fourier Transform (STFT), requiring multiple overlapping windows; (ii) discrete wavelets (DWTs) and continuous wavelets (CWTs) using Daubechies 5 and 7 bases; (iii) dual tree wavelets (DTWTs) with a near sym_a 5,7 tap filter and a Q-shift 14,14 tap filter; (iv) Hartley Transforms; (v) Linear Predictive Coding (LPC) coefficients and (vi) uniformly and nonuniformly weighted moving averages. Feature sets were created differently for each of these transform approaches, depending upon the unique signal processing approaches, with different pattern discriminations.</p>
  <p num="p-0026">The most effective real time SPTs were the windowed STFTs and the DTWT coefficient matrices, each of which was post-processed to provide associated feature vectors. One suitable procedure is the following. Transform coefficient vectors are generated for each word, using, for example, the STFT or the DTWT applied to the magnitude (absolute value) of the raw signal amplitude. Where unipolar, rather than bipolar, electrodes are used, positive and negative sign signals are distinguishable, and STFTs and DTWTs could be applied to the raw signal amplitudes without automatic formation of an absolute value. Vectors were post processed using a Matlab routine to create a matrix M of spectral coefficients. This matrix is tessellated into a set of sub-matrices or cells, depending upon the spectral information complexity. Tessellation sizes were determined in part by average signal energy in a given region of the spectral matrix. Uses of equal and unequal segmentation sizes can be used. A representative value was calculated for each candidate sub-matrix, to reduce the number of features or variables presented to the pattern recognition algorithm and to represent average coefficient energy.</p>
  <p num="p-0027">In one approach, simple mean or average signal energy within a cell was used as a cell representative or “feature.” Other first order statistical values, such as medians, modes and maximum sub-matrix values, can be used but appear to provide no substantial improvement over use of a simple mean of signal energy. The result of this approach is a fixed length feature vector for each sub-audible word tested. Dual tree wavelets are attractive here, as opposed to standard discrete wavelets, to minimize the normal wavelet sensitivity to phase shifts. Continuous wavelets (CWTs) are not presently practical for real time computations. The Hartley Transform, which provides additional information on signal behavior along a non-real line in the transform plane, was also explored, as was use of moving averages of various lengths.</p>
  <p num="p-0028">*In another approach, a set S(M) of one or more local maxima of magnitudes of the matrix entries M(i,j) is identified within the matrix M. Tessellation into (possibly non-uniform) cells C (each containing one of the local maxima) is determined by identifying an exhaustive, mutually exclusive set SS(M) of subsets (not necessarily rectangular) of M for which a weighted sum of variances of the subsets in SS(M) is minimized. A beginning member of a given subset may be an entry with the largest magnitude, among all entries not belonging to any other subset. This approach offers the advantage that the variances of matrix entry magnitudes within each cell C in the resulting set of subsets SS(M) is reduced, or even minimized so that these magnitudes are, in some sense, closer to all other entries in the same subset. A disadvantage of this approach is that it is computationally intensive. However, the computations only need be done once, in the training phase. A feature of each subset is determined as in the first approach.</p>
  <p num="p-0029">Feature vectors for each instance of a word are used to train a neural network (NN) word recognition engine. Accuracy of recognition is evaluated using about 20 percent of the untrained word exemplars and signals from only one electrode pair, which is randomly drawn from the collection of electrode pairs, in a data recording session.</p>
  <p num="p-0030">Five NN paradigms were considered for signal training classification, using the entire feature set: (1) scaled conjugate gradient nets; (2) Leavenberg-Marquardt nets; (3) probabilistic neural nets (PNNs); (4) modified dynamic cell structure (DCS) nets; and (5) linear classifiers. After comparison of the results, a scaled conjugate gradient net was chosen, for the following reasons. A Leavenberg-Marquardt net reaches the lowest mean square error level but requires too much system memory when dealing with large data sets, even where reduced memory variations are used. A signal having a low mean square error (MSE) does not necessarily correspond to, or produce, an improved generalization for new signals, where high sensor noise is present. PNN nets provide reasonable classifications but require very large training sample sizes to reach stable probabilities and do not appear to be superior in ultimate pattern discrimination ability. A dynamic cell structure (DCS) net provides fast net training, which is attractive for real time adaptation, but is less compact for the anticipated applications that are memory sensitive. A scaled conjugate gradient network has fast convergence with adequate error levels for the signal-noise ratios encountered in the data; and the performance is comparable to the Leavenberg-Marquardt performance. The scaled conjugate gradient network uses a “trust” region gradient search criterion, which may contribute to the superior overall results of this approach. However, with improved computing ability, any of the signal training approaches can be used here.</p>
  <p num="p-0031">In other EMG tasks, we successfully applied Hidden Markov Models (HMMs), but these appear to be most effective for non-multi-modal signal distributions, such as are associated with single discrete gestures, rather than with the temporally non-stationary, sub-audible signal patterns of concern here. An HMM approach also requires sensitive pre-training to accurately estimate transition probabilities. A hybrid HMM/neural net approach, is an alternative.</p>
  <p num="p-0032">In order to quickly explore many experimental situations using different transform variations, we have operated in a simulated real time environment that has been developed and used at N.A.S.A. Ames, wherein EMG signals are recorded to file and are later used to train and test the signal recognition engines. Our initial three test subjects were not given immediate feedback about how well their sub-audible signals were recognized. However, some learning occurred as each test subject was permitted to view his or her EMG signals.</p>
  <p num="p-0033"> <figref idrefs="DRAWINGS">FIG. 3</figref> illustrates a simplified example of a neural network classifier <b>31</b> with one hidden layer, configured to analyze a vector of feature values provided according to the invention. The NN configuration <b>31</b> includes a first (input) layer <b>32</b> having four input nodes, numbered k=1, . . . , K (K=4 here), a second (hidden) layer <b>33</b> having two intermediate nodes, numbered h=1, . . . , H (H=2 here), and a third (output) layer <b>34</b> having three output nodes, numbered g=1, . . . , G (G=3 here). A practical neural net classifier may have tens or hundreds of input nodes, hidden layer(s) nodes and output nodes. The input values v<sub>k </sub>received at the first layer of nodes are summed and a first activation function A1 is applied to produce</p>
  <p num="p-0034"> <maths id="MATH-US-00001" num="00001"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mtable> <mtr> <mtd> <mrow> <msub> <mi>u</mi> <mi>h</mi> </msub> <mo>=</mo> <mrow> <mi>A</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> <mo>⁢</mo> <mrow> <mo>{</mo> <mrow> <mrow> <munderover> <mo>∑</mo> <mrow> <mi>k</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>K</mi> </munderover> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <msub> <mi>w</mi> <mrow> <mn>1</mn> <mo>,</mo> <mi>k</mi> <mo>,</mo> <mi>h</mi> </mrow> </msub> <mo>·</mo> <msub> <mi>v</mi> <mi>k</mi> </msub> </mrow> </mrow> <mo>+</mo> <mrow> <mn>1</mn> <mo>·</mo> <mi>b</mi> </mrow> </mrow> <mo>}</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mi>h</mi> <mo>=</mo> <mn>1</mn> </mrow> <mo>,</mo> <mi>…</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>,</mo> <mi>H</mi> </mrow> <mo>)</mo> </mrow> <mo>,</mo> </mrow> </mtd> </mtr> </mtable> </mtd> <mtd> <mrow> <mo>(</mo> <mn>1</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br/>
where the quantities w<sub>1,k,h </sub>are weight coefficients connecting the nodes in the first layer to the nodes in the second layer and b is a bias number. The intermediate values received at the second layer are summed and a second activation function A2 is applied to produce
</p>
  <p num="p-0035"> <maths id="MATH-US-00002" num="00002"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mtable> <mtr> <mtd> <mrow> <msub> <mi>t</mi> <mi>g</mi> </msub> <mo>=</mo> <mrow> <mi>A</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>2</mn> <mo>⁢</mo> <mrow> <mo>{</mo> <mrow> <mrow> <munderover> <mo>∑</mo> <mrow> <mi>h</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>H</mi> </munderover> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <msub> <mi>w</mi> <mrow> <mn>2</mn> <mo>,</mo> <mi>h</mi> <mo>,</mo> <mi>g</mi> </mrow> </msub> <mo>·</mo> <msub> <mi>u</mi> <mi>h</mi> </msub> </mrow> </mrow> <mo>+</mo> <mrow> <mn>1</mn> <mo>·</mo> <mi>b</mi> </mrow> </mrow> <mo>}</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mi>g</mi> <mo>=</mo> <mn>1</mn> </mrow> <mo>,</mo> <mi>…</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>,</mo> <mi>G</mi> </mrow> <mo>)</mo> </mrow> <mo>,</mo> </mrow> </mtd> </mtr> </mtable> </mtd> <mtd> <mrow> <mo>(</mo> <mn>2</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br/>
where the quantities w<sub>2,h,g </sub>are weight coefficients connecting the nodes in the second layer to the nodes in the third layer. Here, A1 and A2 may be, but need not be, the same activation function, and more than one activation function can be used in a given layer. More than one hidden layer can be included, by obvious extensions of the notation. This formalism will be used in the following development of the NN analysis, in <figref idrefs="DRAWINGS">FIGS. 4 and 5</figref>.
<br/>
Training Procedure.
</p>
  <p num="p-0036">The term “training procedure,” according to one embodiment of the invention, includes the following actions: (1) receive R spoken instances, of a sub-audible EMG signal, for at least one known word; (2) detect the beginning of each SASP containing an instance, using a thresholding procedure; (3) for each SASP, create a window, having a selected length Δt(win), that includes the SASP; (4) apply a “signal processing transform” (SPT) to each instance of one of the SASPs; (5) form a matrix (which can be one-dimensional, a vector) from the SPT values for each instance of the SASP; (6) tessellate the matrix into cells, with each cell represented by a cell “feature”, for each instance; (7) (re)format the cell features as entries or components of a vector; (8) (optionally) normalize the vector entries; (9) receive the vector entries for each instance of the SASP in a neural network classifier; (10) for all instances of each word, identify sets of reference weight coefficients for the vector entry values that provide a best match to a reference pattern that corresponds to the words considered; and (11) use the reference weight coefficients in a neural network analysis of an unknown word received by the system.</p>
  <p num="p-0037"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a high level flow chart illustrating a procedure for practicing a training procedure according to the invention. In step <b>41</b>, a sequence of length Δt(win)=1-4 sec (preferably, Δt(win)=1.5 sec) of sampled signal values is received, and a sample thresholding operation is performed to determine where, in the sequence, a sub-audible speech pattern (SASP) begins. SASPs representing samples of a known word at a selected rate (e.g., about 2 kHz) are identified, recorded and optionally rectified. Signal rectification replaces the signal at each sampling point by the signal magnitude (optional). R spoken instances, numbered r=1, . . . , R(R≧10), of a given word (SASP) are preferably used for training the system to recognize that SASP.</p>
  <p num="p-0038">In step <b>42</b>, a Signal Processing Transform operation is performed on the pattern SASP over the window length Δt(win) for each spoken instance r=1, . . . , R, and for each word, numbered q=1, . . . , Q in a database, to provide a spectrum for the received signal for each of the windowed samples. As used herein, a “Signal Processing Transform” (SPT) has a finite domain (compact support) in the time variable, provides a transform dependent upon at least one transform parameter (e.g., window length, number of samples used in forming the transform, scale factor, frequency, etc.), allows summation or integration over this parameter, and a collection of these transforms for different values of the transform parameter is mathematically complete.</p>
  <p num="p-0039">The SPT operation in step <b>42</b> may rely upon a short time interval Fourier transforms (STFTs), discrete wavelets (DWTs) and continuous wavelets (CWTs) using Daubechies 5 and 7 bases; dual tree wavelets (DTWTs) with a near_sym_a 5,7 tap filter and a Q-shift 14,14 tap filter; Hartley Transforms; Linear Predictive Coding (LPC) coefficients, and uniformly and nonuniformly weighted moving averages, or any other suitable transforms. The spectrum obtained by this operation (expressed as a function of one or more transform parameters) is a sequence of data transform samples, formatted as an m-row-by-n-column matrix M (or as a vector, with m=1 or n=1) having a first matrix axis (along a row) and a second matrix axis (along a column), with each matrix entry representing a concentration or intensity associated with a scale factor and/or window time. In a preferred embodiment, for a wavelet SPT, the n columns (e.g., n=30) represent an increasing sequence of window times for constant scale factor, and the m rows (e.g., m=129) represent a dyadic sequence of scale factors used to provide the spectrum for a given window time. Alternatively, the m rows may represent window times and the n columns may represent scale factors. A sequence of further operations is performed on the matrix, as discussed in the following,</p>
  <p num="p-0040">In step <b>43</b>, the matrix entries (e.g., wavelet coefficients) are tessellated or decomposed into “cells,” with each cell representing a grouping of adjacent matrix entries (e.g., a rectangular grouping of one or more sizes), where the entries in a given cell resemble each other according to one or more criteria and associated metric(s). A matrix may be divided into uniform size cells or may be divided according to statistical similarity of cell entries or according to another criterion.</p>
  <p num="p-0041">As an example, consider the following 4×6 matrix</p>
  <p num="p-0042"> <maths id="MATH-US-00003" num="00003"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>M</mi> <mo>=</mo> <mrow> <mo></mo> <mtable> <mtr> <mtd> <mn>1</mn> </mtd> <mtd> <mn>2</mn> </mtd> <mtd> <mn>3</mn> </mtd> <mtd> <mn>4</mn> </mtd> <mtd> <mn>5</mn> </mtd> <mtd> <mn>6</mn> </mtd> </mtr> <mtr> <mtd> <mn>1</mn> </mtd> <mtd> <mn>3</mn> </mtd> <mtd> <mn>5</mn> </mtd> <mtd> <mn>7</mn> </mtd> <mtd> <mn>9</mn> </mtd> <mtd> <mn>11</mn> </mtd> </mtr> <mtr> <mtd> <mn>2</mn> </mtd> <mtd> <mn>6</mn> </mtd> <mtd> <mn>12</mn> </mtd> <mtd> <mn>20</mn> </mtd> <mtd> <mn>15</mn> </mtd> <mtd> <mn>8</mn> </mtd> </mtr> <mtr> <mtd> <mn>3</mn> </mtd> <mtd> <mn>18</mn> </mtd> <mtd> <mn>14</mn> </mtd> <mtd> <mn>7</mn> </mtd> <mtd> <mn>9</mn> </mtd> <mtd> <mn>6</mn> </mtd> </mtr> </mtable> <mo></mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>3</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br/>
The matrix M may be expressed as a vector or single stream of data entries. If one decomposes this matrix M into four 2×3 non-overlapping rectangular groups of entries (cells), the corresponding arithmetic means of the four cells become
</p>
  <p num="p-0043"> <maths id="MATH-US-00004" num="00004"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mstyle> <mtext>&lt;</mtext> <mtext>M</mtext> <mtext>&gt;</mtext> </mstyle> <mo>=</mo> <mrow> <mo></mo> <mtable> <mtr> <mtd> <mn>2.5</mn> </mtd> <mtd> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </mtd> <mtd> <mn>7</mn> </mtd> </mtr> <mtr> <mtd> <mn>9.17</mn> </mtd> <mtd> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </mtd> <mtd> <mn>10.83</mn> </mtd> </mtr> </mtable> <mo></mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>4</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br/>
which can represent each of the four cells, and the corresponding standard deviations of the four cells become
</p>
  <p num="p-0044"> <maths id="MATH-US-00005" num="00005"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mstyle> <mtext>&lt;Δ</mtext> <mtext>M</mtext> <mtext>&gt;</mtext> </mstyle> <mo>=</mo> <mrow> <mo></mo> <mtable> <mtr> <mtd> <mn>42.75</mn> </mtd> <mtd> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </mtd> <mtd> <mn>291</mn> </mtd> </mtr> <mtr> <mtd> <mn>628.97</mn> </mtd> <mtd> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </mtd> <mtd> <mn>117.36</mn> </mtd> </mtr> </mtable> <mo></mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>5</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br/>
Tessellation of the matrix entries into the four 2×3 non-overlapping groups of entries in this example may depend, for example, upon the relative sizes of the entries in the matrix M. More generally, each cell is represented by a “feature” associated therewith, which may be one or more associated numerical coefficient values, such as the entries in the matrix &lt;M&gt; shown in Eq. (4), or a maximum or minimum value from the cell entries.
</p>
  <p num="p-0045">In step <b>44</b>, each cell representative value or feature in the tessellated matrix is optionally normalized by dividing this value by (i) a sum of all values of the cell representatives, (ii) a sum of the magnitudes of all values of the cell representatives, (iii) the largest magnitude of the cell representative values or (iv) another selected sum. Alternatively, a normalized cell representative value is formed as a difference between the cell representative value and a mean value for that population, divided by a standard deviation value for that population. Alternatively, a normalized cell representative value is formed as a difference between the cell representative value and a maximum or minimum cell representative value for the tessellated matrix. One goal of normalization is to reduce the dynamic range of the cell representative values for each instance r=1, . . . , R and each word q=1, . . . , Q.</p>
  <p num="p-0046">In step <b>45</b>, the (normalized) cell representative values determined in step <b>44</b> are arranged as a vector of length K=number of cells) or other suitable entity for subsequent processing.</p>
  <p num="p-0047">In step <b>46</b>, the vector entries v<sub>k</sub>(q;r) are received and processed by a neural net (NN) classifier by multiplying each vector entry v<sub>k</sub>(q;r) by a first set of weight coefficients w<sub>1,k h</sub>(q;r) (0≦w<sub>1,k,h</sub>≦1; k=1, . . . , K; h=1, . . . , H) and summing these weighted values to form</p>
  <p num="p-0048"> <maths id="MATH-US-00006" num="00006"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mtable> <mtr> <mtd> <mrow> <mrow> <mi>S</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> <mo>⁢</mo> <msub> <mrow> <mo>(</mo> <mrow> <mi>q</mi> <mo>;</mo> <mi>r</mi> </mrow> <mo>)</mo> </mrow> <mi>h</mi> </msub> </mrow> <mo>=</mo> <mrow> <munder> <mo>∑</mo> <mi>k</mi> </munder> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mrow> <msub> <mi>w</mi> <mrow> <mn>1</mn> <mo>,</mo> <mi>k</mi> <mo>,</mo> <mi>h</mi> </mrow> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mi>q</mi> <mo>;</mo> <mi>r</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>·</mo> <mrow> <msub> <mi>v</mi> <mi>k</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mi>q</mi> <mo>;</mo> <mi>r</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mrow> <mrow> <mi>h</mi> <mo>=</mo> <mn>1</mn> </mrow> <mo>,</mo> <mi>…</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>,</mo> <mi>H</mi> </mrow> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </mtd> <mtd> <mrow> <mo>(</mo> <mn>6</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br/>
This process is repeated for each of the R spoken instances of the known word. Each of the weighted sums S1(q;r)<sub>h </sub>becomes an argument in a first activation function A1{S1(q;r)<sub>h</sub>}, discussed in the following, also in step <b>46</b>. Also in step <b>46</b>, a second set of sums is formed
</p>
  <p num="p-0049"> <maths id="MATH-US-00007" num="00007"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mtable> <mtr> <mtd> <mrow> <mrow> <mrow> <mi>S</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>2</mn> <mo>⁢</mo> <msub> <mrow> <mo>(</mo> <mrow> <mi>q</mi> <mo>;</mo> <mi>r</mi> </mrow> <mo>)</mo> </mrow> <mi>g</mi> </msub> </mrow> <mo>=</mo> <mrow> <munder> <mo>∑</mo> <mi>h</mi> </munder> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mrow> <mrow> <msub> <mi>w</mi> <mrow> <mrow> <mn>2</mn> <mo>⁢</mo> <mi>h</mi> </mrow> <mo>,</mo> <mi>g</mi> </mrow> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mi>q</mi> <mo>;</mo> <mi>r</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>·</mo> <mi>A</mi> </mrow> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> <mo>⁢</mo> <mrow> <mo>{</mo> <mrow> <mi>S</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> <mo>⁢</mo> <msub> <mrow> <mo>(</mo> <mrow> <mi>q</mi> <mo>;</mo> <mi>r</mi> </mrow> <mo>)</mo> </mrow> <mi>h</mi> </msub> </mrow> <mo>}</mo> </mrow> </mrow> </mrow> </mrow> <mo>,</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mrow> <mrow> <mi>g</mi> <mo>=</mo> <mn>1</mn> </mrow> <mo>,</mo> <mi>…</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>,</mo> <mi>G</mi> </mrow> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </mtd> <mtd> <mrow> <mo>(</mo> <mn>7</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br/>
which becomes an argument in a second activation function A2{S2(q;r)<sub>g</sub>}.
</p>
  <p num="p-0050">In step <b>47</b>, the system provides a set of reference values {A(q;ref)<sub>g</sub>}<sub>g </sub>for the word number q and computes a set of differences</p>
  <p num="p-0051"> <maths id="MATH-US-00008" num="00008"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> <mo>⁢</mo> <mrow> <mo>(</mo> <mi>q</mi> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mn>1</mn> <mo>/</mo> <mi>R</mi> </mrow> <mo>·</mo> <mi>G</mi> </mrow> <mo>)</mo> </mrow> <mo>⁢</mo> <mrow> <munder> <mo>∑</mo> <mrow> <mi>r</mi> <mo>,</mo> <mi>g</mi> </mrow> </munder> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msup> <mrow> <mo></mo> <mrow> <mrow> <mi>A</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>2</mn> <mo>⁢</mo> <mrow> <mo>{</mo> <mrow> <mi>S</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>2</mn> <mo>⁢</mo> <msub> <mrow> <mo>(</mo> <mrow> <mi>q</mi> <mo>;</mo> <mi>r</mi> </mrow> <mo>)</mo> </mrow> <mi>g</mi> </msub> </mrow> <mo>}</mo> </mrow> </mrow> <mo>-</mo> <msub> <mrow> <mi>A</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mi>q</mi> <mo>;</mo> <mi>ref</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mi>g</mi> </msub> </mrow> <mo></mo> </mrow> <mi>p</mi> </msup> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>8</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br/>
where p is a selected positive number. The system determines, in step <b>48</b>, if Δ1(q)≦ε(thr;1), where ε(thr; 1) is a selected threshold error, preferably in a range 0.01 and below.
</p>
  <p num="p-0052">If the answer to the query in step <b>48</b> is “yes,” the system accepts this estimated reference set, in step <b>49</b>, for use in the word recognition procedure, illustrated in <figref idrefs="DRAWINGS">FIG. 5</figref>. If the answer to the query in step <b>48</b> is “no,” the first and second weight coefficients, w<sub>1,k,h</sub>(q;r) and w<sub>2,h,g</sub>(q;r), are adjusted to provide another set of estimated reference values A(q;ref)<sub>g</sub>, in step <b>50</b>, using a neural net analysis approach, and steps <b>46</b>-<b>48</b> are repeated. In the neural net analysis, a gradient method is applied to a geometric space with coordinates w<sub>1,k,h</sub>(q;r) and w<sub>2,h,g</sub>(q;r), as discussed subsequently.</p>
  <p num="p-0053">In the procedure illustrated in <figref idrefs="DRAWINGS">FIG. 4</figref>, two suitable activation functions are
<br/>
<i>A{S</i>}=tan <i>h</i>(<i>S</i>)={exp(<i>a·S</i>)−exp(−<i>a·S</i>)}/{exp(<i>a·S</i>)+exp(−<i>a·S</i>)},  (9A)
<br/>
<i>A{S}=</i>1/{1−exp(−<i>a·S</i>)},  (9B)
<br/>
having the respective ranges of [−1,1] and [0,1] for −∞&lt;S&lt;∞, where a is a selected positive number. Other monotonically increasing, finite range functions can also be used as activation functions.
</p>
  <p num="p-0054">For each word q, each reference value A(q;ref)<sub>g </sub>(q=1, . . . , Q) may be determined by different first reference sets of weight coefficients, {w<sub>1,k,h</sub>(q;ref)}<sub>k </sub>and/or by different second reference sets of weight coefficients {w<sub>2,h,g</sub>(q;ref)}<sub>h </sub>which are now fixed for the word number q. The reference values A(q;ref)<sub>g </sub>and the associated first and second reference sets of weight coefficients will henceforth be used for comparison with not-yet-identified SASP words. Optionally, the NN has F hidden layers and F+1 sets of weight coefficients (F≧1).</p>
  <p num="p-0055">In an alternative embodiment, in steps <b>46</b>-<b>50</b>, a first universal set of weight coefficients, {w<sub>1,k,h</sub>(ref)}<sub>k </sub>and a second universal set of weight coefficients {w<sub>2,h,g</sub>(ref)}<sub>h</sub>, not dependent upon the particular word (q), replace the first and second sets of weight coefficients {w<sub>1,k,h</sub>(q;ref)}<sub>k </sub>and {w<sub>2,h,g</sub>(q;ref)}<sub>h</sub>. In this alternative embodiment, where the database includes at least two words, the order of the instances of different (transformed) words must be randomized, and the neural network classifier seeks to identify first and second universal sets of weight coefficients, {w<sub>1,k,h</sub>(ref)}<sub>k </sub>and {w<sub>2,h,g</sub>(ref)}<sub>h</sub>, that are accurate for all words in the database.</p>
  <p num="h-0006">Word Recognition Procedure.</p>
  <p num="p-0056">The word recognition procedure, according to one embodiment of the invention, includes the following actions: (1) receive a sub-audible EMG signal, representing an unknown word; (2) detect the beginning of an SASP, using a thresholding procedure; (3) create a window, having a selected length Δt(win), that includes the SASP; (4) create a sequence of time-shifted windowed versions of the received SASP, with time shifts equal to a multiple of a time displacement value Δt(displ); (5) apply a signal processing transform (SPT) to each of the time-shifted versions of the SASP; (6) form a matrix (which can be one-dimensional, a vector) from the SPT values for each of the time-shifted versions of the SASP; (7) tessellate the matrix into cells, with each cell represented by a cell “feature”; (8) (re)format the cell features as entries or components of a vector; (9) (optionally) normalize the vector entries; (10) receive the vector entries, for each time-shifted version of the SASP in a trained neural network classifier, and identify a word from a database that provides a best match to an activation function value corresponding to each time-shifted version of the SASP, (11) accumulate a point for each best match; and (12) identify a word, if any, with the highest point count as the best match to a word corresponding to the received SASP.</p>
  <p num="p-0057"> <figref idrefs="DRAWINGS">FIG. 5</figref> is a high level flow chart of a word recognition procedure that uses the results of the training procedure shown in <figref idrefs="DRAWINGS">FIG. 4</figref>. In step <b>51</b>, a sub-audible speech pattern (SASP) representing a sample of a “new” (unknown) word (referred to as number q′) is received and optionally rectified. A sequence of sample values is received at the selected rate used in <figref idrefs="DRAWINGS">FIG. 4</figref>. A sample thresholding operation is performed to determine where, in the sequence, the sub-audible speech pattern (SASP) begins. A sequence of J time-shifted, partially overlapping windows, numbered j=1, . . . , J (J≧2), is formed from the signal representing the new word, with consecutive start times displaced by multiples of a selected displacement time such as Δt(displ)=0−Δt(win)/2.</p>
  <p num="p-0058">In step <b>52</b>, an SPT operation is performed on the new SASP over the window length Δt(win), to provide a first spectrum for the new word for each of the windowed samples. In step <b>53</b>, the matrix entries are tessellated or decomposed into the same cells that were used for each word in step <b>43</b> of <figref idrefs="DRAWINGS">FIG. 4</figref>. In step <b>54</b>, each cell representative value or feature in the tessellated matrix is optionally normalized. In step <b>55</b>, the cell representative values are arranged as a vector V′ having vector entries v′<sub>k </sub>(k=1, . . . , K) or other suitable entity for subsequent processing. In step <b>56</b>, the first and second reference sets of weight coefficients, {w<sub>1,k,h</sub>(q;ref)}<sub>k </sub>and {w<sub>2,h,g</sub>(q;ref)}<sub>h</sub>, (or {w<sub>1,k,h</sub>(ref)}<sub>k </sub>and {w<sub>2,h,g</sub>(ref)}<sub>h</sub>) used to compute the activation function reference value A2{S2(q;ref)}<sub>g </sub>(or A{S(ref)}<sub>g</sub>) for the word number q are used to compute an activation function A2{S2′(q′;ref}<sub>g</sub>, as in Eq. (6).</p>
  <p num="p-0059">In step <b>57</b>, the system computes differences
<br/>
Δ2(<i>q;q′;j</i>)<sub>g</sub> <i>=|A</i>2{<i>S</i>2′(<i>q′;j;ref</i>)}<sub>g</sub> <i>−A</i>(<i>q;ref</i>)<sub>g</sub>|  (10)
<br/>
for each word (q) in the database, for each time shifted window (j) and for each NN third layer index g. Optionally, in step <b>58</b>, only those words (q) in a “reduced” database RDB, for which
<br/>
Δ2(<i>q;q′;j</i>)<sub>g</sub>≦ε(<i>thr;</i>2)  (11)
<br/>
is satisfied, are considered in accumulating points (in step <b>59</b>), where ε(thr;2) is a selected second threshold error, preferably in a range 0.01 and below. Optionally ε(thr;1)=ε(thr;2), but this is not required.
</p>
  <p num="p-0060">In step <b>59</b>, for each time-shifted window (numbered j=1, . . . , J), each word (q) in the database (or in the reduced database RDB) that provides the smallest value Δ2(q;q′;j)<sub>g </sub>among the set of values computed in Eq. (11), is given one point or vote. In step <b>60</b>, the word (q) in the database with the largest number of points is interpreted as the unknown word (q′) that was received. Optionally, the point(s) accumulated according to the minimum value of Δ2(q;q′;j)<sub>g </sub>can be weighted, for example, by multiplying the number 1 by a weight function WF{Δ2(q;q′;j)<sub>g</sub>} that is monotonically decreasing as the argument Δ2(q;q′;j)<sub>g </sub>increases. Two examples of suitable weighting functions are
<br/>
<i>WF</i>(<i>s</i>)=<i>a+b·ex{−αas},</i>  (12A)
<br/>
<i>WF</i>(<i>s</i>)={<i>c−d</i>·(<i>s</i>)<sup>β</sup>}<sup>e</sup>,  (12B)
<br/>
where a, b, c, α and β and the product d·e are non-negative numbers, not all 0. If two or more words (e.g., q1 and q2) in the database have substantially the same largest point accumulation, the system optionally interprets this condition as indicating that no clearly-best match to the unknown word (q′) is available, in step <b>61</b>.
</p>
  <p num="p-0061"> <figref idrefs="DRAWINGS">FIG. 6</figref> sets forth in more detail a first embodiment for a thresholding operation for step <b>41</b> in <figref idrefs="DRAWINGS">FIG. 4</figref>. In step <b>71</b>, two or more moving averages of consecutive sequences of H1 sampled values and H2 sampled values are formed (H1&lt;H2), where, for example, H1=10 and H2=20 is a suitable choice. Initially, the sample amplitudes and both moving averages are substantially 0, except for the presence of noise. As the system encounters the beginning of a sub-audible speech pattern (SASP), the shorter H1-sample will rise before the longer H2-sample rises, when applied to consecutive sample runs with the same starting point. In step <b>72</b>, the system determines if the moving average of the H1-samples is at least a multiple μ of the moving average of the H2-samples, where μ is a selected ratio≧1. If the answer to the query in step <b>72</b> is “no,” the system returns to step <b>71</b> and continues to receive samples and to form the two moving averages. If the answer to the query in step <b>72</b> is “yes,” the system infers that an SASP is present and that an “SASP threshold” has been crossed; and the system begins to divide succeeding time intervals into epochs, in step <b>73</b>. Other methods of determining when an SASP threshold has been crossed can also be used here.</p>
  <p num="p-0062">In step <b>74</b> of <figref idrefs="DRAWINGS">FIG. 6</figref>, a set of signal samples is received, preferably as a stream of data, and the magnitude or absolute value of each SASP signal sample is formed (optional). In step <b>75</b>, a consecutive sequence CS of the signal samples is formed within an event window, preferably of length Δt(win)=1-4 sec. In step <b>76</b> the system creates a new sequence TSCS of time shifted consecutive sequences, with the beginning of each TSCS being shifted by a selected time delay amount Δt(displ) relative to the immediately preceding TSCS. Each TSCS will be processed and classified by a neural network classifier. The number of (above-threshold, consecutive) TSCSs may be used as a parameter in the comparisons in <figref idrefs="DRAWINGS">FIG. 4</figref>. The system then proceeds to step <b>42</b> of <figref idrefs="DRAWINGS">FIG. 4</figref> and continues.</p>
  <p num="p-0063"> <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates a dynamic threshold adjustment procedure, relying in one implementation on a Bollinger band, that may be used in step <b>41</b>. In step <b>81</b>, a sequence of T amplitudes “a” of the signal are received and stored. In step <b>82</b>, a mean (μ) and standard deviation (σ) are computed for the stored sequence. In step <b>83</b>, the system determines if the magnitude of the difference |u−μ| is at least equal to L·σ for at least one amplitude u in the stored sequence, where L is a selected positive number (e.g., L=4-10). If the answer to the query in step <b>83</b> is “no”, the system replaces the stored sequence by a new sequence (e.g., shifted by one sample value), in step <b>84</b>, and returns to step <b>82</b>; no threshold has yet been crossed in this situation. If the answer to the query in step <b>83</b> is “yes”, a threshold has been crossed within the stored sequence and a position representing the beginning of the word can be identified, in step <b>85</b>.</p>
  <p num="p-0064"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a flow chart providing more detail on step <b>42</b> in <figref idrefs="DRAWINGS">FIG. 4</figref>, where a Fourier transform is used for the SPT operation In step <b>91</b>, the data stream is optionally reformatted into a sequence of columns (or into rows) of signal samples, with each column (or row) corresponding to a TSCS, according to the format required for computer analysis. In step <b>92</b>, a Hanning filter is optionally applied to each STFT window. In step <b>93</b>, an SPT operation is performed for each row of (filtered) signal samples. The particular SPT used may be a conventional Fourier transform (applied to a window of finite width), a dual wave tree wavelet transform, a Daubechie transform, a Hartley transform, a moving average with uniform or non-uniform weights, or similar transforms. The particular choice will depend upon the known characteristics of the data received for analysis. Preferably, the SPT of the signal sample sequences will provide real and imaginary components that can be combined and processed as appropriate. In step <b>94</b>, the system forms a selected combination of real and imaginary components of the (filtered and transformed) signal samples in each row. In step <b>85</b>, the columns (or rows) are combined, end-to-end, to provide a spectrogram for each (time-overlapped) window.</p>
  <p num="p-0065"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a flow chart providing more detail on step <b>43</b> in <figref idrefs="DRAWINGS">FIG. 4</figref>, according to a first embodiment for tessellation of the matrix M. In step <b>101</b>, the entries within the matrix M are decomposed into non-overlapping, rectangularly-shaped cells of one or more selected sizes (e.g., 2×3 or 5×5 or 10×7) so that every entry belongs to precisely one cell. Cells adjacent to a boundary of the matrix M may have a different (residual) size. In step <b>102</b>, a first order statistical coefficient m<sub>1 </sub>(e.g., arithmetic mean, median, mode or largest value) is computed for, and associated with, each cell, representing an average magnitude or other feature for the entries within the cell. A second order statistical coefficient m<sub>2 </sub>(e.g., standard deviation) is optionally computed for each cell. Here, the individual values within each cell may be substantially different so that the first order coefficient m<sub>1 </sub>associated with a given cell may not be very representative of the individual entries. However, the cells in this embodiment are of fixed size, which is useful in some of the following computations. At one extreme, each cell may be a single entry in the matrix M.</p>
  <p num="p-0066"> <figref idrefs="DRAWINGS">FIG. 10</figref> is a flow chart of an alternative embodiment for tessellation of the matrix M (step <b>43</b> in <figref idrefs="DRAWINGS">FIG. 4</figref>). In step <b>111</b>, the matrix entries are tentatively aggregated into “pre-cells,” with each pre-cell initially being a single entry and having a second order statistical coefficient m<sub>2 </sub>of 0. Consider a general pre-cell, such as a rectangular set E of entries, having a selected first order statistical coefficient m<sub>1 </sub>(arithmetic mean or median or mode) and having a second order statistical coefficient m<sub>2 </sub>no larger than a selected positive threshold value σ(thr). In step <b>112</b>, an expanded pre-cell set E′, having one more row or one more column than E, is formed, and statistical coefficients m<sub>1</sub>(E′) and m<sub>2</sub>(E′) are computed for this pre-cell E′. In step <b>113</b>, m<sub>2</sub>(E′) is compared with the threshold value σ(thr). If the coefficient m<sub>2</sub>(E′) for the expanded set E′ is no larger than the threshold value σ(thr), the pre-cell is redefined, in step <b>114</b>, to include the expanded set E′, and the system returns to step <b>112</b>. The redefined set E′ is further expanded in step <b>115</b> by one row or one column to form a new set E″, and the system returns to step <b>112</b>. If m<sub>2</sub>(E′) is larger than the threshold σ(thr), the expanded set E′ is rejected, the pre-cell includes the set E but not this particular expanded set E′, and the system returns to step <b>112</b>. However, another expanded set can be formed from E, by adding a different row or column, and the coefficient m<sub>2 </sub>for this new expanded set can be computed and compared with o(thr). At some level, the system identifies a rectangular or other shape set E^ of maximum size whose coefficient m<sub>2</sub>(E^) is no larger than the threshold value σ(thr), and this maximum size set becomes a cell. This process is repeated until every entry in a cell is “similar” to every other entry in that cell, as measured by the threshold value σ(thr). The number of matrix entries has been reduced to a smaller number of cells. The cells may be rectangular but do not necessarily have the same size. In this approach, the entries in a cell are represented by the coefficient m<sub>1 </sub>for that cell, but the cell size is determined by the adjacent entries for which m<sub>2</sub>(E′)≦σ(thr) so that the entries may be more “similar” to each other.</p>
  <p num="p-0067">One practical approach for neural network training is backpropagation of errors, together with conjugate gradient analysis to identify global minima. This approach is discussed, for example, by T. Masters in <i>Practical Neural Network Recipes in C++</i>, Morgan Kaufman Publ., 1993, pp. 102-111.</p>
  <p num="p-0068">With reference to step <b>50</b> in <figref idrefs="DRAWINGS">FIG. 4</figref> in the preceding, a conjugate gradient algorithm with trust region (to limit the extension in any direction in coordinate space) is applied to the error term sum, ε(q) with q fixed, to determine an extremum point (minimum) for the received cell representatives. For example, the basic Fletcher-Reeves algorithm can be utilized, wherein a direction of steepest descent
<br/>
<i>p</i> <sub>0</sub> <i>=−g</i> <sub>0</sub>  (13)
<br/>
for the surface or function is first identified; a line search is performed to estimate the optimal distance to be moved along the current search direction (p<sub>k</sub>)
<br/>
<i>x</i> <sub>k+1</sub> <i>=x</i> <sub>k</sub>+α<sub>k</sub> <i>p</i> <sub>k</sub>;  (14)
<br/>
and a conjugate direction
<br/>
<i>p</i> <sub>k</sub> <i>=g</i> <sub>k</sub>+β<sub>k</sub> <i>p</i> <sub>k−1</sub>  (15)
<br/>
is determined for the new search direction. For the Fletcher-Reeves update, the parameter β<sub>k </sub>is chosen according to
<br/>
β<sub>k</sub> <i>=g</i> <sub>k</sub> <i>·g</i> <sub>k</sub> <i>/{g</i> <sub>k−1</sub> <i>·g</i> <sub>k−1</sub>}.  (16)
<br/>
For the Polak-Ribiere update, the parameter β<sub>k </sub>is chosen according to
<br/>
β<sub>k</sub> <i>=Δg</i> <sub>k−1</sub> <i>·g</i> <sub>k</sub> <i>/{g</i> <sub>k−1</sub> <i>·g</i> <sub>k−1</sub>},  (17)
<br/>
where Δg<sub>k−1</sub>=g<sub>k−1</sub>−g<sub>k-2 </sub>is the preceding change in the direction of steepest descent. In any conjugate gradient approach, it is preferable to periodically reset the search direction to the steepest descent gradient. In a particular approach developed by Powell and Beale, resetting occurs when little orthogonality remains between the present gradient and the preceding gradient; the corresponding test is whether the inequality
<br/>
|<i>g</i> <sub>k−1</sub> <i>·g</i> <sub>k</sub>|≧0.2<i>|g</i> <sub>k</sub>|<sup>2</sup>.  (18)
<br/>
is satisfied. Other variations on the corresponding algorithms can also be used here.
<br/>
Application to Communications in an Interfering Environment
</p>
  <p num="p-0069">*The sub-audible signal processing system disclosed herein can be applied to communication in an interfering environment, for example, an emergency service worker (ESW) working in an extreme environment (e.g., filled with smoke or toxic fumes) in which reliable and accurate communication is more important than high throughput or use of a lexicon having a large number of words. The system disclosed herein has been applied to the following group of 20 “words” (including individual words and short phrases) that an ESW might use in responding to an emergency event.</p>
  <p num="p-0070">*Table 1 sets forth the accuracies determined for one person pronouncing each of 21 words and phrases many times, including a sub-list of 6 words and phrases (“evacuate,” “Mayday,” “mantrap,” fire safe,” “fire clear,” “zero” and “one”) that were pronounced 30 times each, while wearing a self contained breathing apparatus (“SCBA”) that is customarily worn by a first responder ESW at an emergency event. The accuracies associated with the six words in the sub-list (e.g., “Mayday,” “zero” and “one”) are lower (63, 70 and 80 percent) than the remainder, but the range of accuracies for all words in the original list is 53-100 percent. The accuracy of word recognition for an ESW who is not wearing an SCBA should be substantially higher in most situations</p>
  <p num="p-0071"> <tables id="TABLE-US-00001" num="00001"> <tgroup align="left" colsep="0" rowsep="0" cols="1"> <colspec colname="1" colwidth="217pt" align="center"> </colspec> </tgroup> <row> <entry namest="1" nameend="1" rowsep="1">TABLE 1</entry> </row> <row> <entry namest="1" nameend="1" align="center" rowsep="1"> </entry> </row> <row> <entry>Accuracies Associated With Particular EMG Signals.</entry> </row> <row> <entry namest="1" nameend="1" align="center" rowsep="1"> </entry> </row> <row> <entry> </entry> </row> <tgroup align="left" colsep="0" rowsep="0" cols="3"> <colspec colname="offset" colwidth="49pt" align="left"> </colspec> <colspec colname="1" colwidth="35pt" align="left"> </colspec> <colspec colname="2" colwidth="133pt" align="center"> </colspec> </tgroup> <row> <entry> </entry> <entry>evacuate</entry> <entry>(97%)</entry> </row> <row> <entry> </entry> <entry>Mayday</entry> <entry>(80%)</entry> </row> <row> <entry> </entry> <entry>fire safe</entry> <entry>(93%)</entry> </row> <row> <entry> </entry> <entry>mantrap</entry> <entry>(97%)</entry> </row> <row> <entry> </entry> <entry>stop</entry> <entry>(84%)</entry> </row> <row> <entry> </entry> <entry>go</entry> <entry>(100%) </entry> </row> <row> <entry> </entry> <entry>left</entry> <entry>(91%)</entry> </row> <row> <entry> </entry> <entry>right</entry> <entry>(80%)</entry> </row> <row> <entry> </entry> <entry>alpha</entry> <entry>(97%)</entry> </row> <row> <entry> </entry> <entry>omega</entry> <entry>(97%)</entry> </row> <row> <entry> </entry> <entry>zero</entry> <entry>(63%)</entry> </row> <row> <entry> </entry> <entry>one</entry> <entry>(70%)</entry> </row> <row> <entry> </entry> <entry>two</entry> <entry>(53%)</entry> </row> <row> <entry> </entry> <entry>three</entry> <entry>(78%)</entry> </row> <row> <entry> </entry> <entry>four</entry> <entry>(85%)</entry> </row> <row> <entry> </entry> <entry>five</entry> <entry>(75%)</entry> </row> <row> <entry> </entry> <entry>six</entry> <entry>(78%)</entry> </row> <row> <entry> </entry> <entry>seven</entry> <entry>(75%)</entry> </row> <row> <entry> </entry> <entry>eight</entry> <entry>(80%)</entry> </row> <row> <entry> </entry> <entry>nine</entry> <entry>(63%)</entry> </row> <row> <entry> </entry> <entry namest="offset" nameend="2" align="center" rowsep="1"> </entry> </row> <table frame="none" colsep="0" rowsep="0" class="description-table"> <thead>     </thead> <tbody valign="top">  </tbody>  <tbody valign="top">                      </tbody>  </table> </tables> <br/>
The words “alpha” and “omega” can be used for other commands, such as “up” and “down,” for example. The particular set of words and phrases used is not critical, although the recognition accuracies may vary with the particular words and phrases used.
</p>
  <p num="p-0072">*<figref idrefs="DRAWINGS">FIG. 11</figref> is a flow chart of a procedure for practicing an embodiment of the invention. In step <b>121</b>, a sub-audible word or phrase (“SAWP”), or a collection of SAWPs, is received and analyzed at a SAWP signal processor that processes and transmits a representation of a SAWP over a (wireless or wired) signaling channel to at least one recipient. In step <b>122</b>, the system determines if the SAWP is a word or phrase in a SAWP lexicon associated with the module. If the answer to the query in step <b>122</b> is “no,” the system optionally ignores the SAWP and takes no further action, in step <b>123</b>, and returns to step <b>121</b>. If the answer to the query in step <b>122</b> is “yes,” the system transmits a SAWP signal representing the received SAWP over the transmission channel, in step <b>124</b>, optionally using encoded, encrypted or otherwise transformed signals. In step <b>125</b>, the recipient receives the SAWP signal and converts and displays the received SAWP signal in at least one of visually perceptible alphanumeric text, audibly perceptible speech, and a coded signal representing the received SAWP, and optionally returns to step <b>121</b>.</p>
  <p num="p-0073">*As a proof of feasibility, a wireless system <b>131</b> is illustrated in <figref idrefs="DRAWINGS">FIG. 12</figref>, including a receiver that functions as a conventional cell phone, has been programmed to receive and analyze sub-audible sounds representing a subset of the 21 words set forth above and to visually display and audibly display each word from this list that is recognized, for a given speaker <b>132</b>. The system <b>131</b> includes: an initial receiver-amplifier <b>133</b> that receives and amplifies EMG signals from the speaker <b>132</b>; an initial signal processor <b>134</b> that receives the amplified EMG signal(s) from the amplifier <b>133</b>, processes this signal, and provides the processed signal for a real time SAWP recognition processor(“RTRP”) <b>135</b> (which may be part of the initial signal processor <b>134</b>. The RTRP <b>135</b> optionally transmits its output signals to a stealth communications module <b>136</b>, to a robotic control module <b>137</b>, to a VE control module <b>138</b>, to a Web server <b>139</b> and/or to a service provider network <b>140</b> that optionally serves one or more cell radio units <b>141</b>, using general packet cell radio standards (GPRS).</p>
  <p num="p-0074">*For example, a Web server or other communication module <b>139</b> that receives the processed sub-audible message (representing a SAWP); an end receiver and network <b>140</b> (e.g., a conventional or suitably modified cell phone) that a signal representing the sub-audible message from the server <b>139</b>. A recipient (not explicitly shown) holds or controls the GPRS unit <b>140</b> and receives the ultimate signal in visually perceptible and/or audibly perceptible format.</p>
  <p num="p-0075">*If a sub-audible word or phrase (or its EMG equivalent) is recognized from a lexicon maintained by the RTRP <b>135</b>, this word or phrase is optionally presented to the recipient in visually and/or audibly perceptible format by the RTRP <b>135</b>. If the sub-audible word or phrase is not recognized by the initial signal processor <b>134</b> from the lexicon, this word or phrase is optionally presented to the recipient (not explicitly shown) in visually perceptible alphanumeric text, or the word or phrase is indicated to be non-understandable. The accuracy percentage varies from one word to another but generally lies in a range 87-97 percent.</p>
  <p num="p-0076">*In the system <b>131</b> illustrated in <figref idrefs="DRAWINGS">FIG. 12</figref>, a cell phone user <b>132</b> is connected to a Synamp amplifier <b>133</b> using Ag/Agl electrodes. Sub-audible speech processing software, including Java client software customized for sub-audible speech, as discussed in the preceding, is activated on a portable PC or other signal processor(s) <b>134</b>, <b>135</b> connected to receive and process amplified EMG signals for transmission through a USB link and Web server <b>139</b> to the end receiver <b>141</b> (e.g., a cell phone). Various background sounds are turned on, including turn-on of a diesel fire engine with an operative water pump, a high speed rescue saw and various sirens.</p>
  <p num="p-0077">*The user may wear a self contained breathing apparatus (“SCBA”) suit with turnout, and the suit may be closed and pressurized. The cell phone within the suit is activated, and the cell phone menu is transformed to reflect a sub-audible menu of options (e.g., SAWPs). The user forms a sequence of sub-audible words and phrases {e.g., “fire safe,” “mantrap,” “evacuate,” “Mayday,” “stop,” “go”}, for pickup by the PC, which delivers the processed sub-audible signals to a cell phone system. The cell phone system checks to determine if the sub-audible word or phrase is recognized among the set of previously trained words and phrases. If the word or phrase is not recognized, this SAWP is ignored. If the SAWP is recognized, the cell phone displays the word or phrase on the screen and optionally pronounces this in a pre-recorded voice. The percentage of correct recognition for this set of words and phrases is presently estimated at about 80-97 percent. The lower end of this percentage range is expected to rise with increasing familiarity with use of the system. This exemplifies use of the invention over a wireless channel.</p>
  <p num="p-0078">*In a first variation on the preceding approach, the USB link between PC and cell phone is removed, and the processed sub-audible signals are sent by wireless transmission to any phone capable of downloading a Java applet.</p>
  <p num="p-0079">*In a second variation, the word/phrase set is changed to {“stop,” “go,” “left,” “right,” “forward” and “back”} and the processed sub-audible signals are sent by wireless transmission to a PER robot. Communication of these six words is used to issue command/control signals for robot operations and/or motion.</p>
  <p num="p-0080">*This approach can be used to facilitate communication between: (1) two or more emergency service workers in a noisy, toxic, smoky or other signal-interfering environment; (2) two or more first responders in a hazardous substance environment, where one or both are wearing a self contained breathing apparatus (SCBA); (3) two or more workers in an underwater environment who need to communicate directly (visually and/or audibly), where one or more workers is wearing a SCBA apparatus; (4) two or more workers in a continuously or sporadically noisy environment, to avoid or minimize the necessity of repeating a command or observation or other information message; and (5) minimizing environmental damage to, or interference with, electronic communications mechanisms. In a situation involving workers concerned with a particular activity, the number of SAWPs used for specialized communication can be relatively small (e.g., 20-40). Alternatively, a first subset of the words or phrases for which SAWPs are provided and processed can be replaced by a second subset of encoded, encrypted, transformed or other symbolic or representational SAWPs (“R-SAWPs”) than are the SAWPs in the first subset.</p>
  <p num="p-0081">*In certain self-contained systems, such as an SCBA or an underwater diving suit, moisture from a worker&#39;s breath may accumulate to levels that interfere with normal audible speech, through creation of resonances or other distortion mechanisms that mask or otherwise change the perceived meaning of words and/or phrases. Issuance and transmission of a SAWP does not depend, or depends only weakly upon, voice-carrying characteristics that are associated with or determined by standard speech. For example, issuance, transmission, receipt and analysis of a SAWP is not affected by presence of a low level “swish” sound associated with expulsion of moist air or oxygen-depleted air (e.g., with high CO<sub>2 </sub>content) from a self-contained suit. Issuance, transmission, receipt and analysis of a SAWP is also not affected by presence of environmentally hazardous substances, such as toxic, reactive, ignitable, corrosive or poisonous gases that otherwise might limit use of standard speech because of concern for exposure of the speaker&#39;s organs or concern for degraded performance of electronic components. Issuance, transmission, receipt and analysis of a SAWP is also not affected by presence of high frequency or variable frequency noise or electromagnetically varying signals, if at least one frequency range can be found where the magnitude of the interfering signal(s) can be minimized.</p>

</div>
  </div>
  </section>

  <section itemprop="claims" itemscope>
    <h2>Claims (<span itemprop="count">27</span>)</h2>
    
    <div itemprop="content" html><div mxw-id="PCLM23065898" lang="EN" load-source="patent-office" class="claims">
  <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
    <div class="claim-text">1. A method for communicating information in an environment that interferes with audible speech, the method comprising:
<div class="claim-text">choosing at least a first sub-audible word or phrase (SAWP) and a second SAWP to be first and second representational SAWPs (Rep-SAWPs), respectively, where each Rep-SAWP is chosen to provide optimal likelihood of recognition of the transmitted SAWP signal in an ambient environment,</div>
<div class="claim-text">receiving at least one SAWP, from among the first SAWP and the second SAWP, at a wireless signaling module that processes and transmits sub-audible speech signals over a signaling channel to at least one recipient module;</div>
<div class="claim-text">determining if the at least one SAWP is a word or phrase in a SAWP lexicon associated with the module; and</div>
<div class="claim-text">when the at least one SAWP is recognized as belonging to the SAWP lexicon:</div>
<div class="claim-text">(i) transmitting the least one SAWP representing the received SAWP over the signaling channel; and</div>
<div class="claim-text">(ii) receiving the at least one SAWP from the signaling channel at the recipient module and converting the received at least one SAWP to at least one of visually perceptible alphanumeric text, audibly perceptible speech, and a coded signal representing the SAWP.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
    <div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising including in said SAWP lexicon at least one of the following words and phrases: “evacuate,” “Mayday,” “mantrap,” “fire safe,” “stop,” “go,” “left,” “right,” “alpha,” “omega,” “zero,” “one,” “two,” “three,” “four,” “five,” “six,” “seven,” “eight,” and “nine.”</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
    <div class="claim-text">3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising choosing at least one of said first and second Rep-SAWPs to be at least one of an encoded SAWP, an encrypted SAWP and a SAWP that is transformed to a format that resists signal degradation when transmitted in said ambient environment.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
    <div class="claim-text">4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said process of determining if said at least one SAWP is a word or phrase in a SAWP lexicon associated with said module comprises:
<div class="claim-text">receiving R signal sequences associated with said at least one SAWP, with each sequence including an instance of a sub-audible speech pattern (SASP), numbered r=1, . . . , R with R≧2, and each SASP including at least one word drawn from a selected database of Q words, numbered q=1, . . . , Q with Q≧2;</div>
<div class="claim-text">estimating where each of the R SASPs begins in the sequences;</div>
<div class="claim-text">for each of the signal sequences, numbered r=1, . . . , R:
<div class="claim-text">providing signal values of the received signal, number r, within a temporal window having a selected window width Δt(win);</div>
<div class="claim-text">transforming each of the R SASPs, using an Signal Processing Transform (SPT) operation that is expressed in terms of at least one transform parameter having a selected sequence of parameter values;</div>
<div class="claim-text">providing a matrix M with entries equal to the SPTs for the R SASPs, ordered according to the at least one transform parameter along a first matrix axis;</div>
<div class="claim-text">tessellating the matrix M into a sequence of exhaustive and mutually exclusive cells, with each cell containing a collection of contiguous matrix entries, where each cell is characterized according to at least one selected cell criterion;</div>
<div class="claim-text">providing, for each cell, a cell representative value (“feature”), depending upon at least one of the matrix entries within the cell;</div>
<div class="claim-text">organizing the features into a vector V with entry values v<sub>k </sub>(k=1, . . . , K), where K is the number of features for the tessellated matrix M; and</div>
<div class="claim-text">analyzing the entry values v<sub>k</sub>, using a neural net classifier, a selected neural net architecture, and a sequence of estimated weight coefficient values associated with at least one of the neural net classifier layers, where the neural net classifier provides a sequence of output values dependent upon the weight coefficient values and upon at least one of the entry values v<sub>k</sub>;</div>
</div>
<div class="claim-text">providing a sequence of target output values A(q;ref)<sub>g</sub>, one for each word number q, for the neural net classifier output values; and</div>
<div class="claim-text">adjusting the weight coefficient values to provide a reference set of weight coefficient values for which the neural net classifier output values for the R SASPs and for each of the Q words agree with the target output values within a selected threshold error number ε(thr;1).</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
    <div class="claim-text">5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising replacing at least one of said features by a normalized feature for at least one of said cells corresponding to said matrix M.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
    <div class="claim-text">6. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein said process of adjusting said weight coefficient values comprises:
<div class="claim-text">providing a first sequence of estimated weight coefficients {w<sub>1,k,h</sub>} and a second sequence of estimated weight coefficients {w<sub>1,h,g</sub>} (k=1, . . . , K; h=1, . . . , H; g=1, . . . , G), for first and second layers, respectively, of said neural net classifier;</div>
<div class="claim-text">forming a first sum S1(q;r)<sub>h </sub>of products of said entry values v<sub>k </sub>and corresponding weight coefficients w<sub>1,k,h</sub>;</div>
<div class="claim-text">computing a first activation function A1{S1(q;r)<sub>h</sub>} applied to the sum S1(q;r)<sub>h </sub>as argument;</div>
<div class="claim-text">forming a second sum S2(q;r)<sub>g </sub>of products of the first activation function values A1{S1(q;r)<sub>h</sub>} and corresponding weight coefficients w<sub>2,h,g</sub>;</div>
<div class="claim-text">computing a second activation function A2{S2(q;r)<sub>g</sub>} applied to the sum S2(q;r)<sub>g </sub>as argument;</div>
<div class="claim-text">adjusting the first and second weight coefficient sequences to obtain a first reference set {w<sub>1,k,h</sub>(q;ref)}<sub>k </sub>and a second reference set {w<sub>2,h,g</sub>(q;ref)}<sub>h </sub>of the weight coefficients, associated with said at least one word number q, for which, for each of said instances R, a difference between the activation function value A2{S2(q;r)<sub>g</sub>} for said instance r and said target output values A(q;ref)<sub>g</sub>, computed using the reference set of weight coefficients, has a magnitude that is no greater than said selected positive error threshold number ε(thr; 1).</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
    <div class="claim-text">7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:
<div class="claim-text">receiving a new SASP containing an unknown word, referred to as a “new” word, number q′;</div>
<div class="claim-text">estimating where the new SASP begins;</div>
<div class="claim-text">providing signal values of the received signal for the new SASP within each of said J temporal windows, numbered j=1, . . . , J with J&gt;2, that are shifted in time relative to each other by multiples of a selected displacement time Δt(displ);</div>
<div class="claim-text">for the signal values within each of the time-shifted windows, numbered j=1, . . . , J:
<div class="claim-text">transforming each of the signal values of the new SASP, using said Signal Processing Transform operation;</div>
<div class="claim-text">providing a matrix M′ with entries equal to said SPTs, ordered according to said at least one transform parameter along a first matrix axis;</div>
<div class="claim-text">tessellating the matrix M′ into said sequence of exhaustive and mutually exclusive cells that correspond to said cells for said tessellated matrix M;</div>
<div class="claim-text">providing, for each cell in the matrix M′, a cell representative value or “feature”, depending upon at least one of the matrix entries within the cell; and</div>
<div class="claim-text">organizing the features into a vector V′ with entry values v′<sub>k </sub>(k=1, . . . , K);</div>
</div>
<div class="claim-text">applying said neural net classifier and said reference set of said weight coefficients to compute said neural net classifier output values for each of the time-shifted sequences of the new SASP;</div>
<div class="claim-text">forming comparison differences between said neural net classifier target output values and said neural net classifier output values for each of the time-shifted sequences of the new SASP;</div>
<div class="claim-text">for each of said time shifted sequences of the new SASP, choosing at least one word, number q″, in said database for which the magnitude of the comparison difference is no greater than a magnitude of the comparison difference for any other word in said database, and accumulating a weighted point each time the at least one word number q″ is chosen; and</div>
<div class="claim-text">when the at least one word number q″ has a higher number of weighted points than the number of weighted points for any other word in said database, interpreting this condition as indicating that the new SASP includes the word number q″.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
    <div class="claim-text">8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising replacing at least one of said features for said matrix M′ by a normalized feature for at least one of said cells for said matrix M′.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
    <div class="claim-text">9. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<div class="claim-text">when at least two distinct words, number q1 and q2, in said database have substantially equal numbers of said weighted points and the substantially equal numbers of said weighted points of the at least two words, number q1 and q2, are substantially greater than said number of said weighted points of any other word in said database, interpreting this condition as indicating that said new word included in said new SASP cannot be unambiguously identified.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
    <div class="claim-text">10. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising choosing said weighting for said weighted points from the group of weighting formats consisting of (1) substantially uniform weighting and (ii) a weighting that decreases monotonically as said magnitude of said comparison difference increases.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
    <div class="claim-text">11. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein said steps of applying said neural net classifier and forming said comparison differences comprise:
<div class="claim-text">forming a sum S1′(q′;j;ref)<sub>h </sub>over said index k of a product of said first reference set of weight coefficient w<sub>1,k,h</sub>(q′;j;ref) and said vector entries v′<sub>k</sub>, and computing an activation function value A1{S1′(q′;j;ref)<sub>h</sub>} depending upon the sum S1′(q′;j;ref)<sub>h</sub>};</div>
<div class="claim-text">forming a sum S2′(q′;j;ref)<sub>g </sub>over said index h of a product of said second reference set of weight coefficient w<sub>2,h,g</sub>(q′;j;ref) and the activation function values A1{S1′(q′;j;ref)<sub>h</sub>} and computing an activation function value A2{S2′(q′;j;ref)<sub>g</sub>} depending upon the sum S2′(q′;j;ref)<sub>g</sub>}; and</div>
<div class="claim-text">forming a comparison difference between the activation function value A2{S2′(q′;j;ref)<sub>g</sub>} for the new SASP and said target output value A(q;ref)<sub>g </sub>for at least one of said words, number q, in said database.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
    <div class="claim-text">12. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising determining said reference set of said weight coefficients to be independent of said word number q in said database.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
    <div class="claim-text">13. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising determining said reference set of said weight coefficients so that at least one reference set weight coefficient for a first selected word number q1 in said database differs from a corresponding reference set weight coefficient for a second selected word number q2 in said database.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
    <div class="claim-text">14. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising selecting said window width Δt(win) in a range 1-4 sec.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
    <div class="claim-text">15. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising selecting each of said matrix cells to be rectangularly shaped.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
    <div class="claim-text">16. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising selecting at least two of said matrix cells to have different sizes.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
    <div class="claim-text">17. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising determining said matrix cells by at least one of a first order statistical coefficient and a second order statistical coefficient for said entries in that cell.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
    <div class="claim-text">18. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising:
<div class="claim-text">selecting said first order and said second order statistical coefficients to be a statistical mean μ and a standard deviation σ, respectively for a cell; and</div>
<div class="claim-text">including in at least one of said cells a collection of contiguous entry values u of said matrix for which a normalized difference |u−μ|/σ is no greater than a selected positive number.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
    <div class="claim-text">19. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising choosing said SPT operations from the group of SPT operations consisting of (i) a windowed short time interval Fourier Transform (STFT); (ii) discrete wavelets (DWTs) and continuous wavelets (CWTs) using Daubechies 5 and 7 bases; (iii) dual tree wavelets (DTWTs) with a near_sym_a 5,7 tap filter and a Q-shift 14,14 tap filter; (iv) Hartley Transform; (v) Linear Predictive Coding (LPC) coefficients; (vi) a moving average of a selected number of said sample values with uniform weighting; and (vii) a moving average of a selected number of said sample values with non-uniform weighting.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
    <div class="claim-text">20. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising selecting said database to include at least one of the words “stop”, “go”, “left”, “right”, “alpha”, “omega”, “one”, “two”, “three”, “four”, “five”, “six”, “seven”, “eight”, “nine” and “ten.”</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
    <div class="claim-text">21. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising selecting said error threshold number to lie in a range ε(thr;1)≦0.01.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
    <div class="claim-text">22. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising applying a back propagation of error method in said neural net classifier analysis of said features of said cells of said matrix M.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00023" num="00023" class="claim">
    <div class="claim-text">23. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:
<div class="claim-text">receiving a new SASP signal containing an unknown word, referred to as a “new” word, number q′;</div>
<div class="claim-text">estimating where a new SASP begins in the new SASP signal;</div>
<div class="claim-text">providing signal values of the received new signal for the new SASP within each of said J temporal windows, numbered j=1, . . . , J with J≧2, that are shifted in time relative to each other by selected multiples of a selected displacement time Δt(displ);</div>
<div class="claim-text">for the signal values within each of the time-shifted windows, numbered j=1, . . . , J:
<div class="claim-text">transforming each of the signal values of the new SASP, using said Signal Processing Transform operation;</div>
<div class="claim-text">providing a matrix M′ with entries equal to said SPTs, ordered according to said at least one transform parameter along a first matrix axis;</div>
<div class="claim-text">tessellating the matrix M′ into said sequence of exhaustive and mutually exclusive cells that correspond to said cells for said tessellated matrix M;</div>
<div class="claim-text">providing, for each cell in the matrix M′, a cell representative value or “feature”, depending upon at least one of the matrix entries within the cell; and</div>
<div class="claim-text">organizing the features into a vector V′ with entry values v′<sub>k </sub>(k=1, . . . , K);</div>
</div>
<div class="claim-text">applying said neural net classifier and said reference set of said weight coefficients to compute said neural net classifier output values for each of the time-shifted sequences of the new SASP;</div>
<div class="claim-text">forming said comparison differences between said neural net classifier target output values and said neural net classifier output values for each of the time-shifted sequences of the new SASP;</div>
<div class="claim-text">for each of said time shifted sequences of the new SASP, choosing at least one word, number q″, in said database for which (i) the magnitude of the comparison difference is no greater than the comparison difference for any other word in said database and (ii) the magnitude of the comparison difference is no greater than a second selected positive error threshold ε(thr;2), and accumulating a weighted point each time the at least one word number q″ is chosen; and</div>
<div class="claim-text">when the at least one word number q″ has a higher number of weighted points than the number of weighted points for any other word in said database, interpreting this condition as indicating that the new SASP includes the word number q″.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00024" num="00024" class="claim">
    <div class="claim-text">24. A system for communicating information in an environment that interferes with audible speech, the system comprising:
<div class="claim-text">a receiver-evaluator for receiving at least one of a first sub-audible word or phrase (SAWP) and a second SAWP and for determining if the SAWP is a word or phrase in a SAWP lexicon associated with the evaluator; and</div>
<div class="claim-text">a transmitter for transmitting a SAWP representing at least one of the first and second received SAWPs over a signaling channel to at least one recipient, when the received SAWP is determined to be contained in the SAWP lexicon,</div>
<div class="claim-text">wherein the SAWPs associated with first and second received SAWPs are respective first and second representational SAWPs (Rep-SAWPs) that are chosen to provide optimal likelihood of recognition of the transmitted SAWPs in an ambient environment.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00025" num="00025" class="claim">
    <div class="claim-text">25. The system of <claim-ref idref="CLM-00024">claim 24</claim-ref>, further comprising a SAWP receiver, associated with said at least one recipient, for receiving and converting said SAWP to at least one of visually perceptible alphanumeric text, audibly perceptible speech, and a coded signal representing said received SAWP.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00026" num="00026" class="claim">
    <div class="claim-text">26. The system of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein said SAWP lexicon includes at least one of the following words and phrases: “evacuate,” “Mayday,” “mantrap,” “fire safe,” “stop,” “go,” “left,” “right,” “alpha,” “omega,” “zero,” “one,” “two,” “three,” “four,” “five,” “six,” “seven,” “eight,” and “nine.”</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00027" num="00027" class="claim">
    <div class="claim-text">27. The system of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein at least one of said first and second Rep-SAWPs is chosen to be at least one of an encoded SAWP, an encrypted SAWP and a SAWP that is transformed to a format that resists signal degradation when transmitted in said ambient environment.</div>
  </div>
</div> </div>
  </div>
  </section>

  <section itemprop="application" itemscope>

    <section itemprop="metadata" itemscope>
        <span itemprop="applicationNumber">US11/169,265</span>
        <span itemprop="priorityDate">2005-06-24</span>
        <span itemprop="filingDate">2005-06-24</span>
        <span itemprop="title">Applications of sub-audible speech recognition based upon electromyographic signals 
       </span>
        <span itemprop="ifiStatus">Active</span>
        <span itemprop="ifiExpiration">2026-11-09</span>
        <a href="/patent/US7574357B1/en">
            <span itemprop="representativePublication">US7574357B1</span>
            (<span itemprop="primaryLanguage">en</span>)
        </a>
    </section>

    <h2>Priority Applications (1)</h2>
        <table>
            <thead>
                <tr>
                    <th>Application Number</th>
                    <th>Priority Date</th>
                    <th>Filing Date</th>
                    <th>Title</th>
                </tr>
            </thead>
            <tbody>
            <tr itemprop="priorityApps" itemscope repeat>
                <td>
                   <span itemprop="applicationNumber">US11/169,265</span>
                   
                   <a href="/patent/US7574357B1/en">
                        <span itemprop="representativePublication">US7574357B1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                <td itemprop="priorityDate">2005-06-24</td>
                <td itemprop="filingDate">2005-06-24</td>
                <td itemprop="title">Applications of sub-audible speech recognition based upon electromyographic signals 
       </td>
              </tr>
           </tbody>
       </table>

    <h2>Applications Claiming Priority (1)</h2>
        <table>
            <thead>
                <tr>
                    <th>Application Number</th>
                    <th>Priority Date</th>
                    <th>Filing Date</th>
                    <th>Title</th>
                </tr>
            </thead>
            <tbody>
            <tr itemprop="appsClaimingPriority" itemscope repeat>
                <td>
                   <span itemprop="applicationNumber">US11/169,265</span>
                   <a href="/patent/US7574357B1/en">
                        <span itemprop="representativePublication">US7574357B1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                <td itemprop="priorityDate">2005-06-24</td>
                <td itemprop="filingDate">2005-06-24</td>
                <td itemprop="title">Applications of sub-audible speech recognition based upon electromyographic signals 
       </td>
              </tr>
           </tbody>
       </table>

    

    

    <h2>Publications (1)</h2>
        <table>
            <thead>
                <tr>
                    <th>Publication Number</th>
                    <th>Publication Date</th>
                </tr>
            </thead>
            <tbody>
            <tr itemprop="pubs" itemscope repeat>
                <td>
                   <span itemprop="publicationNumber">US7574357B1</span>
                   
                   <span itemprop="thisPatent">true</span>
                   <a href="/patent/US7574357B1/en">US7574357B1
                       (<span itemprop="primaryLanguage">en</span>)
                   </a>
                </td>
                <td itemprop="publicationDate">2009-08-11</td>
              </tr>
           </tbody>
        </table>

  </section>

  <section itemprop="family" itemscope>
    <h1>Family</h1>
    <h2>ID=40934370</h2>

    <h2>Family Applications (1)</h2>
        <table>
            <thead>
                <tr>
                    <th>Application Number</th>
                    <th>Title</th>
                    <th>Priority Date</th>
                    <th>Filing Date</th>
                </tr>
            </thead>
            <tbody>
            <tr itemprop="applications" itemscope repeat>
                <td>
                    <span itemprop="applicationNumber">US11/169,265</span>
                    <span itemprop="ifiStatus">Active</span>
                    <span itemprop="ifiExpiration">2026-11-09</span>
                    <a href="/patent/US7574357B1/en">
                        <span itemprop="representativePublication">US7574357B1</span>
                          (<span itemprop="primaryLanguage">en</span>)
                      </a>
                </td>
                <td itemprop="priorityDate">2005-06-24</td>
                <td itemprop="filingDate">2005-06-24</td>
                <td itemprop="title">Applications of sub-audible speech recognition based upon electromyographic signals 
       </td>
              </tr>
           </tbody>
        </table>

    

    

    <h2>Country Status (1)</h2>
      <table>
        <thead>
          <tr>
            <th>Country</th>
            <th>Link</th>
          </tr>
        </thead>
        <tbody>
        <tr itemprop="countryStatus" itemscope repeat>
            <td>
              <span itemprop="countryCode">US</span>
                (<span itemprop="num">1</span>)
              <meta itemprop="thisCountry" content="true">
            </td>
            <td>
              <a href="/patent/US7574357B1/en">
                <span itemprop="representativePublication">US7574357B1</span>
                  (<span itemprop="primaryLanguage">en</span>)
              </a>
            </td>
          </tr>
      </tbody>
    </table>

    <h2>Cited By (15)</h2>
    <table>
      <caption>* Cited by examiner, † Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20060190822A1/en">
              <span itemprop="publicationNumber">US20060190822A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2005-02-22</td>
          <td itemprop="publicationDate">2006-08-24</td>
          <td><span itemprop="assigneeOriginal">International Business Machines Corporation</span></td>
          <td itemprop="title">Predictive user modeling in user interface design 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20070106501A1/en">
              <span itemprop="publicationNumber">US20070106501A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2005-11-07</td>
          <td itemprop="publicationDate">2007-05-10</td>
          <td><span itemprop="assigneeOriginal">General Electric Company</span></td>
          <td itemprop="title">System and method for subvocal interactions in radiology dictation and UI commands 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20080010071A1/en">
              <span itemprop="publicationNumber">US20080010071A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2006-07-07</td>
          <td itemprop="publicationDate">2008-01-10</td>
          <td><span itemprop="assigneeOriginal">Michael Callahan</span></td>
          <td itemprop="title">Neural translator 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20080270126A1/en">
              <span itemprop="publicationNumber">US20080270126A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2005-10-28</td>
          <td itemprop="publicationDate">2008-10-30</td>
          <td><span itemprop="assigneeOriginal">Electronics And Telecommunications Research Institute</span></td>
          <td itemprop="title">Apparatus for Vocal-Cord Signal Recognition and Method Thereof 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20100075281A1/en">
              <span itemprop="publicationNumber">US20100075281A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2009-11-13</td>
          <td itemprop="publicationDate">2010-03-25</td>
          <td><span itemprop="assigneeOriginal">Manuel-Devadoss Johnson Smith</span></td>
          <td itemprop="title">In-Flight Entertainment Phonetic Language Translation System using Brain Interface 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20110125483A1/en">
              <span itemprop="publicationNumber">US20110125483A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2009-11-20</td>
          <td itemprop="publicationDate">2011-05-26</td>
          <td><span itemprop="assigneeOriginal">Manuel-Devadoss Johnson Smith Johnson</span></td>
          <td itemprop="title">Automated Speech Translation System using Human Brain Language Areas Comprehension Capabilities 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20110246187A1/en">
              <span itemprop="publicationNumber">US20110246187A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2008-12-16</td>
          <td itemprop="publicationDate">2011-10-06</td>
          <td><span itemprop="assigneeOriginal">Koninklijke Philips Electronics N.V.</span></td>
          <td itemprop="title">Speech signal processing 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US8442821B1/en">
              <span itemprop="publicationNumber">US8442821B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2012-07-27</td>
          <td itemprop="publicationDate">2013-05-14</td>
          <td><span itemprop="assigneeOriginal">Google Inc.</span></td>
          <td itemprop="title">Multi-frame prediction for hybrid neural network/hidden Markov models 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US8484022B1/en">
              <span itemprop="publicationNumber">US8484022B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2012-07-27</td>
          <td itemprop="publicationDate">2013-07-09</td>
          <td><span itemprop="assigneeOriginal">Google Inc.</span></td>
          <td itemprop="title">Adaptive auto-encoders 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20130297301A1/en">
              <span itemprop="publicationNumber">US20130297301A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2012-05-03</td>
          <td itemprop="publicationDate">2013-11-07</td>
          <td><span itemprop="assigneeOriginal">Motorola Mobility, Inc.</span></td>
          <td itemprop="title">Coupling an electronic skin tattoo to a mobile communication device 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20130294617A1/en">
              <span itemprop="publicationNumber">US20130294617A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2012-05-03</td>
          <td itemprop="publicationDate">2013-11-07</td>
          <td><span itemprop="assigneeOriginal">Motorola Mobility Llc</span></td>
          <td itemprop="title">Coupling an Electronic Skin Tattoo to a Mobile Communication Device 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20140278432A1/en">
              <span itemprop="publicationNumber">US20140278432A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2013-03-14</td>
          <td itemprop="publicationDate">2014-09-18</td>
          <td><span itemprop="assigneeOriginal">Dale D. Harman</span></td>
          <td itemprop="title">Method And Apparatus For Providing Silent Speech 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US9240184B1/en">
              <span itemprop="publicationNumber">US9240184B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            
            
          </td>
          <td itemprop="priorityDate">2012-11-15</td>
          <td itemprop="publicationDate">2016-01-19</td>
          <td><span itemprop="assigneeOriginal">Google Inc.</span></td>
          <td itemprop="title">Frame-level combination of deep neural network and gaussian mixture models 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20170229125A1/en">
              <span itemprop="publicationNumber">US20170229125A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2016-02-05</td>
          <td itemprop="publicationDate">2017-08-10</td>
          <td><span itemprop="assigneeOriginal">Honeywell International Inc.</span></td>
          <td itemprop="title">Systems and methods for contacting emergency personnel via voice recognition 
       </td>
        </tr><tr itemprop="forwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US10394958B2/en">
              <span itemprop="publicationNumber">US10394958B2</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2017-11-09</td>
          <td itemprop="publicationDate">2019-08-27</td>
          <td><span itemprop="assigneeOriginal">Conduent Business Services, Llc</span></td>
          <td itemprop="title">Performing semantic analyses of user-generated text content using a lexicon 
       </td>
        </tr>
      </tbody>
    </table>

    

    <h2>Citations (5)</h2>
    <table>
      <caption>* Cited by examiner, † Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US6151571A/en">
              <span itemprop="publicationNumber">US6151571A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">1999-08-31</td>
          <td itemprop="publicationDate">2000-11-21</td>
          <td><span itemprop="assigneeOriginal">Andersen Consulting</span></td>
          <td itemprop="title">System, method and article of manufacture for detecting emotion in voice signals through analysis of a plurality of voice signal parameters 
       </td>
        </tr><tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US6182039B1/en">
              <span itemprop="publicationNumber">US6182039B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">1998-03-24</td>
          <td itemprop="publicationDate">2001-01-30</td>
          <td><span itemprop="assigneeOriginal">Matsushita Electric Industrial Co., Ltd.</span></td>
          <td itemprop="title">Method and apparatus using probabilistic language model based on confusable sets for speech recognition 
       </td>
        </tr><tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US6366908B1/en">
              <span itemprop="publicationNumber">US6366908B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">1999-06-28</td>
          <td itemprop="publicationDate">2002-04-02</td>
          <td><span itemprop="assigneeOriginal">Electronics And Telecommunications Research Institute</span></td>
          <td itemprop="title">Keyfact-based text retrieval system, keyfact-based text index method, and retrieval method 
       </td>
        </tr><tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20040044517A1/en">
              <span itemprop="publicationNumber">US20040044517A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2002-08-30</td>
          <td itemprop="publicationDate">2004-03-04</td>
          <td><span itemprop="assigneeOriginal">Robert Palmquist</span></td>
          <td itemprop="title">Translation system 
       </td>
        </tr><tr itemprop="backwardReferencesOrig" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20060129394A1/en">
              <span itemprop="publicationNumber">US20060129394A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2004-12-09</td>
          <td itemprop="publicationDate">2006-06-15</td>
          <td><span itemprop="assigneeOriginal">International Business Machines Corporation</span></td>
          <td itemprop="title">Method for communicating using synthesized speech 
       </td>
        </tr>
      </tbody>
    </table>

    

    
    <ul>
      
      <li itemprop="applicationsByYear" itemscope repeat>
        <span itemprop="year">2005</span>
        <ul>
          
          <li itemprop="application" itemscope repeat>
            <span itemprop="filingDate">2005-06-24</span>
            <span itemprop="countryCode">US</span>
            <span itemprop="applicationNumber">US11/169,265</span>
            <a href="/patent/US7574357B1/en"><span itemprop="documentId">patent/US7574357B1/en</span></a>
            <span itemprop="legalStatusCat">active</span>
            <span itemprop="legalStatus">Active</span>
            
            <span itemprop="thisApp" content="true" bool></span>
            
          </li>
          
        </ul>
      </li>
      
    </ul>
    

    </section>

  <section>
    <h2>Patent Citations (5)</h2>
    <table>
      <caption>* Cited by examiner, † Cited by third party</caption>
      <thead>
        <tr>
          <th>Publication number</th>
          <th>Priority date</th>
          <th>Publication date</th>
          <th>Assignee</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="backwardReferences" itemscope repeat>
          <td>
            
            
            <a href="/patent/US6182039B1/en">
              <span itemprop="publicationNumber">US6182039B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">1998-03-24</td>
          <td itemprop="publicationDate">2001-01-30</td>
          <td><span itemprop="assigneeOriginal">Matsushita Electric Industrial Co., Ltd.</span></td>
          <td itemprop="title">Method and apparatus using probabilistic language model based on confusable sets for speech recognition 
       </td>
        </tr><tr itemprop="backwardReferences" itemscope repeat>
          <td>
            
            
            <a href="/patent/US6366908B1/en">
              <span itemprop="publicationNumber">US6366908B1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">1999-06-28</td>
          <td itemprop="publicationDate">2002-04-02</td>
          <td><span itemprop="assigneeOriginal">Electronics And Telecommunications Research Institute</span></td>
          <td itemprop="title">Keyfact-based text retrieval system, keyfact-based text index method, and retrieval method 
       </td>
        </tr><tr itemprop="backwardReferences" itemscope repeat>
          <td>
            
            
            <a href="/patent/US6151571A/en">
              <span itemprop="publicationNumber">US6151571A</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">1999-08-31</td>
          <td itemprop="publicationDate">2000-11-21</td>
          <td><span itemprop="assigneeOriginal">Andersen Consulting</span></td>
          <td itemprop="title">System, method and article of manufacture for detecting emotion in voice signals through analysis of a plurality of voice signal parameters 
       </td>
        </tr><tr itemprop="backwardReferences" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20040044517A1/en">
              <span itemprop="publicationNumber">US20040044517A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2002-08-30</td>
          <td itemprop="publicationDate">2004-03-04</td>
          <td><span itemprop="assigneeOriginal">Robert Palmquist</span></td>
          <td itemprop="title">Translation system 
       </td>
        </tr><tr itemprop="backwardReferences" itemscope repeat>
          <td>
            
            
            <a href="/patent/US20060129394A1/en">
              <span itemprop="publicationNumber">US20060129394A1</span>
              (<span itemprop="primaryLanguage">en</span>)
            </a>
            <span itemprop="examinerCited">*</span>
            
          </td>
          <td itemprop="priorityDate">2004-12-09</td>
          <td itemprop="publicationDate">2006-06-15</td>
          <td><span itemprop="assigneeOriginal">International Business Machines Corporation</span></td>
          <td itemprop="title">Method for communicating using synthesized speech 
       </td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Non-Patent Citations (7)</h2>
    <table>
      <caption>* Cited by examiner, † Cited by third party</caption>
      <thead>
        <tr>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Brady, et al., Multisensor MELPe Using Parameter Substitution, IEEE International Conference on Acoustics, Speech, and Signal Processing, May 17-21, 2004, I-477-480, 1, IEEE.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Graciarena, et al., Combining Standard and Throat Microphones for Robust Speech Recognition, IEEE Signal Processing Letters, Mar. 2003, 72-74, 10-3, IEEE.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Jou, et al., Whispery Speech Recognition Using Adapted Articulatory, IEEE International Conference on Acoustics, Speech, and Signal Processing, Mar. 18-23, 2005, 1009-1012, 1, IEEE.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Junqua, et al., The Lombard Effect: A Reflex to Better Communicate With Others in Noise, IEEE Internationl Conference on Acoustics, Speech, and Signal Processing, Mar. 15-19, 1999, 2083-2086, 4, IEEE.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Manabe et al. '<a href='http://scholar.google.com/scholar?q="Unvoiced+Speech+Recognition+using+EMG-Mime+Speech+Recognition-%27%2C+CHI+"'>Unvoiced Speech Recognition using EMG-Mime Speech Recognition-', CHI </a>'03 extended abstracts on Human factors in computing systems, Ft. Lauderdale, Florida, pp. 794-795.</span>
            <span itemprop="examinerCited">*</span>
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">NG, et al., Denoising of Human Speech Using Combined Acoustic and EM Sensorsignal Processing, IEEE International Conference on Acoustics, Speech, and Signal Processing, Jun. 5-9, 2000, 229-232, 1, IEEE.</span>
            
            
          </td>
        </tr><tr itemprop="detailedNonPatentLiterature" itemscope repeat>
          <td>
            <span itemprop="title">Shahina, et al., Language Identification in Noisy Environments Using Throat Microphone Signals, Conference on Intelligent Sensing and Information Processing, Jan. 4, 2005, 400-403, IEEE.</span>
            
            
          </td>
        </tr>
      </tbody>
    </table>
  </section>

  <h2>Cited By (24)</h2>
  <table>
    <caption>* Cited by examiner, † Cited by third party</caption>
    <thead>
      <tr>
        <th>Publication number</th>
        <th>Priority date</th>
        <th>Publication date</th>
        <th>Assignee</th>
        <th>Title</th>
      </tr>
    </thead>
    <tbody>
      <tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US9165280B2/en">
            <span itemprop="publicationNumber">US9165280B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2005-02-22</td>
        <td itemprop="publicationDate">2015-10-20</td>
        <td><span itemprop="assigneeOriginal">International Business Machines Corporation</span></td>
        <td itemprop="title">Predictive user modeling in user interface design 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20060190822A1/en">
            <span itemprop="publicationNumber">US20060190822A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2005-02-22</td>
        <td itemprop="publicationDate">2006-08-24</td>
        <td><span itemprop="assigneeOriginal">International Business Machines Corporation</span></td>
        <td itemprop="title">Predictive user modeling in user interface design 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20080270126A1/en">
            <span itemprop="publicationNumber">US20080270126A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2005-10-28</td>
        <td itemprop="publicationDate">2008-10-30</td>
        <td><span itemprop="assigneeOriginal">Electronics And Telecommunications Research Institute</span></td>
        <td itemprop="title">Apparatus for Vocal-Cord Signal Recognition and Method Thereof 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20070106501A1/en">
            <span itemprop="publicationNumber">US20070106501A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2005-11-07</td>
        <td itemprop="publicationDate">2007-05-10</td>
        <td><span itemprop="assigneeOriginal">General Electric Company</span></td>
        <td itemprop="title">System and method for subvocal interactions in radiology dictation and UI commands 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US10162818B2/en">
            <span itemprop="publicationNumber">US10162818B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2006-07-07</td>
        <td itemprop="publicationDate">2018-12-25</td>
        <td><span itemprop="assigneeOriginal">Ambient Corporation</span></td>
        <td itemprop="title">Neural translator 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20080010071A1/en">
            <span itemprop="publicationNumber">US20080010071A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2006-07-07</td>
        <td itemprop="publicationDate">2008-01-10</td>
        <td><span itemprop="assigneeOriginal">Michael Callahan</span></td>
        <td itemprop="title">Neural translator 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US8529473B2/en">
            <span itemprop="publicationNumber">US8529473B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2006-07-07</td>
        <td itemprop="publicationDate">2013-09-10</td>
        <td><span itemprop="assigneeOriginal">Michael Callahan</span></td>
        <td itemprop="title">Neural translator 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US9772997B2/en">
            <span itemprop="publicationNumber">US9772997B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2006-07-07</td>
        <td itemprop="publicationDate">2017-09-26</td>
        <td><span itemprop="assigneeOriginal">Ambient Corporation</span></td>
        <td itemprop="title">Neural translator 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US8251924B2/en">
            <span itemprop="publicationNumber">US8251924B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2006-07-07</td>
        <td itemprop="publicationDate">2012-08-28</td>
        <td><span itemprop="assigneeOriginal">Ambient Corporation</span></td>
        <td itemprop="title">Neural translator 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20150213003A1/en">
            <span itemprop="publicationNumber">US20150213003A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2006-07-07</td>
        <td itemprop="publicationDate">2015-07-30</td>
        <td><span itemprop="assigneeOriginal">Michael Callahan</span></td>
        <td itemprop="title">Neural translator 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US8949129B2/en">
            <span itemprop="publicationNumber">US8949129B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2006-07-07</td>
        <td itemprop="publicationDate">2015-02-03</td>
        <td><span itemprop="assigneeOriginal">Ambient Corporation</span></td>
        <td itemprop="title">Neural translator 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20110246187A1/en">
            <span itemprop="publicationNumber">US20110246187A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2008-12-16</td>
        <td itemprop="publicationDate">2011-10-06</td>
        <td><span itemprop="assigneeOriginal">Koninklijke Philips Electronics N.V.</span></td>
        <td itemprop="title">Speech signal processing 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20100075281A1/en">
            <span itemprop="publicationNumber">US20100075281A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2009-11-13</td>
        <td itemprop="publicationDate">2010-03-25</td>
        <td><span itemprop="assigneeOriginal">Manuel-Devadoss Johnson Smith</span></td>
        <td itemprop="title">In-Flight Entertainment Phonetic Language Translation System using Brain Interface 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/USH2269H1/en">
            <span itemprop="publicationNumber">USH2269H1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2009-11-20</td>
        <td itemprop="publicationDate">2012-06-05</td>
        <td><span itemprop="assigneeOriginal">Manuel-Devadoss Johnson Smith Johnson</span></td>
        <td itemprop="title">Automated speech translation system using human brain language areas comprehension capabilities 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20110125483A1/en">
            <span itemprop="publicationNumber">US20110125483A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2009-11-20</td>
        <td itemprop="publicationDate">2011-05-26</td>
        <td><span itemprop="assigneeOriginal">Manuel-Devadoss Johnson Smith Johnson</span></td>
        <td itemprop="title">Automated Speech Translation System using Human Brain Language Areas Comprehension Capabilities 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20130294617A1/en">
            <span itemprop="publicationNumber">US20130294617A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2012-05-03</td>
        <td itemprop="publicationDate">2013-11-07</td>
        <td><span itemprop="assigneeOriginal">Motorola Mobility Llc</span></td>
        <td itemprop="title">Coupling an Electronic Skin Tattoo to a Mobile Communication Device 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20130297301A1/en">
            <span itemprop="publicationNumber">US20130297301A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2012-05-03</td>
        <td itemprop="publicationDate">2013-11-07</td>
        <td><span itemprop="assigneeOriginal">Motorola Mobility, Inc.</span></td>
        <td itemprop="title">Coupling an electronic skin tattoo to a mobile communication device 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US8484022B1/en">
            <span itemprop="publicationNumber">US8484022B1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2012-07-27</td>
        <td itemprop="publicationDate">2013-07-09</td>
        <td><span itemprop="assigneeOriginal">Google Inc.</span></td>
        <td itemprop="title">Adaptive auto-encoders 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US8442821B1/en">
            <span itemprop="publicationNumber">US8442821B1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2012-07-27</td>
        <td itemprop="publicationDate">2013-05-14</td>
        <td><span itemprop="assigneeOriginal">Google Inc.</span></td>
        <td itemprop="title">Multi-frame prediction for hybrid neural network/hidden Markov models 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US9240184B1/en">
            <span itemprop="publicationNumber">US9240184B1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          
          
        </td>
        <td itemprop="priorityDate">2012-11-15</td>
        <td itemprop="publicationDate">2016-01-19</td>
        <td><span itemprop="assigneeOriginal">Google Inc.</span></td>
        <td itemprop="title">Frame-level combination of deep neural network and gaussian mixture models 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20140278432A1/en">
            <span itemprop="publicationNumber">US20140278432A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2013-03-14</td>
        <td itemprop="publicationDate">2014-09-18</td>
        <td><span itemprop="assigneeOriginal">Dale D. Harman</span></td>
        <td itemprop="title">Method And Apparatus For Providing Silent Speech 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US20170229125A1/en">
            <span itemprop="publicationNumber">US20170229125A1</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2016-02-05</td>
        <td itemprop="publicationDate">2017-08-10</td>
        <td><span itemprop="assigneeOriginal">Honeywell International Inc.</span></td>
        <td itemprop="title">Systems and methods for contacting emergency personnel via voice recognition 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US10062387B2/en">
            <span itemprop="publicationNumber">US10062387B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2016-02-05</td>
        <td itemprop="publicationDate">2018-08-28</td>
        <td><span itemprop="assigneeOriginal">Honeywell International Inc.</span></td>
        <td itemprop="title">Systems and methods for contacting emergency personnel via voice recognition 
       </td>
      </tr><tr itemprop="forwardReferences" itemscope repeat>
        <td>
          
          
          <a href="/patent/US10394958B2/en">
            <span itemprop="publicationNumber">US10394958B2</span>
            (<span itemprop="primaryLanguage">en</span>)
          </a>
          <span itemprop="examinerCited">*</span>
          
        </td>
        <td itemprop="priorityDate">2017-11-09</td>
        <td itemprop="publicationDate">2019-08-27</td>
        <td><span itemprop="assigneeOriginal">Conduent Business Services, Llc</span></td>
        <td itemprop="title">Performing semantic analyses of user-generated text content using a lexicon 
       </td>
      </tr>
    </tbody>
  </table>

  

  <section>
    <h2>Similar Documents</h2>
    <table>
      <thead>
        <tr>
          <th>Publication</th>
          <th>Publication Date</th>
          <th>Title</th>
        </tr>
      </thead>
      <tbody>
        <tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="8924138186127237972">
              <a href="/scholar/8924138186127237972"><span itemprop="scholarAuthors">Hermansky et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="1998">1998</time>
            
          </td>
          <td itemprop="title">Traps-classifiers of temporal patterns</td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/US7283962B2/en">
                <span itemprop="publicationNumber">US7283962B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2007-10-16">2007-10-16</time>
            
            
          </td>
          <td itemprop="title">Methods and systems for detecting, measuring, and monitoring stress in speech 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="3292226122407686292">
              <a href="/scholar/3292226122407686292"><span itemprop="scholarAuthors">Lu et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2011">2011</time>
            
          </td>
          <td itemprop="title">Speakersense: Energy efficient unobtrusive speaker identification on mobile phones</td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/DE4436692C2/en">
                <span itemprop="publicationNumber">DE4436692C2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="1998-04-30">1998-04-30</time>
            
            
          </td>
          <td itemprop="title">  Training system for a speech recognition system  </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/EP1536414B1/en">
                <span itemprop="publicationNumber">EP1536414B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2012-05-23">2012-05-23</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus for multi-sensory speech enhancement 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="18334243261987010285">
              <a href="/scholar/18334243261987010285"><span itemprop="scholarAuthors">Campbell et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2009">2009</time>
            
          </td>
          <td itemprop="title">Forensic speaker recognition</td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/JP4851447B2/en">
                <span itemprop="publicationNumber">JP4851447B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2012-01-11">2012-01-11</time>
            
            
          </td>
          <td itemprop="title">Speech analysis apparatus, speech analysis method, and speech analysis program for detecting pitch frequency 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/US20030033145A1/en">
                <span itemprop="publicationNumber">US20030033145A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2003-02-13">2003-02-13</time>
            
            
          </td>
          <td itemprop="title">System, method, and article of manufacture for detecting emotion in voice signals by utilizing statistics for voice signal parameters 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/EP1770687B1/en">
                <span itemprop="publicationNumber">EP1770687B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2017-04-12">2017-04-12</time>
            
            
          </td>
          <td itemprop="title">Detecting emotion in voice signals through analysis of a plurality of voice signal parameters 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/US6804643B1/en">
                <span itemprop="publicationNumber">US6804643B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2004-10-12">2004-10-12</time>
            
            
          </td>
          <td itemprop="title">Speech recognition 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/US7606701B2/en">
                <span itemprop="publicationNumber">US7606701B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2009-10-20">2009-10-20</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus for determining emotional arousal by speech analysis 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="2734042238596173146">
              <a href="/scholar/2734042238596173146"><span itemprop="scholarAuthors">Alsheikh et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2016">2016</time>
            
          </td>
          <td itemprop="title">Deep activity recognition models with triaxial accelerometers</td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/US6868380B2/en">
                <span itemprop="publicationNumber">US6868380B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2005-03-15">2005-03-15</time>
            
            
          </td>
          <td itemprop="title">Speech recognition system and method for generating phonotic estimates 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="13610556696341385038">
              <a href="/scholar/13610556696341385038"><span itemprop="scholarAuthors">Sroka et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2005">2005</time>
            
          </td>
          <td itemprop="title">Human and machine consonant recognition</td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/US7627475B2/en">
                <span itemprop="publicationNumber">US7627475B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2009-12-01">2009-12-01</time>
            
            
          </td>
          <td itemprop="title">Detecting emotions using voice signal analysis 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/US20050004710A1/en">
                <span itemprop="publicationNumber">US20050004710A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2005-01-06">2005-01-06</time>
            
            
          </td>
          <td itemprop="title">Learning equipment and learning method, and robot apparatus 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/JP2012510088A/en">
                <span itemprop="publicationNumber">JP2012510088A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2012-04-26">2012-04-26</time>
            
            
          </td>
          <td itemprop="title">Speech estimation interface and communication system 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/US7822611B2/en">
                <span itemprop="publicationNumber">US7822611B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2010-10-26">2010-10-26</time>
            
            
          </td>
          <td itemprop="title">Speaker intent analysis system 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="14117496366026220354">
              <a href="/scholar/14117496366026220354"><span itemprop="scholarAuthors">Mesgarani et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2006">2006</time>
            
          </td>
          <td itemprop="title">Discrimination of speech from nonspeech based on multiscale spectro-temporal modulations</td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/US20050185813A1/en">
                <span itemprop="publicationNumber">US20050185813A1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2005-08-25">2005-08-25</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus for multi-sensory speech enhancement on a mobile device 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/KR100619215B1/en">
                <span itemprop="publicationNumber">KR100619215B1</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2006-09-06">2006-09-06</time>
            
            
          </td>
          <td itemprop="title">Microphone and communication interface system 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/US4980917A/en">
                <span itemprop="publicationNumber">US4980917A</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="1990-12-25">1990-12-25</time>
            
            
          </td>
          <td itemprop="title">Method and apparatus for determining articulatory parameters from speech data 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="512685569236543653">
              <a href="/scholar/512685569236543653"><span itemprop="scholarAuthors">Litvak et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2008">2008</time>
            
          </td>
          <td itemprop="title">Fall detection of elderly through floor vibrations and sound</td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            <meta itemprop="isPatent" content="true">
              
              
              <a href="/patent/US9330658B2/en">
                <span itemprop="publicationNumber">US9330658B2</span>
                (<span itemprop="primaryLanguage">en</span>)
              </a>
            
            
          </td>
          <td>
            <time itemprop="publicationDate" datetime="2016-05-03">2016-05-03</time>
            
            
          </td>
          <td itemprop="title">User intent analysis extent of speaker intent analysis system 
       </td>
        </tr><tr itemprop="similarDocuments" itemscope repeat>
          <td>
            
            <meta itemprop="isScholar" content="true">
              <meta itemprop="scholarID" content="9980309544097657570">
              <a href="/scholar/9980309544097657570"><span itemprop="scholarAuthors">Li et al.</span></a>
            
          </td>
          <td>
            
            <time itemprop="publicationDate" datetime="2012">2012</time>
            
          </td>
          <td itemprop="title">A microphone array system for automatic fall detection</td>
        </tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Legal Events</h2>
    <table>
      <thead>
        <tr>
          <th>Date</th>
          <th>Code</th>
          <th>Title</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2006-05-09">2006-05-09</time></td>
          <td itemprop="code">AS</td>
          <td itemprop="title">Assignment</td>
          <td>
            
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Owner name</strong>:
              <span itemprop="value">USA AS REPRESENTED BY THE ADMINISTRATOR OF THE NAS</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:JORGENSEN, CHARLES C.;REEL/FRAME:017592/0814</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Effective date</strong>:
              <span itemprop="value">20050627</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2008-06-04">2008-06-04</time></td>
          <td itemprop="code">AS</td>
          <td itemprop="title">Assignment</td>
          <td>
            
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Owner name</strong>:
              <span itemprop="value">USA AS REPRESENTED BY THE ADMINISTRATOR OF THE NAS</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:COMPUTER SCIENCE CORPORATION;REEL/FRAME:021038/0889</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Effective date</strong>:
              <span itemprop="value">20060103</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2009-07-22">2009-07-22</time></td>
          <td itemprop="code">STCF</td>
          <td itemprop="title">Information on status: patent grant</td>
          <td>
            
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">PATENTED CASE</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2013-01-15">2013-01-15</time></td>
          <td itemprop="code">FPAY</td>
          <td itemprop="title">Fee payment</td>
          <td>
            
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Year of fee payment</strong>:
              <span itemprop="value">4</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2017-03-24">2017-03-24</time></td>
          <td itemprop="code">REMI</td>
          <td itemprop="title">Maintenance fee reminder mailed</td>
          <td>
            
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2017-06-05">2017-06-05</time></td>
          <td itemprop="code">AS</td>
          <td itemprop="title">Assignment</td>
          <td>
            
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Owner name</strong>:
              <span itemprop="value">JORGENSEN, CHARLES C., ARIZONA</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Free format text</strong>:
              <span itemprop="value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:USA AS REPRESENTED BY THE ADMINISTRATOR OF THE NASA;REEL/FRAME:042603/0328</span>
            </p>
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Effective date</strong>:
              <span itemprop="value">20170308</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2017-06-12">2017-06-12</time></td>
          <td itemprop="code">FPAY</td>
          <td itemprop="title">Fee payment</td>
          <td>
            
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Year of fee payment</strong>:
              <span itemprop="value">8</span>
            </p>
          </td>
        </tr>
        <tr itemprop="legalEvents" itemscope repeat>
          <td><time itemprop="date" datetime="2017-06-12">2017-06-12</time></td>
          <td itemprop="code">SULP</td>
          <td itemprop="title">Surcharge for late payment</td>
          <td>
            
            <p itemprop="attributes" itemscope repeat>
              <strong itemprop="label">Year of fee payment</strong>:
              <span itemprop="value">7</span>
            </p>
          </td>
        </tr>
      </tbody>
    </table>
  </section>
</article>

    </search-app>
    <script type="text/javascript" src="//www.gstatic.com/feedback/api.js"></script>
    <script async="" defer="" src="//www.google.com/insights/consumersurveys/async_survey?site=cxkjf7ipxgbnnjy6k35ezcvbbe"></script>
  </body>
</html>

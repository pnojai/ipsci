---
title: "ipsci"
author: "TJ"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    code_folding: hide
    highlight: pygments
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, warning=FALSE}

# Packages used in the ingestion of data
library(httr)     # Wrapper for the curl package.
library(fulltext) # Text mining of open access journals.
library(XML)      # Dealing with text in xml.
library(jsonlite) # Getting and formatting JSON. 
library(rplos)    # API to PLOS for journals.

  
# Special use case libraries

library(stats)
library(topicmodels) # for LDA topic modelling 
library(tm) # general text mining functions, making document term matrixes
library(SnowballC) # for stemming
library(tm)       # Text mining.


# Tidy tools
library(plyr)     # Wrangling data. 
#In tidyverse
#library(dplyr) # SQL-style data processing
#library(tidyr)    # Tidying data, 
#library(readr) # reading and writing delimited text files
#library(tibble)   # Tidier table object.
#library(stringr)  # Manipulating strings. Using RegEx.
# read in the libraries we're going to use
library(tidyverse) # general utility & workflow functions 
library(tidytext) # text analysis in R 
library(lubridate) # working with times and dates   

#display
library(kableExtra)


```
 
# Explore and Transform the Data

Examine the patent text files and view the text sections. We note 932 observations, 8 variables, and 2 integer classes and 6 character classes. 
```{r}
patents <- read.csv("data/patent_text.csv", header = TRUE, stringsAsFactors = FALSE)
glimpse(patents)
kable(head(patents, n=2))

```


## Data integrity
Check to make sure we are not operating on empty data. 
We are not. 
```{r}
nrow(patents) == sum(complete.cases(patents))
```

Track our documents in case we need to make extra rows for each word

```{r}
#patents <- patents %>%    
  #mutate(docId=as.integer(row.names(.)))
`%notin%` <- Negate(`%in%`)
if('docId' %notin% colnames(.)) patents<-add_column(patents, docId=as.integer(row.names(patents)), .before = 1)
head(patents, n=1)
```
References
1. https://github.com/tidyverse/dplyr/issues/2047
2. https://tibble.tidyverse.org/reference/add_column.html
3. https://www.r-bloggers.com/the-notin-operator/



## String-focused transformations


### Remove third-party additions to text
```{r}
head(patents$title, n=1)
patents <- patents %>%
  #head(n=1) %>%
  mutate(title = str_remove_all(title, " - Google Patents"))
head(patents$title, n=1)
```

### Titles and Abstracts

1. Extract document ids, titles and abstracts and convert to tibble in order to use tidy text methods
2. Replace common European characters into transliterated roman characters (e.g. Ã¢ is a)
3. Replace upper with lower case characters
4. Tokenize the title and abstract variables into words
5. Remove default English stopword from those words
6. Normalize white-space: Remove leading, trailing and intermediate excess whitespace
7. Remove numbers
Hold off on punctuation (contractions, dashes) and stemming until analysis

```{r} 
patents <- patents %>%
  mutate(title=iconv(title, to='ASCII//TRANSLIT')) %>%
  mutate(title=tolower(title)) %>%
  mutate(title=str_replace_all(title,'\\w*\\d\\w*','')) %>%
  mutate(abstract=iconv(abstract, to='ASCII//TRANSLIT')) %>%
  mutate(abstract=tolower(abstract)) %>%
  mutate(abstract=str_replace_all(abstract,'\\w*\\d\\w*','')) %>%
  mutate_if(is.character, str_squish)
head(patents)
```



## Data-focused transformations
 
### Character statistics
```{r}
patents_stats <- patents %>% 
  rowwise() %>% 
  transmute(title = title, title_nchar = nchar(title), title_nmeans= nchar(title),
            abstract=abstract, abstract_nchar = nchar(abstract), abstract_nmeans= nchar(abstract),
            origin = origin, origin_nchar = nchar(origin), origin_nmeans= nchar(origin),
            background = background, background_nchar = nchar(background), background_nmeans= nchar(background)) %>%
  mutate(title_nmeans=colMeans(.[3]), 
         abs_nmeans=colMeans(.[6]), 
         origin_nmeans=colMeans(.[9]), 
         background_nmeans=colMeans(.[12])) %>%
  cbind(select(patents, -c(title, abstract, origin, background)),.)
head(patents_stats, n=2)
```

### Dates
Store the dates in date format yyyy-mm-dd.
```{r}
patents_stats <- patents_stats %>%
  rowwise() %>%
  mutate(filingDate=ymd(filingDate))
head(patents_stats, n=1)
```


## Final Data Definitions
docId: A unique patent document data record in our sample (NASA)
appId: A unique patent number 'r'
citedBy: Other papers citing this paper, provided by Google
citation: Number of other papers this paper cites
filingDate: The date the patent was filed in yyyy-mm-dd format (for instance a specific version)
title: The patent title with the prepended phrase " - Google Patents" removed
title_nchar [patent_stats]: the number of characters in the title
title_nmeans [patent_stats]: the average number of characters over all of the titles in all patents
abstract: a summary of the patent abstract
abstract_nchar: the number of characters in the abstract
abstract_nmeans: the average number of characters over all of the abstracts in all patents
origin: a summary of how the patent originated
origin_nchar [patent_stats]: the number of characters in the origin
origin_nmeans [patent_stats]: the average number of characters over all of the origins in all patents
background: a summary of the patent background
background_nchar [patent_stats]: the number of characters in the background
background_nmeans [patent_stats]: the average number of characters over all of the backgrounds in all patents


References:
1. https://dplyr.tidyverse.org/reference/mutate.html#grouped-tibbles
2. https://dplyr.tidyverse.org/reference/summarise.html
3. https://github.com/tidyverse/dplyr/issues/2838
4. https://stackoverflow.com/questions/43897844/r-move-column-to-last-using-dplyr




# Data Analysis
After looking into the patent stats, and transforming the data, we choose title and abstract for our Data Analysis.

## Perform Word Tokenization specific to our type of analysis

### Title word operations
```{r message=FALSE}
tb_titles <- patents %>%
  select(., c(docId, title)) %>%
  as_tibble() %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words)
```

### Abstract word operations
```{r message=FALSE}
tb_abstracts <- patents %>%
  select(., c(docId, abstract)) %>%
  as_tibble() %>%
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words)
```



## Show Basic Word statistics
1. Note the most frequent words over all patents
2. Note the most frequent words per patent

### Most frequent words in titles
```{r message=FALSE}
title_words <- tb_titles %>%
  count(word, sort = TRUE)

title_words %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill=word)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  ylab("Word Frequency") +
  ggtitle("Most Common Title Words") 

#head(title_words, 5)
```


### Most frequent words in each title

```{r message=FALSE}
title_word_counts <- tb_titles %>%
  count(docId, word, sort = TRUE)%>%
  ungroup()

head(title_word_counts, 5)

```


### Most frequent words in Abstracts
```{r message=FALSE}
abstract_words <- tb_abstracts %>%
  count(word, sort = TRUE)

abstract_words %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill=word)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  ylab("Word Frequency") +
  ggtitle("Most Common Abstract Words") 

#head(abstract_words, 5)
```

### Most frequent words in each Abstract

```{r message=FALSE}
abstract_word_counts <- tb_abstracts %>%
  count(docId, word, sort = TRUE)%>%
  ungroup()

head(abstract_word_counts, 5)

```

References:
1. https://stackoverflow.com/questions/20495598/replace-accented-characters-in-r-with-non-accented-counterpart-utf-8-encoding
2. https://www.tidytextmining.com/tidytext.html

## Cast to Document Term Matrix

### Title DTM
```{r}
title_desc_dtm <- title_word_counts %>%
  cast_dtm(docId, word, n)

title_desc_dtm
```


### Abstract DTM
```{r}
abstract_desc_dtm <- abstract_word_counts %>%
  cast_dtm(docId, word, n)

abstract_desc_dtm
```

## Perform Abstract LDA and Visualize

### Build the LDA
```{r}
#Disable to avoid unintentional overwrites
abstract_desc_lda <- LDA(abstract_desc_dtm, k = 24, control = list(seed = 1234))
abstract_desc_lda
```


```{r}
#Free memory
gc(abstract_desc_dtm)
```

### Arrange the LDA and Visualize top words per topic
```{r}
tidy_abstract_lda <- tidy(abstract_desc_lda)

top_abstract_terms <- tidy_abstract_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```
 
 
 
```{r fig.width=10, fig.height=20}
top_abstract_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free")

```

 
### Review performance of Abstract LDA per document

Our LDA shows low probability for a large number of documents. 

```{r message=FALSE}
lda_gamma <- tidy(abstract_desc_lda, matrix = "gamma")


ggplot(lda_gamma, aes(gamma)) +
  geom_histogram() +
  scale_y_log10() +
  labs(title = "Distribution of probabilities for all topics",
       y = "Number of documents", x = expression(gamma))
```


```{r warning=FALSE, message=FALSE, fig.align=center, fig.width=10, fig.height=10}
ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 4) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))

```


## TF-IDF 

Measures the importance of a word within the corpus, potentially surfacing not unique words to the document (tf) but those that are not very frequent in the corpus (IDF). We investigate this potentially more reliable method (given the small dataset (corpus)) until we are able to expand or augment the LDA topic analysis and then test it. 

### Visualize the top TD-IDF words in the corpus
```{r message=FALSE}
abstracts_tf_idf <- tb_abstracts %>% 
  count(docId, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, docId, n) 

 
abstracts_tf_idf %>% 
  top_n(10) %>%
  ggplot(aes(word, tf_idf, fill=word)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  ylab("Word TF-IDF") +
  ggtitle("Sample of Most Relevant, Unique Abstract Words in Corpus") 

```

### Find the top one TF-IDF term per document
```{r}
doc_abs_tf_idf <- abstracts_tf_idf %>%
  group_by(docId) %>% 
  filter(tf_idf == max(tf_idf)) %>% 
  slice(1) %>% 
  ungroup() 
head(doc_abs_tf_idf,n=5)
```


### Merge with dataset and extract to csv
Merge
```{r}
doc_terms <- doc_abs_tf_idf %>% transmute(docId=docId, term=word) 
patents_terms <- merge(doc_terms, patents, by='docId')

kable(head(patents_terms, n=2))
```
Extract
```{r}
write.csv(patents_terms,"data/patents_terms.csv")
```


Reference:
1. https://www.tidytextmining.com/tidytext.html



 


 


---
title: "ipsci"
author: "Jai Jeffryes"
author: "Jai Jeffryes, Tamiko Jenkins, Nicholas Chung"
date: "11/22/2019"
output: 
  html_document:
    code_folding: hide
    highlight: pygments
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
# todo: install if not found
library(rplos)    # API to PLOS for journals.
library(readr)    # Reading rectangular data.
library(plyr)     # Wrangling data.
library(dplyr)    # Wrangling data.
library(tibble)   # Tidier table object.
library(tidyr)    # Tidying data,
library(stringr)  # Manipulating strings. Using RegEx.
library(tm)       # Text mining.
library(fulltext) # Text mining of open access journals.
library(XML)      # Dealing with text in xml.
library(httr)     # Wrapper for the curl package.
library(jsonlite) # Getting and formatting JSON.
library(rvest)    # Download and manipulate HTML.
library(rjson)    # Work with JSON format.
library(ggplot2)
library(purrr)
```

## NASA Patents
- Data source: [NASA's Open Data Portal](https://data.nasa.gov/Raw-Data/NASA-Patents/gquh-watm). Includes column specification.
- File: NASA_Patents.csv
- Download date: 11/14/2019

### Notes
- Once you determine the data types of your columns,set the specifications and then use them for reading.
- [`readr` blog post](https://blog.rstudio.com/2016/08/05/readr-1-0-0/). Illustrates the approach, although this syntax doesn't work anymore. The approach writes the specification to a file.
- [readr - how to update col_spec object from spec()](https://stackoverflow.com/questions/39135129/readr-how-to-update-col-spec-object-from-spec). Greg H.'s answer on Stack Overflow doesn't require writing to a file. We use this approach. 

```{r nasa}
nasa_patents_fn <- "NASA_Patents.csv"

# Specify datatypes.
nasa_spec <- spec_csv(file = file.path("data", nasa_patents_fn))
nasa_spec$cols[["Status"]] <- col_factor()
nasa_spec$cols[["Patent Expiration Date"]] <- col_date("%m/%d/%Y")

# Read patent file.
nasa_patents <- readr::read_csv(file = file.path("data", nasa_patents_fn),
                                col_types = nasa_spec)

# In variable names, replace spaces.
names(nasa_patents) <- str_replace_all(string = names(nasa_patents),
                                       pattern = " ",
                                       replacement = "_")

# Transform the application number to USPTO format
nasa_patents <- nasa_patents %>%
  mutate(Application_SN_uspto = str_c("US",
        str_replace_all(Application_SN, "[[:punct:]]", "")))
```

## Retrieve patent data
### Google
```{r get_google_patent}
get_patent_google <- function(pat_num) {
  file_path <- file.path("data", str_c(pat_num, ".html"))

  if (!file.exists(file_path)) {
    req_url <- "https://patents.google.com/patent/USPATNUMSTUB"
    req_url <- str_replace(string = req_url,
                           pattern = "PATNUMSTUB",
                           replacement = as.character(pat_num))
    print(str_c("Downloading patent ",
                as.character(pat_num), "..."))
    
    r <- GET(req_url, write_disk(file_path))
    warn_for_status(r)
  }
}

# Filter missing patent numbers
is_na_patnum <- is.na(nasa_patents$Patent_Number)
is_patnum <- nasa_patents$Patent_Number > 0

nasa_patent_numbers <- nasa_patents$Patent_Number[!is_na_patnum & is_patnum]

for (patnum in nasa_patent_numbers) {
   get_patent_google(patnum)
}

print("Downloads complete.")
```

### select relevant patent data from Google HTML
```{r}
# explore downloaded data
raw.html <- read_html("data/6020587.html")
# title
raw.html %>% html_nodes("body search-app article h1") %>% html_text() %>% .[[1]]
# filing date
raw.html %>% html_nodes("[itemprop='filingDate']") %>% html_text() %>% .[[2]]
# abstract
raw.html %>% html_nodes("abstract") %>% html_text()
# description > origin of the invention
raw.html %>% html_nodes("body search-app article section div div p") %>% html_text() %>% .[[1]]
# description > background of the invention > field of the invention
raw.html %>% html_nodes("body search-app article section div div p") %>% html_text() %>% .[[3]]
# cited by 
raw.html %>% html_nodes(xpath="/html/body/search-app/article/table") %>% html_table()
# cited by (count) nrow()
cited_by <- raw.html %>% html_nodes(xpath="/html/body/search-app/article/table") %>% html_table() 
#nrow(cited_by[[1]])
# patent citations 
raw.html %>% html_nodes(xpath="/html/body/search-app/article/section/table") %>% html_table()
raw.html %>% html_nodes(xpath="/html/body/search-app/article/section/h2") %>% html_text()
#filter(str_detect(raw.html %>% html_nodes(xpath="/html/body/search-app/article/section/h2") %>% html_text(), "Patent Citations"))
#print(raw.html %>% html_nodes(xpath="/html/body/search-app/article/section/table") %>% html_table())[[2]]
```

### clean and transform selected data into dataframe
```{r}
# create object with all filenames
files <- list.files(path="data", pattern="[0-9]{5,}.html", full.names=TRUE, recursive=FALSE)

# create lists of extracted data
# title
raw.title <- lapply(files, function(x) {
  raw.html <- read_html(x, header=TRUE) # load file
  # apply function
  raw.html %>% html_nodes("body search-app article h1") %>% html_text() %>% .[[1]] %>% str_remove(" \\n +\\- [[:alpha:]]{6,} [[:alpha:]]{6,}")
})
# abstract
raw.abstract <- lapply(files, function(x) {
  raw.html <- read_html(x, header=TRUE) # load file
  # apply function
  raw.html %>% html_nodes("abstract") %>% html_text()
})
# prior art date as proxy for filing date that includes support 1899 example
filing.date <- lapply(files, function(x) {
  raw.html <- read_html(x, header=TRUE) # load file
  # apply function
  raw.html %>% html_nodes("[itemprop='priorArtDate']") %>% html_text() %>% .[[1]]
})
# description > origin of the invention
raw.origin <- lapply(files, function(x) {
  raw.html <- read_html(x, header=TRUE) # load file
  # apply function
  raw.html %>% html_nodes("body search-app article section div div p") %>% html_text() %>% .[[1]]
})
# description > background of the invention > field of the invention
raw.background <- lapply(files, function(x) {
  raw.html <- read_html(x, header=TRUE) # load file
  # apply function
  raw.html %>% html_nodes("body search-app article section div div p") %>% html_text() %>% .[[3]]
})
```

### section to answer question, "how many other patents does a patent reference ('count.citations'), and how many other patents reference it ('count.cited_by')?" This question is a analagous to, "how many shoulders does a given patent stand on?"
# todo: deuglify the regex
```{r}
# cited by (count) 
cited_by <- lapply(files, function(x) {
  raw.html <- read_html(x, header=TRUE) # load file
  # apply function
  raw.html %>% html_nodes(xpath="/html/body/search-app/article/section/h2") %>% html_text() %>% str_subset(pattern = "Cited")
})

# clean output
cited_by <- as.character(cited_by)

# cited by (count) 
count.cited_by <- lapply(cited_by, function(x) 
  # remove up to digit
  x %>% str_remove("\\D+")
)
count.cited_by <- lapply(count.cited_by, function(x) 
  # remove after digit
  x %>% str_remove("\\)$")
)

count.cited_by <- lapply(count.cited_by, function(x) 
  # remove after digit
  x %>% as.integer()
)

# citations (count) 
citations <- lapply(files, function(x) {
  raw.html <- read_html(x, header=TRUE) # load file
  # apply function
  raw.html %>% html_nodes(xpath="/html/body/search-app/article/section/h2") %>% html_text() %>% str_subset(pattern = "Patent Citations")
})

# drop Non-Patent Citations
citations <- lapply(citations, function(x) {
  # apply function
  str_detect(x, "Non-") %>%
                  discard(x, .)
})

# clean output
citations <- as.character(citations)

# cited by (count) 
count.citations <- lapply(citations, function(x) 
  # remove up to digit
  x %>% str_remove("\\D+")
)
count.citations <- lapply(count.citations, function(x) 
  # remove after digit
  x %>% str_remove("\\)$")
)

count.citations <- lapply(count.citations, function(x) 
  # remove after digit
  x %>% as.integer()
)
```

### clean text and output to df
```{r}
# not necessary, but makes lists nicer to read
# todo: make consistent
raw.title <- unlist(raw.title, recursive = FALSE)
#raw.title
raw.abstract <- unlist(raw.abstract, recursive = FALSE)
raw.origin <- unlist(raw.origin, recursive = FALSE)
filing.date <- filing.date %>% unlist(recursive = FALSE)
raw.background <- unlist(raw.background, recursive = FALSE)
count.citations %>% unlist(recursive = FALSE)
count.cited_by %>% unlist(recursive = FALSE)

# get "appId" with regex
app_num <- unlist(str_extract_all(raw.title, "[A-Z]{2,}[0-9]{6,}[A-Z]+[0-9]?"))

# remove "appId" from "title" 
raw.title <- str_remove(raw.title, app_num)
#raw.title
raw.title <- str_remove(raw.title, " - ") # this could be less greedy
#raw.title

# bind lists into dataframe and name columns
raw.df <- setNames(do.call(rbind.data.frame, Map('c', app_num, raw.title, raw.abstract, raw.origin, raw.background, count.cited_by, count.citations, filing.date)), c("appId", "title", "abstract", "origin", "background", "citedBy", "citations", "filingDate"))
raw.df
```

### write to file for easy access
```{r}
write.csv(raw.df,"data/patent_text.csv", row.names = FALSE)
raw.df <- read.csv("data/patent_text.csv", as.is = TRUE)
raw.df$filingDate <- as.Date(raw.df$filingDate, format = "%Y-%m-%d")
```

### EDA and data transformation for visualization
```{r}
ggplot(raw.df, aes(x=filingDate, y=citedBy)) + geom_point()
date.df <- as.Date(raw.df$filingDate, format = "%Y-%m-%d")
probs <- 1:4/4
# date quartiles 
date.df <- as.Date(quantile(unclass(date.df), probs), origin = "1970-01-01")
date.df <- mutate_if(raw.df, is.factor, as.Date)
date.df
# todo: plot facets, patent cited/citations across date quartiles
str(raw.df) # check structure
str(date.df)

raw.df %>% 
  select(citations, title, filingDate) %>%
  filter(filingDate >= as.Date("2007-12-14"))
```

```{r}
#qplot(raw.df$citedBy, geom="histogram")
#qplot(raw.df$citations, geom="histogram")
#ggplot(data=raw.df, aes(raw.df$citedBy)) + geom_histogram()

# Histogram with density plot
p1 <- ggplot(raw.df, aes(x=citedBy)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white") +
 geom_density(alpha=.2, fill="#99badd") + 
facet_grid()
p2 <- ggplot(raw.df, aes(x=citations)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666") 
multiplot(p1, p2, cols = 2)
```

## Appendix
### USPTO
We didn't use the USPTO source. We acquired patent data from Google, instead. This code chunk for this acquistion is disabled because I don't understand what it retrieves, but I'll keep the code in case I can use it later.
```{r get_uspto, eval=FALSE}
# Curl
# curl -X GET --header 'Accept: application/json' 'https://developer.uspto.gov/ibd-api/v1/patent/application?applicationNumber=US14202699%2CUS12795356&start=0&rows=100'
# Request URL
# https://developer.uspto.gov/ibd-api/v1/patent/application?applicationNumber=US14202699%2CUS12795356&start=0&rows=100

get_patent_application_uspto <- function(app_num) {
  file_path <- file.path("data", str_c(app_num, ".json"))
  
  if (!file.exists(file_path)) {
    req_url <- "https://developer.uspto.gov/ibd-api/v1/patent/application?APPSTUB&start=0&rows=100"
    req_url <- str_replace(string = req_url,
                           pattern = "APPSTUB",
                           replacement = as.character(app_num))
    print(str_c("Downloading application ",
                as.character(app_num), "..."))
    
    patent_application <- fromJSON(req_url)
    stream_out(x = patent_application[["response"]][["docs"]],
               file(file_path))
    
    print(file_path)
  }
}

# Need to review contents of JSON files.
for (app in nasa_patents$Application_SN_uspto) {
  get_patent_application_uspto(app)
}

print("Downloads complete.")
```

### Google Patents
Other attributes explored
```{r}
# prior art keywords
raw.html %>% html_nodes("[itemprop='priorArtKeywords']") %>% html_text() 
# inventors
raw.html %>% html_nodes("[itemprop='inventor']") %>% html_text()
# current assignee
raw.html %>% html_nodes("[itemprop='assigneeCurrent']") %>% html_text()
# original assignee
#raw.html %>% html_nodes("[itemprop='assigneeOriginal']") %>% html_text() # lots of noise; need to parse later
# filing date
raw.html %>% html_nodes("[itemprop='filingDate']") %>% html_text() # which one to select?
# classifications
raw.html %>% html_nodes("[itemprop='cpcs']") %>% html_text() # what does it mean?

```
